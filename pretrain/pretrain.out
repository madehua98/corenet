nohup: ignoring input
2024-07-20 11:58:54 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
small
dci
2024-07-20 11:58:55 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-20 11:58:55 - [34m[1mLOGS   [0m - [36mModel[0m
ViTamin(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=7476, bias=True, channel_first=False)
)
[31m=================================================================[0m
                            ViTamin Summary
[31m=================================================================[0m
Total parameters     =   29.490 M
Total trainable parameters =   29.490 M

2024-07-20 11:58:55 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-20 11:58:55 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 29.49M                 | 3.389G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  0.116G    |
|   pool.proj                          |   1.18M                |   0.116G   |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.502M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  51.38M    |
|   mlp.0                              |   0.263M               |   25.69M   |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   25.69M   |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  3.835M                |  3.828M    |
|   classifier.weight                  |   (7476, 512)          |            |
|   classifier.bias                    |   (7476,)              |            |
2024-07-20 11:58:55 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-20 11:58:55 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks.0.attn.attn_drop', 'blocks1.6.attn.attn_drop', 'blocks.5.ls1', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks.5.drop_path1', 'blocks.6.attn.attn_drop', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks.5.attn.attn_drop', 'blocks.1.attn.k_norm', 'blocks1.5.ls2', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'neural_augmentor.contrast', 'patch_embed.backbone.stages.1.0.down', 'neural_augmentor.brightness.max_fn', 'blocks.2.attn.k_norm', 'blocks.6.ls2', 'blocks.3.attn.k_norm', 'blocks.2.drop_path2', 'blocks.4.attn.q_norm', 'blocks.2.ls2', 'patch_embed.backbone.stages.1.0.drop_path', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks1.4.drop_path2', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks.5.attn.q_norm', 'blocks1.4.attn.attn_drop', 'blocks1.3.attn.k_norm', 'neural_augmentor.noise.max_fn', 'blocks.0.attn.k_norm', 'blocks.6.ls1', 'blocks.1.ls2', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks.0.attn.q_norm', 'blocks.0.ls1', 'patch_embed.backbone.stages.1.2.down', 'blocks.5.drop_path2', 'patch_embed.backbone.stages.1.3.down', 'blocks1.6.drop_path2', 'blocks1.1.drop_path1', 'blocks1.2.ls2', 'neural_augmentor.noise', 'neural_augmentor.brightness.min_fn', 'blocks1.3.drop_path1', 'patch_embed.backbone.stages.1.1.drop_path', 'patch_embed.backbone.stem.norm1.drop', 'blocks1.1.ls1', 'norm_pre', 'blocks.4.drop_path2', 'blocks1.3.ls2', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks.4.attn.k_norm', 'blocks.4.attn.attn_drop', 'blocks.1.ls1', 'blocks.0.ls2', 'blocks1.2.drop_path2', 'blocks.3.attn.q_norm', 'blocks1.2.ls1', 'blocks.2.drop_path1', 'blocks1.4.ls2', 'blocks.1.attn.q_norm', 'blocks.1.drop_path2', 'neural_augmentor.contrast.min_fn', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks.5.ls2', 'blocks1.2.attn.k_norm', 'blocks.4.ls1', 'neural_augmentor', 'blocks1.5.drop_path1', 'blocks1.5.attn.attn_drop', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks1.6.ls2', 'blocks.0.drop_path2', 'blocks.1.attn.attn_drop', 'blocks.6.attn.q_norm', 'patch_embed.backbone.stages.1.1.shortcut', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'norm', 'blocks.4.ls2', 'blocks.6.drop_path1', 'blocks1.6.attn.k_norm', 'blocks1.4.ls1', 'patch_embed.backbone.stages.0.1.down', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks.1.drop_path1', 'blocks.2.attn.attn_drop', 'blocks1.4.attn.k_norm', 'blocks1.1.attn.q_norm', 'blocks.6.drop_path2', 'blocks1.5.drop_path2', 'blocks1.1.attn.k_norm', 'blocks1.3.drop_path2', 'blocks.2.ls1', 'blocks1.3.attn.attn_drop', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks.0.drop_path1', 'blocks1.2.attn.attn_drop', 'blocks1.0.drop_path1', 'neural_augmentor.brightness', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.0.attn.q_norm', 'patch_drop', 'blocks1.3.attn.q_norm', 'blocks1.5.attn.k_norm', 'blocks.3.drop_path1', 'blocks1.6.drop_path1', 'blocks1.5.attn.q_norm', 'blocks1.0.attn.k_norm', 'blocks.4.drop_path1', 'blocks1.5.ls1', 'blocks.3.ls1', 'blocks1.1.drop_path2', 'patch_embed.proj', 'blocks1.2.drop_path1', 'neural_augmentor.noise.min_fn', 'patch_embed.backbone.stages.0.0.down', 'blocks1.6.ls1', 'blocks.5.attn.k_norm', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks.3.attn.attn_drop', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks1.1.attn.attn_drop', 'blocks1.3.ls1', 'patch_embed.backbone.stages.1.1.down', 'blocks.3.drop_path2', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks.6.attn.k_norm', 'blocks.2.attn.q_norm', 'blocks.3.ls2', 'blocks1.4.drop_path1', 'blocks1.0.attn.attn_drop', 'blocks1.0.ls2', 'neural_augmentor.contrast.max_fn', 'blocks1.1.ls2', 'blocks1.2.attn.q_norm', 'blocks1.0.drop_path2', 'blocks1.0.ls1', 'blocks1.4.attn.q_norm', 'blocks1.6.attn.q_norm', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'patch_embed.backbone.stages.1.3.pre_norm.drop'}
2024-07-20 11:58:55 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::avg_pool2d': 2, 'aten::sum': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-20 11:58:55 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-20 11:58:55 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-20 11:58:55 - [34m[1mLOGS   [0m - Available GPUs: 8
2024-07-20 11:58:55 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-20 11:58:56 - [34m[1mLOGS   [0m - Directory exists at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train
2024-07-20 11:59:00 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:40002
small
dci
2024-07-20 11:59:00 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:40002
small
dci
2024-07-20 11:59:00 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:40002
small
dci
2024-07-20 11:59:00 - [32m[1mINFO   [0m - distributed init (rank 4): tcp://localhost:40002
small
dci
2024-07-20 11:59:00 - [32m[1mINFO   [0m - distributed init (rank 6): tcp://localhost:40002
small
dci
2024-07-20 11:59:00 - [32m[1mINFO   [0m - distributed init (rank 5): tcp://localhost:40002
small
dci
2024-07-20 11:59:00 - [32m[1mINFO   [0m - distributed init (rank 7): tcp://localhost:40002
small
dci
2024-07-20 11:58:59 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:40002
2024-07-20 11:59:02 - [34m[1mLOGS   [0m - Training dataset details are given below
WordnetTaggedClassificationDataset(
	root= 
	is_training=True 
	num_samples=64290000
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	total_tar_files=6429
	max_files_per_tar=10000
	num_synsets=7476
)
2024-07-20 11:59:04 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=True
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=200
	 scales=[(128, 128, 612), (144, 144, 483), (160, 160, 392), (176, 176, 323), (192, 192, 272), (208, 208, 231), (224, 224, 200), (240, 240, 174), (256, 256, 153), (272, 272, 135), (288, 288, 120), (304, 304, 108), (320, 320, 98)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-20 11:59:04 - [34m[1mLOGS   [0m - Number of data workers: 64
small
dci
2024-07-20 11:59:06 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-20 11:59:06 - [34m[1mLOGS   [0m - [36mModel[0m
ViTamin(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=7476, bias=True, channel_first=False)
)
[31m=================================================================[0m
                            ViTamin Summary
[31m=================================================================[0m
Total parameters     =   29.490 M
Total trainable parameters =   29.490 M

2024-07-20 11:59:06 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-20 11:59:06 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 29.49M                 | 3.389G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  0.116G    |
|   pool.proj                          |   1.18M                |   0.116G   |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.502M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  51.38M    |
|   mlp.0                              |   0.263M               |   25.69M   |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   25.69M   |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  3.835M                |  3.828M    |
|   classifier.weight                  |   (7476, 512)          |            |
|   classifier.bias                    |   (7476,)              |            |
2024-07-20 11:59:06 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-20 11:59:06 - [33m[1mWARNING[0m - Uncalled Modules:
{'patch_embed.backbone.stages.0.0.pre_norm.act', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks.3.drop_path2', 'blocks1.3.attn.q_norm', 'blocks1.6.drop_path1', 'blocks1.5.drop_path1', 'blocks.2.attn.q_norm', 'blocks1.0.ls1', 'patch_embed.backbone.stages.1.2.down', 'patch_embed.backbone.stages.1.3.down', 'blocks1.6.attn.k_norm', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks.4.attn.q_norm', 'blocks1.1.attn.q_norm', 'blocks1.1.ls1', 'blocks1.0.drop_path2', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks1.4.ls2', 'blocks.2.ls2', 'blocks1.1.attn.attn_drop', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks1.2.attn.attn_drop', 'blocks.1.ls1', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks1.6.ls2', 'blocks1.2.attn.q_norm', 'blocks1.5.ls1', 'blocks1.5.drop_path2', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks.6.attn.k_norm', 'blocks1.2.drop_path2', 'blocks.1.attn.q_norm', 'patch_embed.backbone.stages.0.1.drop_path', 'patch_drop', 'blocks.5.drop_path1', 'blocks1.4.attn.k_norm', 'blocks1.4.drop_path2', 'blocks.2.attn.k_norm', 'blocks.4.attn.k_norm', 'neural_augmentor.noise.max_fn', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks1.6.attn.q_norm', 'patch_embed.backbone.stages.1.3.drop_path', 'norm', 'neural_augmentor.contrast.max_fn', 'neural_augmentor.noise', 'blocks.1.drop_path1', 'patch_embed.backbone.stages.1.1.shortcut', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'neural_augmentor.brightness', 'neural_augmentor.contrast', 'blocks1.3.attn.k_norm', 'blocks1.2.ls1', 'blocks.0.attn.q_norm', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks.0.ls1', 'blocks1.0.attn.q_norm', 'blocks.5.ls2', 'patch_embed.backbone.stem.norm1.drop', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'neural_augmentor', 'blocks1.0.ls2', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks.0.attn.k_norm', 'patch_embed.backbone.stages.1.1.down', 'blocks1.3.ls1', 'blocks.5.attn.k_norm', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks.3.ls2', 'blocks1.5.ls2', 'blocks1.1.drop_path2', 'blocks1.1.drop_path1', 'patch_embed.backbone.stages.0.1.down', 'blocks.6.drop_path2', 'blocks.1.ls2', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks1.2.drop_path1', 'blocks1.5.attn.q_norm', 'blocks.3.attn.k_norm', 'blocks.4.attn.attn_drop', 'blocks.5.attn.attn_drop', 'blocks.5.attn.q_norm', 'blocks1.3.drop_path1', 'blocks.3.attn.q_norm', 'blocks.2.ls1', 'neural_augmentor.brightness.min_fn', 'blocks.2.drop_path1', 'patch_embed.proj', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks.0.drop_path2', 'blocks.0.attn.attn_drop', 'blocks1.5.attn.attn_drop', 'blocks.6.attn.q_norm', 'blocks1.5.attn.k_norm', 'blocks1.6.attn.attn_drop', 'blocks1.6.drop_path2', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'neural_augmentor.noise.min_fn', 'blocks.5.ls1', 'neural_augmentor.contrast.min_fn', 'blocks.4.ls1', 'blocks1.2.ls2', 'blocks.4.drop_path1', 'blocks.3.drop_path1', 'blocks.0.ls2', 'blocks.3.ls1', 'blocks1.2.attn.k_norm', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks.6.ls2', 'patch_embed.backbone.stages.1.0.down', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks.1.attn.attn_drop', 'neural_augmentor.brightness.max_fn', 'blocks.3.attn.attn_drop', 'blocks1.4.drop_path1', 'blocks1.0.attn.k_norm', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks1.4.attn.attn_drop', 'blocks1.6.ls1', 'blocks1.0.attn.attn_drop', 'blocks1.3.attn.attn_drop', 'blocks.5.drop_path2', 'blocks.1.attn.k_norm', 'blocks.4.drop_path2', 'norm_pre', 'blocks1.0.drop_path1', 'blocks.4.ls2', 'blocks.2.attn.attn_drop', 'blocks.6.ls1', 'patch_embed.backbone.stages.0.0.down', 'blocks1.1.ls2', 'blocks1.4.ls1', 'blocks.6.attn.attn_drop', 'blocks1.1.attn.k_norm', 'blocks.0.drop_path1', 'blocks.1.drop_path2', 'blocks1.3.ls2', 'blocks.6.drop_path1', 'blocks1.3.drop_path2', 'blocks1.4.attn.q_norm', 'blocks.2.drop_path2', 'patch_embed.backbone.stages.0.1.pre_norm.drop'}
2024-07-20 11:59:06 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::avg_pool2d': 2, 'aten::sum': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-20 11:59:06 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-20 11:59:06 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-20 11:59:06 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-07-20 11:59:06 - [34m[1mLOGS   [0m - Max. iteration for training: 100000
2024-07-20 11:59:06 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=1e-05
 	 max_lr=0.001
 	 period=90001
 	 warmup_init_lr=1e-06
 	 warmup_iters=10000
 )
2024-07-20 11:59:07 - [34m[1mLOGS   [0m - Loaded checkpoint from /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/training_checkpoint_last.pt
2024-07-20 11:59:07 - [34m[1mLOGS   [0m - Resuming training for epoch 1
2024-07-20 11:59:07 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-20 11:59:09 - [32m[1mINFO   [0m - Training epoch 1
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-20 12:04:24 - [34m[1mLOGS   [0m - Epoch:   1 [    7861/  100000], loss: {'classification': 62.871, 'neural_augmentation': 9.3221, 'total_loss': 72.1931}, LR: [0.000786, 0.000786], Avg. batch load time: 283.868, Elapsed time: 314.32
2024-07-20 12:07:29 - [34m[1mLOGS   [0m - Epoch:   1 [    7986/  100000], loss: {'classification': 72.6803, 'neural_augmentation': 8.6421, 'total_loss': 81.3224}, LR: [0.000799, 0.000799], Avg. batch load time: 0.633, Elapsed time: 499.43
2024-07-20 12:10:34 - [34m[1mLOGS   [0m - Epoch:   1 [    8111/  100000], loss: {'classification': 61.8263, 'neural_augmentation': 8.3326, 'total_loss': 70.159}, LR: [0.000811, 0.000811], Avg. batch load time: 0.355, Elapsed time: 685.09
2024-07-20 12:13:29 - [34m[1mLOGS   [0m - Epoch:   1 [    8236/  100000], loss: {'classification': 58.1456, 'neural_augmentation': 8.1336, 'total_loss': 66.2792}, LR: [0.000824, 0.000824], Avg. batch load time: 0.256, Elapsed time: 859.63
2024-07-20 12:16:34 - [34m[1mLOGS   [0m - Epoch:   1 [    8361/  100000], loss: {'classification': 56.1806, 'neural_augmentation': 7.9745, 'total_loss': 64.1551}, LR: [0.000836, 0.000836], Avg. batch load time: 0.207, Elapsed time: 1044.33
2024-07-20 12:19:52 - [34m[1mLOGS   [0m - Epoch:   1 [    8486/  100000], loss: {'classification': 54.967, 'neural_augmentation': 7.8197, 'total_loss': 62.7866}, LR: [0.000849, 0.000849], Avg. batch load time: 0.186, Elapsed time: 1242.22
2024-07-20 12:23:17 - [34m[1mLOGS   [0m - Epoch:   1 [    8611/  100000], loss: {'classification': 54.0035, 'neural_augmentation': 7.6785, 'total_loss': 61.682}, LR: [0.000861, 0.000861], Avg. batch load time: 0.175, Elapsed time: 1447.37
2024-07-20 12:26:34 - [34m[1mLOGS   [0m - Epoch:   1 [    8736/  100000], loss: {'classification': 53.3326, 'neural_augmentation': 7.5572, 'total_loss': 60.8898}, LR: [0.000874, 0.000874], Avg. batch load time: 0.167, Elapsed time: 1644.91
2024-07-20 12:29:47 - [34m[1mLOGS   [0m - Epoch:   1 [    8861/  100000], loss: {'classification': 52.7948, 'neural_augmentation': 7.4447, 'total_loss': 60.2395}, LR: [0.000886, 0.000886], Avg. batch load time: 0.154, Elapsed time: 1838.10
2024-07-20 12:33:12 - [34m[1mLOGS   [0m - Epoch:   1 [    8986/  100000], loss: {'classification': 52.3621, 'neural_augmentation': 7.3387, 'total_loss': 59.7008}, LR: [0.000899, 0.000899], Avg. batch load time: 0.149, Elapsed time: 2042.79
2024-07-20 12:36:18 - [34m[1mLOGS   [0m - Epoch:   1 [    9111/  100000], loss: {'classification': 51.9981, 'neural_augmentation': 7.2346, 'total_loss': 59.2327}, LR: [0.000911, 0.000911], Avg. batch load time: 0.144, Elapsed time: 2228.83
2024-07-20 12:39:51 - [34m[1mLOGS   [0m - Epoch:   1 [    9236/  100000], loss: {'classification': 51.6672, 'neural_augmentation': 7.131, 'total_loss': 58.7982}, LR: [0.000924, 0.000924], Avg. batch load time: 0.143, Elapsed time: 2441.30
2024-07-20 12:43:24 - [34m[1mLOGS   [0m - Epoch:   1 [    9361/  100000], loss: {'classification': 51.3784, 'neural_augmentation': 7.0267, 'total_loss': 58.4052}, LR: [0.000936, 0.000936], Avg. batch load time: 0.142, Elapsed time: 2655.04
2024-07-20 12:46:24 - [34m[1mLOGS   [0m - Epoch:   1 [    9486/  100000], loss: {'classification': 51.126, 'neural_augmentation': 6.9326, 'total_loss': 58.0587}, LR: [0.000949, 0.000949], Avg. batch load time: 0.137, Elapsed time: 2834.54
2024-07-20 12:49:50 - [34m[1mLOGS   [0m - Epoch:   1 [    9611/  100000], loss: {'classification': 50.9023, 'neural_augmentation': 6.838, 'total_loss': 57.7403}, LR: [0.000961, 0.000961], Avg. batch load time: 0.135, Elapsed time: 3040.84
2024-07-20 12:53:02 - [34m[1mLOGS   [0m - Epoch:   1 [    9736/  100000], loss: {'classification': 50.6875, 'neural_augmentation': 6.7439, 'total_loss': 57.4314}, LR: [0.000974, 0.000974], Avg. batch load time: 0.132, Elapsed time: 3232.91
2024-07-20 12:56:26 - [34m[1mLOGS   [0m - Epoch:   1 [    9861/  100000], loss: {'classification': 50.5042, 'neural_augmentation': 6.6541, 'total_loss': 57.1584}, LR: [0.000986, 0.000986], Avg. batch load time: 0.131, Elapsed time: 3436.92
2024-07-20 12:59:50 - [34m[1mLOGS   [0m - Epoch:   1 [    9986/  100000], loss: {'classification': 50.3398, 'neural_augmentation': 6.5618, 'total_loss': 56.9016}, LR: [0.000999, 0.000999], Avg. batch load time: 0.130, Elapsed time: 3640.58
2024-07-20 13:03:04 - [34m[1mLOGS   [0m - Epoch:   1 [   10111/  100000], loss: {'classification': 50.1776, 'neural_augmentation': 6.4704, 'total_loss': 56.648}, LR: [0.001, 0.001], Avg. batch load time: 0.128, Elapsed time: 3834.23
2024-07-20 13:06:15 - [34m[1mLOGS   [0m - Epoch:   1 [   10236/  100000], loss: {'classification': 50.0392, 'neural_augmentation': 6.384, 'total_loss': 56.4232}, LR: [0.001, 0.001], Avg. batch load time: 0.127, Elapsed time: 4025.72
2024-07-20 13:09:37 - [34m[1mLOGS   [0m - Epoch:   1 [   10361/  100000], loss: {'classification': 49.8937, 'neural_augmentation': 6.2938, 'total_loss': 56.1876}, LR: [0.001, 0.001], Avg. batch load time: 0.126, Elapsed time: 4227.26
2024-07-20 13:12:42 - [34m[1mLOGS   [0m - Epoch:   1 [   10486/  100000], loss: {'classification': 49.756, 'neural_augmentation': 6.2065, 'total_loss': 55.9625}, LR: [0.001, 0.001], Avg. batch load time: 0.124, Elapsed time: 4412.43
2024-07-20 13:15:54 - [34m[1mLOGS   [0m - Epoch:   1 [   10611/  100000], loss: {'classification': 49.628, 'neural_augmentation': 6.123, 'total_loss': 55.751}, LR: [0.001, 0.001], Avg. batch load time: 0.123, Elapsed time: 4604.56
2024-07-20 13:19:11 - [34m[1mLOGS   [0m - Epoch:   1 [   10736/  100000], loss: {'classification': 49.5025, 'neural_augmentation': 6.0341, 'total_loss': 55.5366}, LR: [0.001, 0.001], Avg. batch load time: 0.122, Elapsed time: 4801.78
2024-07-20 13:22:30 - [34m[1mLOGS   [0m - Epoch:   1 [   10861/  100000], loss: {'classification': 49.3823, 'neural_augmentation': 5.9462, 'total_loss': 55.3285}, LR: [0.001, 0.001], Avg. batch load time: 0.120, Elapsed time: 5000.21
2024-07-20 13:25:46 - [34m[1mLOGS   [0m - Epoch:   1 [   10986/  100000], loss: {'classification': 49.2646, 'neural_augmentation': 5.857, 'total_loss': 55.1215}, LR: [0.001, 0.001], Avg. batch load time: 0.120, Elapsed time: 5196.85
2024-07-20 13:29:05 - [34m[1mLOGS   [0m - Epoch:   1 [   11111/  100000], loss: {'classification': 49.1487, 'neural_augmentation': 5.7696, 'total_loss': 54.9182}, LR: [0.001, 0.001], Avg. batch load time: 0.119, Elapsed time: 5395.39
2024-07-20 13:32:25 - [34m[1mLOGS   [0m - Epoch:   1 [   11236/  100000], loss: {'classification': 49.0379, 'neural_augmentation': 5.6825, 'total_loss': 54.7204}, LR: [0.001, 0.001], Avg. batch load time: 0.120, Elapsed time: 5595.71
2024-07-20 13:35:47 - [34m[1mLOGS   [0m - Epoch:   1 [   11361/  100000], loss: {'classification': 48.931, 'neural_augmentation': 5.5968, 'total_loss': 54.5278}, LR: [0.000999, 0.000999], Avg. batch load time: 0.120, Elapsed time: 5797.53
2024-07-20 13:38:56 - [34m[1mLOGS   [0m - Epoch:   1 [   11486/  100000], loss: {'classification': 48.8274, 'neural_augmentation': 5.5124, 'total_loss': 54.3399}, LR: [0.000999, 0.000999], Avg. batch load time: 0.119, Elapsed time: 5986.41
2024-07-20 13:42:20 - [34m[1mLOGS   [0m - Epoch:   1 [   11611/  100000], loss: {'classification': 48.7176, 'neural_augmentation': 5.4282, 'total_loss': 54.1458}, LR: [0.000999, 0.000999], Avg. batch load time: 0.118, Elapsed time: 6190.95
2024-07-20 13:45:41 - [34m[1mLOGS   [0m - Epoch:   1 [   11736/  100000], loss: {'classification': 48.6092, 'neural_augmentation': 5.3424, 'total_loss': 53.9517}, LR: [0.000999, 0.000999], Avg. batch load time: 0.118, Elapsed time: 6391.49
2024-07-20 13:48:52 - [34m[1mLOGS   [0m - Epoch:   1 [   11861/  100000], loss: {'classification': 48.5173, 'neural_augmentation': 5.2623, 'total_loss': 53.7796}, LR: [0.000999, 0.000999], Avg. batch load time: 0.117, Elapsed time: 6583.06
2024-07-20 13:52:04 - [34m[1mLOGS   [0m - Epoch:   1 [   11986/  100000], loss: {'classification': 48.4243, 'neural_augmentation': 5.185, 'total_loss': 53.6093}, LR: [0.000999, 0.000999], Avg. batch load time: 0.116, Elapsed time: 6775.03
2024-07-20 13:55:28 - [34m[1mLOGS   [0m - Epoch:   1 [   12111/  100000], loss: {'classification': 48.3234, 'neural_augmentation': 5.1043, 'total_loss': 53.4277}, LR: [0.000999, 0.000999], Avg. batch load time: 0.117, Elapsed time: 6978.96
2024-07-20 13:58:31 - [34m[1mLOGS   [0m - Epoch:   1 [   12236/  100000], loss: {'classification': 48.2215, 'neural_augmentation': 5.0278, 'total_loss': 53.2493}, LR: [0.000998, 0.000998], Avg. batch load time: 0.116, Elapsed time: 7161.87
2024-07-20 14:02:11 - [34m[1mLOGS   [0m - Epoch:   1 [   12361/  100000], loss: {'classification': 48.1227, 'neural_augmentation': 4.9447, 'total_loss': 53.0674}, LR: [0.000998, 0.000998], Avg. batch load time: 0.117, Elapsed time: 7382.09
2024-07-20 14:05:26 - [34m[1mLOGS   [0m - Epoch:   1 [   12486/  100000], loss: {'classification': 48.0313, 'neural_augmentation': 4.8701, 'total_loss': 52.9014}, LR: [0.000998, 0.000998], Avg. batch load time: 0.116, Elapsed time: 7576.66
2024-07-20 14:09:11 - [34m[1mLOGS   [0m - Epoch:   1 [   12611/  100000], loss: {'classification': 47.9297, 'neural_augmentation': 4.7908, 'total_loss': 52.7205}, LR: [0.000998, 0.000998], Avg. batch load time: 0.117, Elapsed time: 7801.67
2024-07-20 14:12:14 - [34m[1mLOGS   [0m - Epoch:   1 [   12736/  100000], loss: {'classification': 47.8326, 'neural_augmentation': 4.7193, 'total_loss': 52.5519}, LR: [0.000998, 0.000998], Avg. batch load time: 0.116, Elapsed time: 7984.86
2024-07-20 14:15:20 - [34m[1mLOGS   [0m - Epoch:   1 [   12861/  100000], loss: {'classification': 47.733, 'neural_augmentation': 4.6475, 'total_loss': 52.3805}, LR: [0.000998, 0.000998], Avg. batch load time: 0.114, Elapsed time: 8170.37
2024-07-20 14:18:34 - [34m[1mLOGS   [0m - Epoch:   1 [   12986/  100000], loss: {'classification': 47.6388, 'neural_augmentation': 4.5796, 'total_loss': 52.2184}, LR: [0.000997, 0.000997], Avg. batch load time: 0.114, Elapsed time: 8364.74
2024-07-20 14:21:47 - [34m[1mLOGS   [0m - Epoch:   1 [   13111/  100000], loss: {'classification': 47.5387, 'neural_augmentation': 4.5093, 'total_loss': 52.048}, LR: [0.000997, 0.000997], Avg. batch load time: 0.113, Elapsed time: 8557.31
2024-07-20 14:25:01 - [34m[1mLOGS   [0m - Epoch:   1 [   13236/  100000], loss: {'classification': 47.4389, 'neural_augmentation': 4.441, 'total_loss': 51.88}, LR: [0.000997, 0.000997], Avg. batch load time: 0.113, Elapsed time: 8752.00
2024-07-20 14:28:24 - [34m[1mLOGS   [0m - Epoch:   1 [   13361/  100000], loss: {'classification': 47.3424, 'neural_augmentation': 4.3748, 'total_loss': 51.7171}, LR: [0.000997, 0.000997], Avg. batch load time: 0.113, Elapsed time: 8954.99
2024-07-20 14:31:43 - [34m[1mLOGS   [0m - Epoch:   1 [   13486/  100000], loss: {'classification': 47.2441, 'neural_augmentation': 4.3068, 'total_loss': 51.5509}, LR: [0.000996, 0.000996], Avg. batch load time: 0.113, Elapsed time: 9153.26
2024-07-20 14:34:41 - [34m[1mLOGS   [0m - Epoch:   1 [   13611/  100000], loss: {'classification': 47.1499, 'neural_augmentation': 4.2462, 'total_loss': 51.3961}, LR: [0.000996, 0.000996], Avg. batch load time: 0.112, Elapsed time: 9331.26
2024-07-20 14:37:59 - [34m[1mLOGS   [0m - Epoch:   1 [   13736/  100000], loss: {'classification': 47.0607, 'neural_augmentation': 4.1849, 'total_loss': 51.2456}, LR: [0.000996, 0.000996], Avg. batch load time: 0.112, Elapsed time: 9529.95
2024-07-20 14:41:07 - [34m[1mLOGS   [0m - Epoch:   1 [   13861/  100000], loss: {'classification': 46.9662, 'neural_augmentation': 4.1252, 'total_loss': 51.0915}, LR: [0.000996, 0.000996], Avg. batch load time: 0.112, Elapsed time: 9717.50
2024-07-20 14:44:17 - [34m[1mLOGS   [0m - Epoch:   1 [   13986/  100000], loss: {'classification': 46.8771, 'neural_augmentation': 4.0668, 'total_loss': 50.944}, LR: [0.000995, 0.000995], Avg. batch load time: 0.112, Elapsed time: 9907.62
2024-07-20 14:47:46 - [34m[1mLOGS   [0m - Epoch:   1 [   14111/  100000], loss: {'classification': 46.7829, 'neural_augmentation': 4.0068, 'total_loss': 50.7897}, LR: [0.000995, 0.000995], Avg. batch load time: 0.111, Elapsed time: 10116.26
2024-07-20 14:50:55 - [34m[1mLOGS   [0m - Epoch:   1 [   14236/  100000], loss: {'classification': 46.6939, 'neural_augmentation': 3.9518, 'total_loss': 50.6457}, LR: [0.000995, 0.000995], Avg. batch load time: 0.111, Elapsed time: 10305.72
2024-07-20 14:54:02 - [34m[1mLOGS   [0m - Epoch:   1 [   14361/  100000], loss: {'classification': 46.6054, 'neural_augmentation': 3.8984, 'total_loss': 50.5038}, LR: [0.000994, 0.000994], Avg. batch load time: 0.110, Elapsed time: 10492.93
2024-07-20 14:57:28 - [34m[1mLOGS   [0m - Epoch:   1 [   14486/  100000], loss: {'classification': 46.5178, 'neural_augmentation': 3.8432, 'total_loss': 50.3611}, LR: [0.000994, 0.000994], Avg. batch load time: 0.111, Elapsed time: 10698.41
2024-07-20 15:00:59 - [34m[1mLOGS   [0m - Epoch:   1 [   14611/  100000], loss: {'classification': 46.4234, 'neural_augmentation': 3.7881, 'total_loss': 50.2115}, LR: [0.000994, 0.000994], Avg. batch load time: 0.111, Elapsed time: 10909.99
2024-07-20 15:04:23 - [34m[1mLOGS   [0m - Epoch:   1 [   14736/  100000], loss: {'classification': 46.3316, 'neural_augmentation': 3.7351, 'total_loss': 50.0668}, LR: [0.000993, 0.000993], Avg. batch load time: 0.111, Elapsed time: 11113.70
2024-07-20 15:07:39 - [34m[1mLOGS   [0m - Epoch:   1 [   14861/  100000], loss: {'classification': 46.2449, 'neural_augmentation': 3.685, 'total_loss': 49.9299}, LR: [0.000993, 0.000993], Avg. batch load time: 0.111, Elapsed time: 11309.78
2024-07-20 15:10:51 - [34m[1mLOGS   [0m - Epoch:   1 [   14986/  100000], loss: {'classification': 46.1596, 'neural_augmentation': 3.6376, 'total_loss': 49.7972}, LR: [0.000993, 0.000993], Avg. batch load time: 0.111, Elapsed time: 11501.53
2024-07-20 15:14:21 - [34m[1mLOGS   [0m - Epoch:   1 [   15111/  100000], loss: {'classification': 46.0706, 'neural_augmentation': 3.5874, 'total_loss': 49.6579}, LR: [0.000992, 0.000992], Avg. batch load time: 0.111, Elapsed time: 11711.91
2024-07-20 15:17:43 - [34m[1mLOGS   [0m - Epoch:   1 [   15236/  100000], loss: {'classification': 45.989, 'neural_augmentation': 3.5413, 'total_loss': 49.5303}, LR: [0.000992, 0.000992], Avg. batch load time: 0.111, Elapsed time: 11913.38
2024-07-20 15:20:49 - [34m[1mLOGS   [0m - Epoch:   1 [   15361/  100000], loss: {'classification': 45.9083, 'neural_augmentation': 3.4974, 'total_loss': 49.4057}, LR: [0.000991, 0.000991], Avg. batch load time: 0.111, Elapsed time: 12099.76
2024-07-20 15:24:08 - [34m[1mLOGS   [0m - Epoch:   1 [   15486/  100000], loss: {'classification': 45.8236, 'neural_augmentation': 3.4521, 'total_loss': 49.2756}, LR: [0.000991, 0.000991], Avg. batch load time: 0.111, Elapsed time: 12298.28
2024-07-20 15:27:23 - [34m[1mLOGS   [0m - Epoch:   1 [   15611/  100000], loss: {'classification': 45.7413, 'neural_augmentation': 3.4085, 'total_loss': 49.1498}, LR: [0.000991, 0.000991], Avg. batch load time: 0.110, Elapsed time: 12493.45
2024-07-20 15:30:56 - [34m[1mLOGS   [0m - Epoch:   1 [   15736/  100000], loss: {'classification': 45.6598, 'neural_augmentation': 3.3653, 'total_loss': 49.0252}, LR: [0.00099, 0.00099], Avg. batch load time: 0.111, Elapsed time: 12706.82
2024-07-20 15:32:15 - [34m[1mLOGS   [0m - *** Training summary for epoch 1
	 loss={'classification': 45.6214, 'neural_augmentation': 3.3455, 'total_loss': 48.9669}
2024-07-20 15:32:17 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/checkpoint_best.pt
2024-07-20 15:32:18 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/training_checkpoint_last.pt
2024-07-20 15:32:18 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/checkpoint_last.pt
2024-07-20 15:32:19 - [34m[1mLOGS   [0m - Training checkpoint for epoch 1/iteration 15793 is saved at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/training_checkpoint_epoch_1_iter_15793.pt
2024-07-20 15:32:19 - [34m[1mLOGS   [0m - Model state for epoch 1/iteration 15793 is saved at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/checkpoint_epoch_1_iter_15793.pt
[31m===========================================================================[0m
2024-07-20 15:32:21 - [32m[1mINFO   [0m - Training epoch 2
2024-07-20 15:33:34 - [34m[1mLOGS   [0m - Epoch:   2 [   15793/  100000], loss: {'classification': 40.7009, 'neural_augmentation': 0.6712, 'total_loss': 41.3721}, LR: [0.00099, 0.00099], Avg. batch load time: 70.295, Elapsed time: 73.05
2024-07-20 15:36:57 - [34m[1mLOGS   [0m - Epoch:   2 [   15918/  100000], loss: {'classification': 40.5362, 'neural_augmentation': 0.6955, 'total_loss': 41.2317}, LR: [0.000989, 0.000989], Avg. batch load time: 0.256, Elapsed time: 276.25
2024-07-20 15:40:04 - [34m[1mLOGS   [0m - Epoch:   2 [   16043/  100000], loss: {'classification': 40.5048, 'neural_augmentation': 0.6854, 'total_loss': 41.1902}, LR: [0.000989, 0.000989], Avg. batch load time: 0.177, Elapsed time: 463.66
2024-07-20 15:43:05 - [34m[1mLOGS   [0m - Epoch:   2 [   16168/  100000], loss: {'classification': 40.4284, 'neural_augmentation': 0.6765, 'total_loss': 41.1049}, LR: [0.000989, 0.000989], Avg. batch load time: 0.145, Elapsed time: 643.71
2024-07-20 15:46:26 - [34m[1mLOGS   [0m - Epoch:   2 [   16293/  100000], loss: {'classification': 40.3969, 'neural_augmentation': 0.6677, 'total_loss': 41.0646}, LR: [0.000988, 0.000988], Avg. batch load time: 0.139, Elapsed time: 845.04
2024-07-20 15:49:25 - [34m[1mLOGS   [0m - Epoch:   2 [   16418/  100000], loss: {'classification': 40.3571, 'neural_augmentation': 0.6602, 'total_loss': 41.0173}, LR: [0.000988, 0.000988], Avg. batch load time: 0.127, Elapsed time: 1024.63
2024-07-20 15:52:42 - [34m[1mLOGS   [0m - Epoch:   2 [   16543/  100000], loss: {'classification': 40.3208, 'neural_augmentation': 0.6523, 'total_loss': 40.9731}, LR: [0.000987, 0.000987], Avg. batch load time: 0.125, Elapsed time: 1220.92
2024-07-20 15:55:52 - [34m[1mLOGS   [0m - Epoch:   2 [   16668/  100000], loss: {'classification': 40.2959, 'neural_augmentation': 0.6442, 'total_loss': 40.94}, LR: [0.000987, 0.000987], Avg. batch load time: 0.119, Elapsed time: 1411.06
2024-07-20 15:58:47 - [34m[1mLOGS   [0m - Epoch:   2 [   16793/  100000], loss: {'classification': 40.2599, 'neural_augmentation': 0.6367, 'total_loss': 40.8966}, LR: [0.000986, 0.000986], Avg. batch load time: 0.110, Elapsed time: 1585.88
2024-07-20 16:02:05 - [34m[1mLOGS   [0m - Epoch:   2 [   16918/  100000], loss: {'classification': 40.2152, 'neural_augmentation': 0.6286, 'total_loss': 40.8439}, LR: [0.000986, 0.000986], Avg. batch load time: 0.111, Elapsed time: 1783.71
2024-07-20 16:05:28 - [34m[1mLOGS   [0m - Epoch:   2 [   17043/  100000], loss: {'classification': 40.1841, 'neural_augmentation': 0.6208, 'total_loss': 40.8049}, LR: [0.000985, 0.000985], Avg. batch load time: 0.112, Elapsed time: 1987.46
2024-07-20 16:08:33 - [34m[1mLOGS   [0m - Epoch:   2 [   17168/  100000], loss: {'classification': 40.1434, 'neural_augmentation': 0.613, 'total_loss': 40.7565}, LR: [0.000985, 0.000985], Avg. batch load time: 0.110, Elapsed time: 2172.61
2024-07-20 16:12:02 - [34m[1mLOGS   [0m - Epoch:   2 [   17293/  100000], loss: {'classification': 40.0994, 'neural_augmentation': 0.6056, 'total_loss': 40.705}, LR: [0.000984, 0.000984], Avg. batch load time: 0.114, Elapsed time: 2381.29
2024-07-20 16:15:20 - [34m[1mLOGS   [0m - Epoch:   2 [   17418/  100000], loss: {'classification': 40.0547, 'neural_augmentation': 0.5985, 'total_loss': 40.6532}, LR: [0.000983, 0.000983], Avg. batch load time: 0.113, Elapsed time: 2579.14
2024-07-20 16:18:51 - [34m[1mLOGS   [0m - Epoch:   2 [   17543/  100000], loss: {'classification': 40.0049, 'neural_augmentation': 0.5915, 'total_loss': 40.5964}, LR: [0.000983, 0.000983], Avg. batch load time: 0.115, Elapsed time: 2789.81
2024-07-20 16:22:09 - [34m[1mLOGS   [0m - Epoch:   2 [   17668/  100000], loss: {'classification': 39.9677, 'neural_augmentation': 0.5845, 'total_loss': 40.5522}, LR: [0.000982, 0.000982], Avg. batch load time: 0.113, Elapsed time: 2987.70
2024-07-20 16:25:21 - [34m[1mLOGS   [0m - Epoch:   2 [   17793/  100000], loss: {'classification': 39.9374, 'neural_augmentation': 0.5782, 'total_loss': 40.5155}, LR: [0.000982, 0.000982], Avg. batch load time: 0.112, Elapsed time: 3180.67
2024-07-20 16:28:32 - [34m[1mLOGS   [0m - Epoch:   2 [   17918/  100000], loss: {'classification': 39.9013, 'neural_augmentation': 0.5721, 'total_loss': 40.4733}, LR: [0.000981, 0.000981], Avg. batch load time: 0.112, Elapsed time: 3371.11
2024-07-20 16:31:56 - [34m[1mLOGS   [0m - Epoch:   2 [   18043/  100000], loss: {'classification': 39.8623, 'neural_augmentation': 0.5661, 'total_loss': 40.4284}, LR: [0.000981, 0.000981], Avg. batch load time: 0.112, Elapsed time: 3575.64
2024-07-20 16:35:22 - [34m[1mLOGS   [0m - Epoch:   2 [   18168/  100000], loss: {'classification': 39.8366, 'neural_augmentation': 0.5601, 'total_loss': 40.3967}, LR: [0.00098, 0.00098], Avg. batch load time: 0.113, Elapsed time: 3781.62
2024-07-20 16:38:46 - [34m[1mLOGS   [0m - Epoch:   2 [   18293/  100000], loss: {'classification': 39.8074, 'neural_augmentation': 0.5544, 'total_loss': 40.3618}, LR: [0.000979, 0.000979], Avg. batch load time: 0.113, Elapsed time: 3985.53
2024-07-20 16:41:53 - [34m[1mLOGS   [0m - Epoch:   2 [   18418/  100000], loss: {'classification': 39.7715, 'neural_augmentation': 0.5491, 'total_loss': 40.3206}, LR: [0.000979, 0.000979], Avg. batch load time: 0.112, Elapsed time: 4172.25
2024-07-20 16:45:11 - [34m[1mLOGS   [0m - Epoch:   2 [   18543/  100000], loss: {'classification': 39.7378, 'neural_augmentation': 0.5436, 'total_loss': 40.2814}, LR: [0.000978, 0.000978], Avg. batch load time: 0.111, Elapsed time: 4369.93
2024-07-20 16:48:45 - [34m[1mLOGS   [0m - Epoch:   2 [   18668/  100000], loss: {'classification': 39.7032, 'neural_augmentation': 0.5384, 'total_loss': 40.2416}, LR: [0.000978, 0.000978], Avg. batch load time: 0.113, Elapsed time: 4583.82
2024-07-20 16:51:56 - [34m[1mLOGS   [0m - Epoch:   2 [   18793/  100000], loss: {'classification': 39.6701, 'neural_augmentation': 0.5334, 'total_loss': 40.2035}, LR: [0.000977, 0.000977], Avg. batch load time: 0.112, Elapsed time: 4775.50
2024-07-20 16:55:16 - [34m[1mLOGS   [0m - Epoch:   2 [   18918/  100000], loss: {'classification': 39.6421, 'neural_augmentation': 0.5283, 'total_loss': 40.1704}, LR: [0.000976, 0.000976], Avg. batch load time: 0.113, Elapsed time: 4975.22
2024-07-20 16:58:34 - [34m[1mLOGS   [0m - Epoch:   2 [   19043/  100000], loss: {'classification': 39.6098, 'neural_augmentation': 0.5234, 'total_loss': 40.1332}, LR: [0.000976, 0.000976], Avg. batch load time: 0.113, Elapsed time: 5173.18
2024-07-20 17:01:49 - [34m[1mLOGS   [0m - Epoch:   2 [   19168/  100000], loss: {'classification': 39.581, 'neural_augmentation': 0.5185, 'total_loss': 40.0995}, LR: [0.000975, 0.000975], Avg. batch load time: 0.112, Elapsed time: 5368.34
2024-07-20 17:05:14 - [34m[1mLOGS   [0m - Epoch:   2 [   19293/  100000], loss: {'classification': 39.5511, 'neural_augmentation': 0.5138, 'total_loss': 40.0648}, LR: [0.000974, 0.000974], Avg. batch load time: 0.113, Elapsed time: 5573.40
2024-07-20 17:08:44 - [34m[1mLOGS   [0m - Epoch:   2 [   19418/  100000], loss: {'classification': 39.5214, 'neural_augmentation': 0.5092, 'total_loss': 40.0306}, LR: [0.000973, 0.000973], Avg. batch load time: 0.113, Elapsed time: 5783.33
2024-07-20 17:12:01 - [34m[1mLOGS   [0m - Epoch:   2 [   19543/  100000], loss: {'classification': 39.4907, 'neural_augmentation': 0.5047, 'total_loss': 39.9954}, LR: [0.000973, 0.000973], Avg. batch load time: 0.112, Elapsed time: 5980.14
2024-07-20 17:15:40 - [34m[1mLOGS   [0m - Epoch:   2 [   19668/  100000], loss: {'classification': 39.4607, 'neural_augmentation': 0.5003, 'total_loss': 39.961}, LR: [0.000972, 0.000972], Avg. batch load time: 0.112, Elapsed time: 6199.02
2024-07-20 17:19:05 - [34m[1mLOGS   [0m - Epoch:   2 [   19793/  100000], loss: {'classification': 39.4303, 'neural_augmentation': 0.4959, 'total_loss': 39.9262}, LR: [0.000971, 0.000971], Avg. batch load time: 0.112, Elapsed time: 6404.24
2024-07-20 17:22:37 - [34m[1mLOGS   [0m - Epoch:   2 [   19918/  100000], loss: {'classification': 39.4016, 'neural_augmentation': 0.4916, 'total_loss': 39.8932}, LR: [0.000971, 0.000971], Avg. batch load time: 0.113, Elapsed time: 6615.77
2024-07-20 17:25:59 - [34m[1mLOGS   [0m - Epoch:   2 [   20043/  100000], loss: {'classification': 39.374, 'neural_augmentation': 0.4875, 'total_loss': 39.8616}, LR: [0.00097, 0.00097], Avg. batch load time: 0.114, Elapsed time: 6818.27
