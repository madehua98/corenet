nohup: ignoring input
2024-07-29 02:03:03 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
small
dci
2024-07-29 02:03:07 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-29 02:03:07 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=7476, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =   29.490 M
Total trainable parameters =   29.490 M

2024-07-29 02:03:08 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-29 02:03:08 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 29.49M                 | 3.389G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  0.116G    |
|   pool.proj                          |   1.18M                |   0.116G   |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.502M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  51.38M    |
|   mlp.0                              |   0.263M               |   25.69M   |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   25.69M   |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  3.835M                |  3.828M    |
|   classifier.weight                  |   (7476, 512)          |            |
|   classifier.bias                    |   (7476,)              |            |
2024-07-29 02:03:08 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-29 02:03:08 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks1.0.ls1', 'blocks1.2.attn.k_norm', 'blocks1.4.drop_path2', 'blocks.0.attn.attn_drop', 'blocks1.4.ls2', 'blocks.0.attn.q_norm', 'blocks1.1.drop_path1', 'patch_embed.backbone.stages.0.0.drop_path', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'patch_embed.backbone.stages.1.3.down', 'blocks1.6.attn.k_norm', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks1.3.attn.q_norm', 'blocks.1.drop_path2', 'blocks1.1.ls2', 'blocks.1.ls1', 'neural_augmentor.noise.max_fn', 'blocks.4.attn.q_norm', 'blocks.1.drop_path1', 'blocks1.4.attn.q_norm', 'blocks1.4.drop_path1', 'patch_embed.backbone.stages.1.0.drop_path', 'neural_augmentor.contrast', 'blocks.3.attn.q_norm', 'blocks.2.drop_path1', 'blocks1.4.ls1', 'patch_embed.backbone.stages.1.1.down', 'blocks.0.attn.k_norm', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'neural_augmentor.brightness.min_fn', 'neural_augmentor', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks.6.drop_path2', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks.5.ls1', 'blocks.6.drop_path1', 'blocks1.1.attn.k_norm', 'blocks1.2.drop_path1', 'blocks.3.attn.k_norm', 'blocks1.3.ls1', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks1.6.attn.q_norm', 'blocks1.6.drop_path1', 'blocks.1.attn.q_norm', 'blocks.4.attn.attn_drop', 'blocks.5.attn.q_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks.6.attn.q_norm', 'patch_embed.backbone.stages.1.2.shortcut', 'patch_embed.backbone.stem.norm1.drop', 'blocks.4.drop_path1', 'blocks.0.ls1', 'blocks.6.ls2', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks.3.attn.attn_drop', 'blocks.0.drop_path2', 'blocks1.5.attn.k_norm', 'norm_pre', 'blocks.4.drop_path2', 'blocks1.6.ls1', 'blocks1.6.drop_path2', 'blocks.6.attn.attn_drop', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks1.0.drop_path1', 'patch_embed.backbone.stages.1.2.down', 'blocks1.2.attn.q_norm', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks1.0.drop_path2', 'blocks.4.attn.k_norm', 'blocks.2.ls1', 'blocks.4.ls2', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks1.2.attn.attn_drop', 'neural_augmentor.noise.min_fn', 'blocks1.1.drop_path2', 'neural_augmentor.contrast.max_fn', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks1.4.attn.k_norm', 'blocks1.2.drop_path2', 'blocks1.1.attn.q_norm', 'blocks1.1.ls1', 'blocks.0.drop_path1', 'blocks1.2.ls2', 'blocks1.5.ls2', 'blocks1.3.attn.k_norm', 'patch_embed.backbone.stages.1.0.down', 'blocks.0.ls2', 'blocks1.5.ls1', 'blocks1.5.drop_path1', 'norm', 'patch_embed.backbone.stages.1.1.shortcut', 'neural_augmentor.noise', 'patch_drop', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks.5.drop_path1', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks.2.ls2', 'blocks1.3.ls2', 'blocks.1.attn.attn_drop', 'blocks1.3.attn.attn_drop', 'blocks1.0.attn.k_norm', 'blocks.2.attn.attn_drop', 'blocks1.2.ls1', 'blocks1.5.drop_path2', 'neural_augmentor.brightness.max_fn', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks.6.attn.k_norm', 'blocks1.3.drop_path2', 'blocks.3.drop_path2', 'blocks.4.ls1', 'blocks1.6.attn.attn_drop', 'neural_augmentor.brightness', 'blocks1.6.ls2', 'blocks.5.drop_path2', 'blocks.5.attn.k_norm', 'blocks.2.attn.q_norm', 'blocks.1.ls2', 'blocks.6.ls1', 'blocks.5.ls2', 'blocks.3.ls2', 'blocks.2.attn.k_norm', 'blocks.1.attn.k_norm', 'blocks1.0.attn.q_norm', 'blocks1.4.attn.attn_drop', 'patch_embed.backbone.stages.0.0.down', 'blocks1.0.ls2', 'blocks1.1.attn.attn_drop', 'patch_embed.backbone.stages.0.1.down', 'blocks.3.drop_path1', 'blocks1.0.attn.attn_drop', 'blocks1.5.attn.attn_drop', 'blocks.5.attn.attn_drop', 'blocks1.5.attn.q_norm', 'blocks1.3.drop_path1', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks.2.drop_path2', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'neural_augmentor.contrast.min_fn', 'blocks.3.ls1', 'patch_embed.proj'}
2024-07-29 02:03:08 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::add_': 14, 'aten::avg_pool2d': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-29 02:03:08 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-29 02:03:08 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-29 02:03:08 - [34m[1mLOGS   [0m - Available GPUs: 8
2024-07-29 02:03:08 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-29 02:03:08 - [34m[1mLOGS   [0m - Directory created at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train
2024-07-29 02:03:12 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:40002
2024-07-29 02:03:16 - [34m[1mLOGS   [0m - Training dataset details are given below
WordnetTaggedClassificationDataset(
	root= 
	is_training=True 
	num_samples=64290000
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	total_tar_files=6429
	max_files_per_tar=10000
	num_synsets=7476
)
2024-07-29 02:03:18 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=True
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=100
	 scales=[(128, 128, 306), (144, 144, 241), (160, 160, 196), (176, 176, 161), (192, 192, 136), (208, 208, 115), (224, 224, 100), (240, 240, 87), (256, 256, 76), (272, 272, 67), (288, 288, 60), (304, 304, 54), (320, 320, 49)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-29 02:03:18 - [34m[1mLOGS   [0m - Number of data workers: 64
small
dci
2024-07-29 02:03:19 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-29 02:03:19 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=7476, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =   29.490 M
Total trainable parameters =   29.490 M

2024-07-29 02:03:20 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-29 02:03:20 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 29.49M                 | 3.389G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  0.116G    |
|   pool.proj                          |   1.18M                |   0.116G   |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.502M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  51.38M    |
|   mlp.0                              |   0.263M               |   25.69M   |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   25.69M   |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  3.835M                |  3.828M    |
|   classifier.weight                  |   (7476, 512)          |            |
|   classifier.bias                    |   (7476,)              |            |
2024-07-29 02:03:20 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-29 02:03:20 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks1.0.ls1', 'blocks1.3.attn.k_norm', 'blocks1.1.attn.k_norm', 'blocks.1.attn.q_norm', 'blocks1.3.ls2', 'blocks1.1.ls2', 'blocks.0.drop_path1', 'blocks1.3.ls1', 'blocks1.6.drop_path2', 'blocks.1.attn.k_norm', 'blocks.3.attn.k_norm', 'blocks1.2.attn.attn_drop', 'blocks.4.ls2', 'blocks1.6.attn.attn_drop', 'patch_embed.backbone.stages.0.0.down', 'blocks.2.attn.k_norm', 'blocks1.6.drop_path1', 'blocks.1.drop_path1', 'blocks1.2.drop_path1', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks.3.attn.attn_drop', 'blocks1.4.drop_path2', 'blocks.0.attn.q_norm', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'patch_embed.backbone.stages.1.1.down', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks.5.drop_path2', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks.3.ls2', 'blocks.0.attn.k_norm', 'blocks1.1.drop_path2', 'blocks.6.drop_path2', 'blocks.0.ls1', 'blocks.6.ls1', 'blocks1.1.drop_path1', 'blocks1.0.drop_path1', 'blocks.1.attn.attn_drop', 'blocks1.0.attn.attn_drop', 'blocks1.2.ls2', 'patch_embed.backbone.stem.norm1.drop', 'blocks1.2.ls1', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks1.0.drop_path2', 'blocks1.4.ls1', 'blocks1.5.attn.q_norm', 'norm_pre', 'blocks1.5.ls1', 'blocks1.4.attn.k_norm', 'neural_augmentor.contrast', 'neural_augmentor', 'neural_augmentor.noise.min_fn', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks.6.attn.attn_drop', 'blocks1.1.attn.attn_drop', 'neural_augmentor.brightness.max_fn', 'blocks1.6.attn.q_norm', 'blocks.5.drop_path1', 'patch_embed.backbone.stages.0.1.down', 'blocks.3.drop_path1', 'blocks.2.drop_path1', 'blocks1.4.attn.attn_drop', 'blocks1.0.attn.k_norm', 'neural_augmentor.contrast.min_fn', 'blocks.2.ls1', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'neural_augmentor.brightness', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks.2.attn.attn_drop', 'blocks.4.attn.k_norm', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks1.4.drop_path1', 'blocks.4.drop_path1', 'blocks.3.attn.q_norm', 'blocks.0.drop_path2', 'blocks.0.ls2', 'neural_augmentor.noise', 'blocks.0.attn.attn_drop', 'blocks1.5.drop_path1', 'blocks1.2.attn.k_norm', 'patch_embed.backbone.stages.1.2.down', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks.4.ls1', 'patch_embed.proj', 'blocks1.0.ls2', 'blocks1.3.drop_path2', 'blocks.4.drop_path2', 'blocks.6.ls2', 'blocks.1.ls2', 'blocks1.4.attn.q_norm', 'blocks1.6.ls2', 'blocks.5.attn.attn_drop', 'blocks1.5.drop_path2', 'patch_embed.backbone.stages.1.3.drop_path', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks.5.attn.k_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks1.0.attn.q_norm', 'patch_embed.backbone.stages.1.1.shortcut', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks1.6.ls1', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks.5.ls2', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'neural_augmentor.noise.max_fn', 'blocks.1.drop_path2', 'blocks1.1.ls1', 'blocks.2.ls2', 'blocks1.5.attn.attn_drop', 'blocks.4.attn.attn_drop', 'blocks1.3.attn.q_norm', 'blocks.2.drop_path2', 'patch_embed.backbone.stages.1.3.down', 'blocks.2.attn.q_norm', 'patch_embed.backbone.stages.1.0.down', 'blocks.6.attn.k_norm', 'blocks1.2.drop_path2', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'patch_drop', 'blocks1.4.ls2', 'blocks.5.attn.q_norm', 'blocks.3.ls1', 'neural_augmentor.contrast.max_fn', 'blocks.5.ls1', 'blocks1.3.attn.attn_drop', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks1.1.attn.q_norm', 'norm', 'blocks.6.attn.q_norm', 'blocks.6.drop_path1', 'blocks1.6.attn.k_norm', 'neural_augmentor.brightness.min_fn', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks1.5.attn.k_norm', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks.4.attn.q_norm', 'blocks1.3.drop_path1', 'blocks1.5.ls2', 'blocks.1.ls1', 'blocks.3.drop_path2', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks1.2.attn.q_norm'}
2024-07-29 02:03:20 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::add_': 14, 'aten::avg_pool2d': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-29 02:03:21 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-29 02:03:21 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-29 02:03:21 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-07-29 02:03:21 - [34m[1mLOGS   [0m - Max. iteration for training: 200000
2024-07-29 02:03:21 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=1e-05
 	 max_lr=0.001
 	 period=180001
 	 warmup_init_lr=1e-06
 	 warmup_iters=20000
 )
2024-07-29 02:03:21 - [34m[1mLOGS   [0m - No checkpoint found at '/ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/training_checkpoint_last.pt'
2024-07-29 02:03:21 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-29 02:03:23 - [32m[1mINFO   [0m - Training epoch 0
2024-07-29 02:03:13 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:40002
small
dci
2024-07-29 02:03:12 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:40002
small
dci
2024-07-29 02:03:12 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:40002
small
dci
2024-07-29 02:03:13 - [32m[1mINFO   [0m - distributed init (rank 5): tcp://localhost:40002
small
dci
2024-07-29 02:03:12 - [32m[1mINFO   [0m - distributed init (rank 4): tcp://localhost:40002
small
dci
2024-07-29 02:03:13 - [32m[1mINFO   [0m - distributed init (rank 6): tcp://localhost:40002
small
dci
2024-07-29 02:03:12 - [32m[1mINFO   [0m - distributed init (rank 7): tcp://localhost:40002
small
dci
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-29 02:07:27 - [34m[1mLOGS   [0m - Epoch:   0 [       0/  200000], loss: {'classification': 5311.6, 'neural_augmentation': 8.3181, 'total_loss': 5319.9181}, LR: [1e-06, 1e-06], Avg. batch load time: 221.414, Elapsed time: 244.47
2024-07-29 02:08:44 - [34m[1mLOGS   [0m - Epoch:   0 [      62/  200000], loss: {'classification': 5087.3308, 'neural_augmentation': 9.2574, 'total_loss': 5096.5882}, LR: [4e-06, 4e-06], Avg. batch load time: 0.446, Elapsed time: 320.89
2024-07-29 02:09:48 - [34m[1mLOGS   [0m - Epoch:   0 [     125/  200000], loss: {'classification': 4764.7095, 'neural_augmentation': 9.3096, 'total_loss': 4774.0191}, LR: [7e-06, 7e-06], Avg. batch load time: 0.223, Elapsed time: 384.55
2024-07-29 02:10:51 - [34m[1mLOGS   [0m - Epoch:   0 [     187/  200000], loss: {'classification': 4349.9012, 'neural_augmentation': 9.326, 'total_loss': 4359.2272}, LR: [1e-05, 1e-05], Avg. batch load time: 0.149, Elapsed time: 447.93
2024-07-29 02:11:54 - [34m[1mLOGS   [0m - Epoch:   0 [     250/  200000], loss: {'classification': 3889.7434, 'neural_augmentation': 9.3332, 'total_loss': 3899.0766}, LR: [1.3e-05, 1.3e-05], Avg. batch load time: 0.112, Elapsed time: 511.38
2024-07-29 02:12:57 - [34m[1mLOGS   [0m - Epoch:   0 [     312/  200000], loss: {'classification': 3458.3043, 'neural_augmentation': 9.3333, 'total_loss': 3467.6376}, LR: [1.7e-05, 1.7e-05], Avg. batch load time: 0.090, Elapsed time: 574.41
2024-07-29 02:14:01 - [34m[1mLOGS   [0m - Epoch:   0 [     375/  200000], loss: {'classification': 3061.2374, 'neural_augmentation': 9.3358, 'total_loss': 3070.5732}, LR: [2e-05, 2e-05], Avg. batch load time: 0.075, Elapsed time: 637.85
2024-07-29 02:15:04 - [34m[1mLOGS   [0m - Epoch:   0 [     437/  200000], loss: {'classification': 2713.5142, 'neural_augmentation': 9.3351, 'total_loss': 2722.8493}, LR: [2.3e-05, 2.3e-05], Avg. batch load time: 0.064, Elapsed time: 701.18
2024-07-29 02:16:07 - [34m[1mLOGS   [0m - Epoch:   0 [     500/  200000], loss: {'classification': 2422.3175, 'neural_augmentation': 9.3276, 'total_loss': 2431.6451}, LR: [2.6e-05, 2.6e-05], Avg. batch load time: 0.056, Elapsed time: 764.48
2024-07-29 02:17:11 - [34m[1mLOGS   [0m - Epoch:   0 [     562/  200000], loss: {'classification': 2167.9611, 'neural_augmentation': 9.3226, 'total_loss': 2177.2837}, LR: [2.9e-05, 2.9e-05], Avg. batch load time: 0.050, Elapsed time: 827.73
2024-07-29 02:18:14 - [34m[1mLOGS   [0m - Epoch:   0 [     625/  200000], loss: {'classification': 1960.1305, 'neural_augmentation': 9.3215, 'total_loss': 1969.452}, LR: [3.2e-05, 3.2e-05], Avg. batch load time: 0.045, Elapsed time: 891.15
2024-07-29 02:19:22 - [34m[1mLOGS   [0m - Epoch:   0 [     687/  200000], loss: {'classification': 1780.964, 'neural_augmentation': 9.3195, 'total_loss': 1790.2835}, LR: [3.5e-05, 3.5e-05], Avg. batch load time: 0.041, Elapsed time: 959.29
2024-07-29 02:20:26 - [34m[1mLOGS   [0m - Epoch:   0 [     750/  200000], loss: {'classification': 1633.7201, 'neural_augmentation': 9.3174, 'total_loss': 1643.0375}, LR: [3.8e-05, 3.8e-05], Avg. batch load time: 0.038, Elapsed time: 1022.86
2024-07-29 02:21:30 - [34m[1mLOGS   [0m - Epoch:   0 [     812/  200000], loss: {'classification': 1506.867, 'neural_augmentation': 9.3213, 'total_loss': 1516.1882}, LR: [4.2e-05, 4.2e-05], Avg. batch load time: 0.035, Elapsed time: 1086.57
2024-07-29 02:22:33 - [34m[1mLOGS   [0m - Epoch:   0 [     875/  200000], loss: {'classification': 1399.9534, 'neural_augmentation': 9.3153, 'total_loss': 1409.2687}, LR: [4.5e-05, 4.5e-05], Avg. batch load time: 0.033, Elapsed time: 1149.99
2024-07-29 02:23:37 - [34m[1mLOGS   [0m - Epoch:   0 [     937/  200000], loss: {'classification': 1304.5201, 'neural_augmentation': 9.3093, 'total_loss': 1313.8293}, LR: [4.8e-05, 4.8e-05], Avg. batch load time: 0.030, Elapsed time: 1213.75
2024-07-29 02:24:40 - [34m[1mLOGS   [0m - Epoch:   0 [    1000/  200000], loss: {'classification': 1224.5679, 'neural_augmentation': 9.3057, 'total_loss': 1233.8736}, LR: [5.1e-05, 5.1e-05], Avg. batch load time: 0.029, Elapsed time: 1277.15
2024-07-29 02:25:43 - [34m[1mLOGS   [0m - Epoch:   0 [    1062/  200000], loss: {'classification': 1155.2918, 'neural_augmentation': 9.2991, 'total_loss': 1164.5909}, LR: [5.4e-05, 5.4e-05], Avg. batch load time: 0.027, Elapsed time: 1340.32
2024-07-29 02:26:47 - [34m[1mLOGS   [0m - Epoch:   0 [    1125/  200000], loss: {'classification': 1092.6559, 'neural_augmentation': 9.2945, 'total_loss': 1101.9504}, LR: [5.7e-05, 5.7e-05], Avg. batch load time: 0.025, Elapsed time: 1403.57
2024-07-29 02:27:50 - [34m[1mLOGS   [0m - Epoch:   0 [    1187/  200000], loss: {'classification': 1034.727, 'neural_augmentation': 9.2885, 'total_loss': 1044.0156}, LR: [6e-05, 6e-05], Avg. batch load time: 0.024, Elapsed time: 1467.14
2024-07-29 02:28:53 - [34m[1mLOGS   [0m - Epoch:   0 [    1250/  200000], loss: {'classification': 984.2453, 'neural_augmentation': 9.2836, 'total_loss': 993.5289}, LR: [6.3e-05, 6.3e-05], Avg. batch load time: 0.023, Elapsed time: 1530.44
2024-07-29 02:29:57 - [34m[1mLOGS   [0m - Epoch:   0 [    1312/  200000], loss: {'classification': 939.7816, 'neural_augmentation': 9.2812, 'total_loss': 949.0628}, LR: [6.7e-05, 6.7e-05], Avg. batch load time: 0.022, Elapsed time: 1593.65
2024-07-29 02:31:00 - [34m[1mLOGS   [0m - Epoch:   0 [    1375/  200000], loss: {'classification': 899.1863, 'neural_augmentation': 9.2755, 'total_loss': 908.4617}, LR: [7e-05, 7e-05], Avg. batch load time: 0.021, Elapsed time: 1656.83
2024-07-29 02:32:03 - [34m[1mLOGS   [0m - Epoch:   0 [    1437/  200000], loss: {'classification': 860.9067, 'neural_augmentation': 9.2705, 'total_loss': 870.1771}, LR: [7.3e-05, 7.3e-05], Avg. batch load time: 0.020, Elapsed time: 1719.95
2024-07-29 02:33:13 - [34m[1mLOGS   [0m - Epoch:   0 [    1500/  200000], loss: {'classification': 826.2979, 'neural_augmentation': 9.2671, 'total_loss': 835.5651}, LR: [7.6e-05, 7.6e-05], Avg. batch load time: 0.019, Elapsed time: 1789.56
2024-07-29 02:34:16 - [34m[1mLOGS   [0m - Epoch:   0 [    1562/  200000], loss: {'classification': 796.099, 'neural_augmentation': 9.2661, 'total_loss': 805.3651}, LR: [7.9e-05, 7.9e-05], Avg. batch load time: 0.019, Elapsed time: 1852.59
2024-07-29 02:35:19 - [34m[1mLOGS   [0m - Epoch:   0 [    1625/  200000], loss: {'classification': 766.2445, 'neural_augmentation': 9.2637, 'total_loss': 775.5082}, LR: [8.2e-05, 8.2e-05], Avg. batch load time: 0.018, Elapsed time: 1915.60
2024-07-29 02:36:22 - [34m[1mLOGS   [0m - Epoch:   0 [    1687/  200000], loss: {'classification': 738.804, 'neural_augmentation': 9.2603, 'total_loss': 748.0643}, LR: [8.5e-05, 8.5e-05], Avg. batch load time: 0.017, Elapsed time: 1978.85
2024-07-29 02:37:25 - [34m[1mLOGS   [0m - Epoch:   0 [    1750/  200000], loss: {'classification': 713.453, 'neural_augmentation': 9.2583, 'total_loss': 722.7114}, LR: [8.8e-05, 8.8e-05], Avg. batch load time: 0.017, Elapsed time: 2042.05
2024-07-29 02:38:28 - [34m[1mLOGS   [0m - Epoch:   0 [    1812/  200000], loss: {'classification': 691.0191, 'neural_augmentation': 9.2533, 'total_loss': 700.2724}, LR: [9.2e-05, 9.2e-05], Avg. batch load time: 0.016, Elapsed time: 2105.15
2024-07-29 02:39:31 - [34m[1mLOGS   [0m - Epoch:   0 [    1875/  200000], loss: {'classification': 668.0981, 'neural_augmentation': 9.251, 'total_loss': 677.3491}, LR: [9.5e-05, 9.5e-05], Avg. batch load time: 0.016, Elapsed time: 2168.43
2024-07-29 02:40:34 - [34m[1mLOGS   [0m - Epoch:   0 [    1937/  200000], loss: {'classification': 647.7723, 'neural_augmentation': 9.2478, 'total_loss': 657.0202}, LR: [9.8e-05, 9.8e-05], Avg. batch load time: 0.015, Elapsed time: 2231.36
2024-07-29 02:41:38 - [34m[1mLOGS   [0m - Epoch:   0 [    2000/  200000], loss: {'classification': 627.6633, 'neural_augmentation': 9.2454, 'total_loss': 636.9087}, LR: [0.000101, 0.000101], Avg. batch load time: 0.015, Elapsed time: 2294.81
2024-07-29 02:42:41 - [34m[1mLOGS   [0m - Epoch:   0 [    2062/  200000], loss: {'classification': 608.4215, 'neural_augmentation': 9.2413, 'total_loss': 617.6628}, LR: [0.000104, 0.000104], Avg. batch load time: 0.014, Elapsed time: 2358.07
2024-07-29 02:43:44 - [34m[1mLOGS   [0m - Epoch:   0 [    2125/  200000], loss: {'classification': 591.3979, 'neural_augmentation': 9.2357, 'total_loss': 600.6336}, LR: [0.000107, 0.000107], Avg. batch load time: 0.014, Elapsed time: 2421.19
2024-07-29 02:44:47 - [34m[1mLOGS   [0m - Epoch:   0 [    2187/  200000], loss: {'classification': 575.3291, 'neural_augmentation': 9.233, 'total_loss': 584.5621}, LR: [0.00011, 0.00011], Avg. batch load time: 0.013, Elapsed time: 2484.10
2024-07-29 02:45:56 - [34m[1mLOGS   [0m - Epoch:   0 [    2250/  200000], loss: {'classification': 560.3875, 'neural_augmentation': 9.231, 'total_loss': 569.6185}, LR: [0.000113, 0.000113], Avg. batch load time: 0.013, Elapsed time: 2553.37
2024-07-29 02:47:00 - [34m[1mLOGS   [0m - Epoch:   0 [    2312/  200000], loss: {'classification': 546.295, 'neural_augmentation': 9.2269, 'total_loss': 555.5219}, LR: [0.000116, 0.000116], Avg. batch load time: 0.013, Elapsed time: 2616.55
2024-07-29 02:48:03 - [34m[1mLOGS   [0m - Epoch:   0 [    2375/  200000], loss: {'classification': 532.5469, 'neural_augmentation': 9.2208, 'total_loss': 541.7676}, LR: [0.00012, 0.00012], Avg. batch load time: 0.012, Elapsed time: 2679.69
2024-07-29 02:49:06 - [34m[1mLOGS   [0m - Epoch:   0 [    2437/  200000], loss: {'classification': 519.3092, 'neural_augmentation': 9.2149, 'total_loss': 528.5241}, LR: [0.000123, 0.000123], Avg. batch load time: 0.012, Elapsed time: 2743.02
2024-07-29 02:50:09 - [34m[1mLOGS   [0m - Epoch:   0 [    2500/  200000], loss: {'classification': 507.7861, 'neural_augmentation': 9.2072, 'total_loss': 516.9932}, LR: [0.000126, 0.000126], Avg. batch load time: 0.012, Elapsed time: 2806.15
2024-07-29 02:51:12 - [34m[1mLOGS   [0m - Epoch:   0 [    2562/  200000], loss: {'classification': 495.5443, 'neural_augmentation': 9.2028, 'total_loss': 504.7471}, LR: [0.000129, 0.000129], Avg. batch load time: 0.012, Elapsed time: 2869.41
2024-07-29 02:52:15 - [34m[1mLOGS   [0m - Epoch:   0 [    2625/  200000], loss: {'classification': 484.1455, 'neural_augmentation': 9.2002, 'total_loss': 493.3457}, LR: [0.000132, 0.000132], Avg. batch load time: 0.011, Elapsed time: 2932.52
2024-07-29 02:53:19 - [34m[1mLOGS   [0m - Epoch:   0 [    2687/  200000], loss: {'classification': 473.6563, 'neural_augmentation': 9.1988, 'total_loss': 482.855}, LR: [0.000135, 0.000135], Avg. batch load time: 0.011, Elapsed time: 2995.75
2024-07-29 02:54:22 - [34m[1mLOGS   [0m - Epoch:   0 [    2750/  200000], loss: {'classification': 463.5346, 'neural_augmentation': 9.1985, 'total_loss': 472.733}, LR: [0.000138, 0.000138], Avg. batch load time: 0.011, Elapsed time: 3058.84
2024-07-29 02:55:25 - [34m[1mLOGS   [0m - Epoch:   0 [    2812/  200000], loss: {'classification': 453.7903, 'neural_augmentation': 9.1976, 'total_loss': 462.988}, LR: [0.000141, 0.000141], Avg. batch load time: 0.011, Elapsed time: 3122.09
2024-07-29 02:56:28 - [34m[1mLOGS   [0m - Epoch:   0 [    2875/  200000], loss: {'classification': 444.5558, 'neural_augmentation': 9.1961, 'total_loss': 453.7519}, LR: [0.000145, 0.000145], Avg. batch load time: 0.010, Elapsed time: 3185.38
2024-07-29 02:57:32 - [34m[1mLOGS   [0m - Epoch:   0 [    2937/  200000], loss: {'classification': 435.8411, 'neural_augmentation': 9.1927, 'total_loss': 445.0338}, LR: [0.000148, 0.000148], Avg. batch load time: 0.010, Elapsed time: 3248.70
2024-07-29 02:58:42 - [34m[1mLOGS   [0m - Epoch:   0 [    3000/  200000], loss: {'classification': 427.4568, 'neural_augmentation': 9.1905, 'total_loss': 436.6473}, LR: [0.000151, 0.000151], Avg. batch load time: 0.010, Elapsed time: 3319.40
2024-07-29 02:59:46 - [34m[1mLOGS   [0m - Epoch:   0 [    3062/  200000], loss: {'classification': 419.3327, 'neural_augmentation': 9.1879, 'total_loss': 428.5207}, LR: [0.000154, 0.000154], Avg. batch load time: 0.010, Elapsed time: 3382.67
2024-07-29 03:00:49 - [34m[1mLOGS   [0m - Epoch:   0 [    3125/  200000], loss: {'classification': 411.8595, 'neural_augmentation': 9.1854, 'total_loss': 421.0449}, LR: [0.000157, 0.000157], Avg. batch load time: 0.010, Elapsed time: 3445.91
2024-07-29 03:01:52 - [34m[1mLOGS   [0m - Epoch:   0 [    3187/  200000], loss: {'classification': 404.3527, 'neural_augmentation': 9.1821, 'total_loss': 413.5349}, LR: [0.00016, 0.00016], Avg. batch load time: 0.009, Elapsed time: 3509.25
2024-07-29 03:02:55 - [34m[1mLOGS   [0m - Epoch:   0 [    3250/  200000], loss: {'classification': 396.9403, 'neural_augmentation': 9.1801, 'total_loss': 406.1203}, LR: [0.000163, 0.000163], Avg. batch load time: 0.009, Elapsed time: 3572.45
2024-07-29 03:03:59 - [34m[1mLOGS   [0m - Epoch:   0 [    3312/  200000], loss: {'classification': 389.6357, 'neural_augmentation': 9.1779, 'total_loss': 398.8136}, LR: [0.000166, 0.000166], Avg. batch load time: 0.009, Elapsed time: 3636.12
2024-07-29 03:05:02 - [34m[1mLOGS   [0m - Epoch:   0 [    3375/  200000], loss: {'classification': 382.8753, 'neural_augmentation': 9.173, 'total_loss': 392.0483}, LR: [0.00017, 0.00017], Avg. batch load time: 0.009, Elapsed time: 3699.48
2024-07-29 03:06:06 - [34m[1mLOGS   [0m - Epoch:   0 [    3437/  200000], loss: {'classification': 376.3706, 'neural_augmentation': 9.1709, 'total_loss': 385.5415}, LR: [0.000173, 0.000173], Avg. batch load time: 0.009, Elapsed time: 3762.80
2024-07-29 03:07:09 - [34m[1mLOGS   [0m - Epoch:   0 [    3500/  200000], loss: {'classification': 369.7269, 'neural_augmentation': 9.1676, 'total_loss': 378.8945}, LR: [0.000176, 0.000176], Avg. batch load time: 0.009, Elapsed time: 3826.30
2024-07-29 03:08:13 - [34m[1mLOGS   [0m - Epoch:   0 [    3562/  200000], loss: {'classification': 363.7108, 'neural_augmentation': 9.1629, 'total_loss': 372.8737}, LR: [0.000179, 0.000179], Avg. batch load time: 0.009, Elapsed time: 3889.63
2024-07-29 03:10:35 - [34m[1mLOGS   [0m - Epoch:   0 [    3625/  200000], loss: {'classification': 357.6886, 'neural_augmentation': 9.1582, 'total_loss': 366.8468}, LR: [0.000182, 0.000182], Avg. batch load time: 0.010, Elapsed time: 4031.81
2024-07-29 03:12:50 - [34m[1mLOGS   [0m - Epoch:   0 [    3687/  200000], loss: {'classification': 352.0026, 'neural_augmentation': 9.152, 'total_loss': 361.1546}, LR: [0.000185, 0.000185], Avg. batch load time: 0.012, Elapsed time: 4166.67
2024-07-29 03:15:20 - [34m[1mLOGS   [0m - Epoch:   0 [    3750/  200000], loss: {'classification': 346.2141, 'neural_augmentation': 9.1451, 'total_loss': 355.3592}, LR: [0.000188, 0.000188], Avg. batch load time: 0.014, Elapsed time: 4316.94
2024-07-29 03:17:34 - [34m[1mLOGS   [0m - Epoch:   0 [    3812/  200000], loss: {'classification': 340.9481, 'neural_augmentation': 9.1386, 'total_loss': 350.0867}, LR: [0.000191, 0.000191], Avg. batch load time: 0.016, Elapsed time: 4451.09
2024-07-29 03:19:56 - [34m[1mLOGS   [0m - Epoch:   0 [    3875/  200000], loss: {'classification': 335.6416, 'neural_augmentation': 9.1317, 'total_loss': 344.7733}, LR: [0.000195, 0.000195], Avg. batch load time: 0.017, Elapsed time: 4593.32
2024-07-29 03:22:18 - [34m[1mLOGS   [0m - Epoch:   0 [    3937/  200000], loss: {'classification': 330.6233, 'neural_augmentation': 9.1252, 'total_loss': 339.7485}, LR: [0.000198, 0.000198], Avg. batch load time: 0.019, Elapsed time: 4735.06
2024-07-29 03:24:36 - [34m[1mLOGS   [0m - Epoch:   0 [    4000/  200000], loss: {'classification': 325.7257, 'neural_augmentation': 9.1178, 'total_loss': 334.8435}, LR: [0.000201, 0.000201], Avg. batch load time: 0.020, Elapsed time: 4873.15
2024-07-29 03:26:55 - [34m[1mLOGS   [0m - Epoch:   0 [    4062/  200000], loss: {'classification': 321.199, 'neural_augmentation': 9.1112, 'total_loss': 330.3102}, LR: [0.000204, 0.000204], Avg. batch load time: 0.022, Elapsed time: 5012.46
2024-07-29 03:29:14 - [34m[1mLOGS   [0m - Epoch:   0 [    4125/  200000], loss: {'classification': 316.6547, 'neural_augmentation': 9.1063, 'total_loss': 325.761}, LR: [0.000207, 0.000207], Avg. batch load time: 0.024, Elapsed time: 5151.17
2024-07-29 03:31:34 - [34m[1mLOGS   [0m - Epoch:   0 [    4187/  200000], loss: {'classification': 312.1266, 'neural_augmentation': 9.1003, 'total_loss': 321.227}, LR: [0.00021, 0.00021], Avg. batch load time: 0.025, Elapsed time: 5291.53
2024-07-29 03:33:51 - [34m[1mLOGS   [0m - Epoch:   0 [    4250/  200000], loss: {'classification': 308.0035, 'neural_augmentation': 9.0946, 'total_loss': 317.0981}, LR: [0.000213, 0.000213], Avg. batch load time: 0.026, Elapsed time: 5427.59
2024-07-29 03:36:04 - [34m[1mLOGS   [0m - Epoch:   0 [    4312/  200000], loss: {'classification': 303.915, 'neural_augmentation': 9.0889, 'total_loss': 313.004}, LR: [0.000216, 0.000216], Avg. batch load time: 0.028, Elapsed time: 5560.71
2024-07-29 03:38:24 - [34m[1mLOGS   [0m - Epoch:   0 [    4375/  200000], loss: {'classification': 300.0472, 'neural_augmentation': 9.0832, 'total_loss': 309.1303}, LR: [0.00022, 0.00022], Avg. batch load time: 0.029, Elapsed time: 5700.84
2024-07-29 03:40:34 - [34m[1mLOGS   [0m - Epoch:   0 [    4437/  200000], loss: {'classification': 296.2685, 'neural_augmentation': 9.0774, 'total_loss': 305.3459}, LR: [0.000223, 0.000223], Avg. batch load time: 0.030, Elapsed time: 5830.82
2024-07-29 03:42:42 - [34m[1mLOGS   [0m - Epoch:   0 [    4500/  200000], loss: {'classification': 292.694, 'neural_augmentation': 9.0717, 'total_loss': 301.7657}, LR: [0.000226, 0.000226], Avg. batch load time: 0.031, Elapsed time: 5959.23
2024-07-29 03:45:12 - [34m[1mLOGS   [0m - Epoch:   0 [    4562/  200000], loss: {'classification': 288.8964, 'neural_augmentation': 9.0654, 'total_loss': 297.9618}, LR: [0.000229, 0.000229], Avg. batch load time: 0.032, Elapsed time: 6109.20
2024-07-29 03:47:26 - [34m[1mLOGS   [0m - Epoch:   0 [    4625/  200000], loss: {'classification': 285.2734, 'neural_augmentation': 9.0588, 'total_loss': 294.3322}, LR: [0.000232, 0.000232], Avg. batch load time: 0.033, Elapsed time: 6243.19
2024-07-29 03:49:36 - [34m[1mLOGS   [0m - Epoch:   0 [    4687/  200000], loss: {'classification': 281.9769, 'neural_augmentation': 9.0521, 'total_loss': 291.029}, LR: [0.000235, 0.000235], Avg. batch load time: 0.034, Elapsed time: 6372.96
2024-07-29 03:51:52 - [34m[1mLOGS   [0m - Epoch:   0 [    4750/  200000], loss: {'classification': 278.6303, 'neural_augmentation': 9.0461, 'total_loss': 287.6764}, LR: [0.000238, 0.000238], Avg. batch load time: 0.035, Elapsed time: 6508.81
2024-07-29 03:54:05 - [34m[1mLOGS   [0m - Epoch:   0 [    4812/  200000], loss: {'classification': 275.4762, 'neural_augmentation': 9.0403, 'total_loss': 284.5165}, LR: [0.000241, 0.000241], Avg. batch load time: 0.036, Elapsed time: 6641.70
2024-07-29 03:56:23 - [34m[1mLOGS   [0m - Epoch:   0 [    4875/  200000], loss: {'classification': 272.1509, 'neural_augmentation': 9.0333, 'total_loss': 281.1842}, LR: [0.000245, 0.000245], Avg. batch load time: 0.037, Elapsed time: 6779.55
2024-07-29 03:58:45 - [34m[1mLOGS   [0m - Epoch:   0 [    4937/  200000], loss: {'classification': 269.0708, 'neural_augmentation': 9.0267, 'total_loss': 278.0975}, LR: [0.000248, 0.000248], Avg. batch load time: 0.038, Elapsed time: 6922.33
2024-07-29 04:01:00 - [34m[1mLOGS   [0m - Epoch:   0 [    5000/  200000], loss: {'classification': 265.9962, 'neural_augmentation': 9.0194, 'total_loss': 275.0156}, LR: [0.000251, 0.000251], Avg. batch load time: 0.039, Elapsed time: 7057.35
2024-07-29 04:03:13 - [34m[1mLOGS   [0m - Epoch:   0 [    5062/  200000], loss: {'classification': 263.1516, 'neural_augmentation': 9.0136, 'total_loss': 272.1652}, LR: [0.000254, 0.000254], Avg. batch load time: 0.040, Elapsed time: 7190.19
2024-07-29 04:05:36 - [34m[1mLOGS   [0m - Epoch:   0 [    5125/  200000], loss: {'classification': 260.2486, 'neural_augmentation': 9.0072, 'total_loss': 269.2557}, LR: [0.000257, 0.000257], Avg. batch load time: 0.041, Elapsed time: 7333.16
2024-07-29 04:07:49 - [34m[1mLOGS   [0m - Epoch:   0 [    5187/  200000], loss: {'classification': 257.4535, 'neural_augmentation': 9.0003, 'total_loss': 266.4538}, LR: [0.00026, 0.00026], Avg. batch load time: 0.042, Elapsed time: 7465.86
2024-07-29 04:10:01 - [34m[1mLOGS   [0m - Epoch:   0 [    5250/  200000], loss: {'classification': 254.7613, 'neural_augmentation': 8.9928, 'total_loss': 263.7541}, LR: [0.000263, 0.000263], Avg. batch load time: 0.043, Elapsed time: 7598.37
2024-07-29 04:12:21 - [34m[1mLOGS   [0m - Epoch:   0 [    5312/  200000], loss: {'classification': 252.0396, 'neural_augmentation': 8.985, 'total_loss': 261.0246}, LR: [0.000266, 0.000266], Avg. batch load time: 0.044, Elapsed time: 7737.73
2024-07-29 04:14:34 - [34m[1mLOGS   [0m - Epoch:   0 [    5375/  200000], loss: {'classification': 249.6136, 'neural_augmentation': 8.9783, 'total_loss': 258.5919}, LR: [0.000269, 0.000269], Avg. batch load time: 0.045, Elapsed time: 7871.16
2024-07-29 04:16:52 - [34m[1mLOGS   [0m - Epoch:   0 [    5437/  200000], loss: {'classification': 246.9529, 'neural_augmentation': 8.9704, 'total_loss': 255.9233}, LR: [0.000273, 0.000273], Avg. batch load time: 0.046, Elapsed time: 8008.82
2024-07-29 04:19:08 - [34m[1mLOGS   [0m - Epoch:   0 [    5500/  200000], loss: {'classification': 244.5606, 'neural_augmentation': 8.9629, 'total_loss': 253.5235}, LR: [0.000276, 0.000276], Avg. batch load time: 0.046, Elapsed time: 8145.09
2024-07-29 04:21:26 - [34m[1mLOGS   [0m - Epoch:   0 [    5562/  200000], loss: {'classification': 242.0603, 'neural_augmentation': 8.9549, 'total_loss': 251.0152}, LR: [0.000279, 0.000279], Avg. batch load time: 0.047, Elapsed time: 8282.95
2024-07-29 04:23:44 - [34m[1mLOGS   [0m - Epoch:   0 [    5625/  200000], loss: {'classification': 239.6805, 'neural_augmentation': 8.9467, 'total_loss': 248.6272}, LR: [0.000282, 0.000282], Avg. batch load time: 0.048, Elapsed time: 8421.32
2024-07-29 04:26:03 - [34m[1mLOGS   [0m - Epoch:   0 [    5687/  200000], loss: {'classification': 237.2647, 'neural_augmentation': 8.9379, 'total_loss': 246.2026}, LR: [0.000285, 0.000285], Avg. batch load time: 0.049, Elapsed time: 8560.51
2024-07-29 04:28:19 - [34m[1mLOGS   [0m - Epoch:   0 [    5750/  200000], loss: {'classification': 235.0131, 'neural_augmentation': 8.9291, 'total_loss': 243.9422}, LR: [0.000288, 0.000288], Avg. batch load time: 0.049, Elapsed time: 8696.29
2024-07-29 04:30:35 - [34m[1mLOGS   [0m - Epoch:   0 [    5812/  200000], loss: {'classification': 232.83, 'neural_augmentation': 8.9205, 'total_loss': 241.7505}, LR: [0.000291, 0.000291], Avg. batch load time: 0.050, Elapsed time: 8832.00
2024-07-29 04:32:59 - [34m[1mLOGS   [0m - Epoch:   0 [    5875/  200000], loss: {'classification': 230.5594, 'neural_augmentation': 8.9106, 'total_loss': 239.4699}, LR: [0.000294, 0.000294], Avg. batch load time: 0.051, Elapsed time: 8975.57
2024-07-29 04:35:16 - [34m[1mLOGS   [0m - Epoch:   0 [    5937/  200000], loss: {'classification': 228.4514, 'neural_augmentation': 8.9016, 'total_loss': 237.353}, LR: [0.000298, 0.000298], Avg. batch load time: 0.052, Elapsed time: 9113.03
2024-07-29 04:37:39 - [34m[1mLOGS   [0m - Epoch:   0 [    6000/  200000], loss: {'classification': 226.3241, 'neural_augmentation': 8.8919, 'total_loss': 235.216}, LR: [0.000301, 0.000301], Avg. batch load time: 0.053, Elapsed time: 9255.77
2024-07-29 04:39:57 - [34m[1mLOGS   [0m - Epoch:   0 [    6062/  200000], loss: {'classification': 224.2401, 'neural_augmentation': 8.8822, 'total_loss': 233.1223}, LR: [0.000304, 0.000304], Avg. batch load time: 0.053, Elapsed time: 9393.75
2024-07-29 04:42:22 - [34m[1mLOGS   [0m - Epoch:   0 [    6125/  200000], loss: {'classification': 222.1672, 'neural_augmentation': 8.8722, 'total_loss': 231.0394}, LR: [0.000307, 0.000307], Avg. batch load time: 0.054, Elapsed time: 9539.45
2024-07-29 04:44:31 - [34m[1mLOGS   [0m - Epoch:   0 [    6187/  200000], loss: {'classification': 220.2929, 'neural_augmentation': 8.8623, 'total_loss': 229.1552}, LR: [0.00031, 0.00031], Avg. batch load time: 0.055, Elapsed time: 9668.21
2024-07-29 04:46:53 - [34m[1mLOGS   [0m - Epoch:   0 [    6250/  200000], loss: {'classification': 219.0295, 'neural_augmentation': 8.8548, 'total_loss': 227.8843}, LR: [0.000313, 0.000313], Avg. batch load time: 0.056, Elapsed time: 9810.41
2024-07-29 04:49:11 - [34m[1mLOGS   [0m - Epoch:   0 [    6312/  200000], loss: {'classification': 217.9081, 'neural_augmentation': 8.8477, 'total_loss': 226.7558}, LR: [0.000316, 0.000316], Avg. batch load time: 0.056, Elapsed time: 9948.04
2024-07-29 04:51:23 - [34m[1mLOGS   [0m - Epoch:   0 [    6375/  200000], loss: {'classification': 216.7961, 'neural_augmentation': 8.8409, 'total_loss': 225.6371}, LR: [0.000319, 0.000319], Avg. batch load time: 0.057, Elapsed time: 10079.74
2024-07-29 04:53:43 - [34m[1mLOGS   [0m - Epoch:   0 [    6437/  200000], loss: {'classification': 215.6792, 'neural_augmentation': 8.8336, 'total_loss': 224.5128}, LR: [0.000323, 0.000323], Avg. batch load time: 0.057, Elapsed time: 10219.80
2024-07-29 04:56:08 - [34m[1mLOGS   [0m - Epoch:   0 [    6500/  200000], loss: {'classification': 214.6022, 'neural_augmentation': 8.826, 'total_loss': 223.4282}, LR: [0.000326, 0.000326], Avg. batch load time: 0.058, Elapsed time: 10365.10
2024-07-29 04:58:17 - [34m[1mLOGS   [0m - Epoch:   0 [    6562/  200000], loss: {'classification': 213.544, 'neural_augmentation': 8.8187, 'total_loss': 222.3627}, LR: [0.000329, 0.000329], Avg. batch load time: 0.058, Elapsed time: 10494.00
2024-07-29 05:00:39 - [34m[1mLOGS   [0m - Epoch:   0 [    6625/  200000], loss: {'classification': 212.4868, 'neural_augmentation': 8.8108, 'total_loss': 221.2976}, LR: [0.000332, 0.000332], Avg. batch load time: 0.059, Elapsed time: 10636.39
2024-07-29 05:02:54 - [34m[1mLOGS   [0m - Epoch:   0 [    6687/  200000], loss: {'classification': 211.4515, 'neural_augmentation': 8.8028, 'total_loss': 220.2543}, LR: [0.000335, 0.000335], Avg. batch load time: 0.060, Elapsed time: 10771.03
2024-07-29 05:05:09 - [34m[1mLOGS   [0m - Epoch:   0 [    6750/  200000], loss: {'classification': 210.4785, 'neural_augmentation': 8.7949, 'total_loss': 219.2734}, LR: [0.000338, 0.000338], Avg. batch load time: 0.060, Elapsed time: 10906.02
2024-07-29 05:07:27 - [34m[1mLOGS   [0m - Epoch:   0 [    6812/  200000], loss: {'classification': 209.4982, 'neural_augmentation': 8.7864, 'total_loss': 218.2847}, LR: [0.000341, 0.000341], Avg. batch load time: 0.061, Elapsed time: 11044.50
2024-07-29 05:09:50 - [34m[1mLOGS   [0m - Epoch:   0 [    6875/  200000], loss: {'classification': 208.4801, 'neural_augmentation': 8.777, 'total_loss': 217.2571}, LR: [0.000344, 0.000344], Avg. batch load time: 0.061, Elapsed time: 11186.75
2024-07-29 05:12:08 - [34m[1mLOGS   [0m - Epoch:   0 [    6937/  200000], loss: {'classification': 207.5341, 'neural_augmentation': 8.768, 'total_loss': 216.3021}, LR: [0.000347, 0.000347], Avg. batch load time: 0.062, Elapsed time: 11324.57
2024-07-29 05:14:21 - [34m[1mLOGS   [0m - Epoch:   0 [    7000/  200000], loss: {'classification': 206.6128, 'neural_augmentation': 8.7592, 'total_loss': 215.372}, LR: [0.000351, 0.000351], Avg. batch load time: 0.062, Elapsed time: 11457.75
2024-07-29 05:16:39 - [34m[1mLOGS   [0m - Epoch:   0 [    7062/  200000], loss: {'classification': 205.6875, 'neural_augmentation': 8.7495, 'total_loss': 214.437}, LR: [0.000354, 0.000354], Avg. batch load time: 0.063, Elapsed time: 11595.65
2024-07-29 05:19:08 - [34m[1mLOGS   [0m - Epoch:   0 [    7125/  200000], loss: {'classification': 204.6955, 'neural_augmentation': 8.7385, 'total_loss': 213.434}, LR: [0.000357, 0.000357], Avg. batch load time: 0.064, Elapsed time: 11745.02
2024-07-29 05:21:25 - [34m[1mLOGS   [0m - Epoch:   0 [    7187/  200000], loss: {'classification': 203.8183, 'neural_augmentation': 8.7285, 'total_loss': 212.5468}, LR: [0.00036, 0.00036], Avg. batch load time: 0.064, Elapsed time: 11881.86
2024-07-29 05:23:47 - [34m[1mLOGS   [0m - Epoch:   0 [    7250/  200000], loss: {'classification': 202.8973, 'neural_augmentation': 8.7173, 'total_loss': 211.6146}, LR: [0.000363, 0.000363], Avg. batch load time: 0.065, Elapsed time: 12024.34
2024-07-29 05:26:06 - [34m[1mLOGS   [0m - Epoch:   0 [    7312/  200000], loss: {'classification': 202.0159, 'neural_augmentation': 8.7059, 'total_loss': 210.7218}, LR: [0.000366, 0.000366], Avg. batch load time: 0.065, Elapsed time: 12163.48
2024-07-29 05:28:34 - [34m[1mLOGS   [0m - Epoch:   0 [    7375/  200000], loss: {'classification': 201.133, 'neural_augmentation': 8.6942, 'total_loss': 209.8272}, LR: [0.000369, 0.000369], Avg. batch load time: 0.066, Elapsed time: 12311.18
2024-07-29 05:30:50 - [34m[1mLOGS   [0m - Epoch:   0 [    7437/  200000], loss: {'classification': 200.2947, 'neural_augmentation': 8.683, 'total_loss': 208.9777}, LR: [0.000372, 0.000372], Avg. batch load time: 0.066, Elapsed time: 12447.43
2024-07-29 05:33:09 - [34m[1mLOGS   [0m - Epoch:   0 [    7500/  200000], loss: {'classification': 199.4525, 'neural_augmentation': 8.6708, 'total_loss': 208.1233}, LR: [0.000376, 0.000376], Avg. batch load time: 0.066, Elapsed time: 12586.21
2024-07-29 05:35:27 - [34m[1mLOGS   [0m - Epoch:   0 [    7562/  200000], loss: {'classification': 198.639, 'neural_augmentation': 8.6584, 'total_loss': 207.2974}, LR: [0.000379, 0.000379], Avg. batch load time: 0.067, Elapsed time: 12724.32
2024-07-29 05:37:49 - [34m[1mLOGS   [0m - Epoch:   0 [    7625/  200000], loss: {'classification': 197.8457, 'neural_augmentation': 8.6456, 'total_loss': 206.4913}, LR: [0.000382, 0.000382], Avg. batch load time: 0.067, Elapsed time: 12865.85
2024-07-29 05:40:11 - [34m[1mLOGS   [0m - Epoch:   0 [    7687/  200000], loss: {'classification': 197.0256, 'neural_augmentation': 8.6322, 'total_loss': 205.6578}, LR: [0.000385, 0.000385], Avg. batch load time: 0.068, Elapsed time: 13007.60
2024-07-29 05:42:32 - [34m[1mLOGS   [0m - Epoch:   0 [    7750/  200000], loss: {'classification': 196.237, 'neural_augmentation': 8.6187, 'total_loss': 204.8557}, LR: [0.000388, 0.000388], Avg. batch load time: 0.069, Elapsed time: 13148.89
2024-07-29 05:44:47 - [34m[1mLOGS   [0m - Epoch:   0 [    7812/  200000], loss: {'classification': 195.4851, 'neural_augmentation': 8.6052, 'total_loss': 204.0903}, LR: [0.000391, 0.000391], Avg. batch load time: 0.069, Elapsed time: 13284.27
2024-07-29 05:47:12 - [34m[1mLOGS   [0m - Epoch:   0 [    7875/  200000], loss: {'classification': 194.7078, 'neural_augmentation': 8.5908, 'total_loss': 203.2987}, LR: [0.000394, 0.000394], Avg. batch load time: 0.069, Elapsed time: 13429.07
2024-07-29 05:47:53 - [34m[1mLOGS   [0m - *** Training summary for epoch 0
	 loss={'classification': 194.435, 'neural_augmentation': 8.5856, 'total_loss': 203.0206}
2024-07-29 05:47:55 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/checkpoint_best.pt
2024-07-29 05:47:56 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/training_checkpoint_last.pt
2024-07-29 05:47:56 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/checkpoint_last.pt
2024-07-29 05:47:56 - [34m[1mLOGS   [0m - Training checkpoint for epoch 0/iteration 7897 is saved at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/training_checkpoint_epoch_0_iter_7897.pt
2024-07-29 05:47:57 - [34m[1mLOGS   [0m - Model state for epoch 0/iteration 7897 is saved at: /ML-A100/team/mm/models/catlip_data/results500_accum_dci/train/checkpoint_epoch_0_iter_7897.pt
[31m===========================================================================[0m
2024-07-29 05:47:59 - [32m[1mINFO   [0m - Training epoch 1
2024-07-29 05:49:14 - [34m[1mLOGS   [0m - Epoch:   1 [    7897/  200000], loss: {'classification': 61.4302, 'neural_augmentation': 6.0744, 'total_loss': 67.5046}, LR: [0.000395, 0.000395], Avg. batch load time: 59.822, Elapsed time: 75.14
2024-07-29 05:50:38 - [34m[1mLOGS   [0m - Epoch:   1 [    7959/  200000], loss: {'classification': 48.3456, 'neural_augmentation': 6.4207, 'total_loss': 54.7663}, LR: [0.000399, 0.000399], Avg. batch load time: 0.143, Elapsed time: 159.89
2024-07-29 05:52:14 - [34m[1mLOGS   [0m - Epoch:   1 [    8022/  200000], loss: {'classification': 47.4, 'neural_augmentation': 6.4017, 'total_loss': 53.8016}, LR: [0.000402, 0.000402], Avg. batch load time: 0.088, Elapsed time: 255.91
2024-07-29 05:53:41 - [34m[1mLOGS   [0m - Epoch:   1 [    8084/  200000], loss: {'classification': 46.9601, 'neural_augmentation': 6.3855, 'total_loss': 53.3457}, LR: [0.000405, 0.000405], Avg. batch load time: 0.066, Elapsed time: 342.55
2024-07-29 05:55:12 - [34m[1mLOGS   [0m - Epoch:   1 [    8147/  200000], loss: {'classification': 46.6306, 'neural_augmentation': 6.3658, 'total_loss': 52.9964}, LR: [0.000408, 0.000408], Avg. batch load time: 0.057, Elapsed time: 433.23
2024-07-29 05:56:45 - [34m[1mLOGS   [0m - Epoch:   1 [    8209/  200000], loss: {'classification': 46.4359, 'neural_augmentation': 6.345, 'total_loss': 52.7809}, LR: [0.000411, 0.000411], Avg. batch load time: 0.050, Elapsed time: 526.50
2024-07-29 05:58:23 - [34m[1mLOGS   [0m - Epoch:   1 [    8272/  200000], loss: {'classification': 46.221, 'neural_augmentation': 6.3202, 'total_loss': 52.5412}, LR: [0.000414, 0.000414], Avg. batch load time: 0.050, Elapsed time: 624.36
2024-07-29 05:59:57 - [34m[1mLOGS   [0m - Epoch:   1 [    8334/  200000], loss: {'classification': 46.0722, 'neural_augmentation': 6.2963, 'total_loss': 52.3685}, LR: [0.000417, 0.000417], Avg. batch load time: 0.049, Elapsed time: 718.88
2024-07-29 06:01:32 - [34m[1mLOGS   [0m - Epoch:   1 [    8397/  200000], loss: {'classification': 45.9272, 'neural_augmentation': 6.275, 'total_loss': 52.2021}, LR: [0.00042, 0.00042], Avg. batch load time: 0.047, Elapsed time: 813.16
2024-07-29 06:03:06 - [34m[1mLOGS   [0m - Epoch:   1 [    8459/  200000], loss: {'classification': 45.8273, 'neural_augmentation': 6.254, 'total_loss': 52.0813}, LR: [0.000424, 0.000424], Avg. batch load time: 0.046, Elapsed time: 907.04
2024-07-29 06:04:35 - [34m[1mLOGS   [0m - Epoch:   1 [    8522/  200000], loss: {'classification': 45.716, 'neural_augmentation': 6.2294, 'total_loss': 51.9454}, LR: [0.000427, 0.000427], Avg. batch load time: 0.045, Elapsed time: 996.31
2024-07-29 06:06:12 - [34m[1mLOGS   [0m - Epoch:   1 [    8584/  200000], loss: {'classification': 45.6314, 'neural_augmentation': 6.2035, 'total_loss': 51.8349}, LR: [0.00043, 0.00043], Avg. batch load time: 0.044, Elapsed time: 1093.30
2024-07-29 06:07:48 - [34m[1mLOGS   [0m - Epoch:   1 [    8647/  200000], loss: {'classification': 45.5464, 'neural_augmentation': 6.1773, 'total_loss': 51.7237}, LR: [0.000433, 0.000433], Avg. batch load time: 0.045, Elapsed time: 1189.65
2024-07-29 06:09:23 - [34m[1mLOGS   [0m - Epoch:   1 [    8709/  200000], loss: {'classification': 45.4577, 'neural_augmentation': 6.1519, 'total_loss': 51.6096}, LR: [0.000436, 0.000436], Avg. batch load time: 0.044, Elapsed time: 1284.32
2024-07-29 06:10:59 - [34m[1mLOGS   [0m - Epoch:   1 [    8772/  200000], loss: {'classification': 45.3858, 'neural_augmentation': 6.1266, 'total_loss': 51.5124}, LR: [0.000439, 0.000439], Avg. batch load time: 0.044, Elapsed time: 1380.21
2024-07-29 06:12:29 - [34m[1mLOGS   [0m - Epoch:   1 [    8834/  200000], loss: {'classification': 45.3202, 'neural_augmentation': 6.1019, 'total_loss': 51.4221}, LR: [0.000442, 0.000442], Avg. batch load time: 0.043, Elapsed time: 1470.89
2024-07-29 06:14:06 - [34m[1mLOGS   [0m - Epoch:   1 [    8897/  200000], loss: {'classification': 45.2529, 'neural_augmentation': 6.0775, 'total_loss': 51.3304}, LR: [0.000445, 0.000445], Avg. batch load time: 0.043, Elapsed time: 1567.62
2024-07-29 06:15:44 - [34m[1mLOGS   [0m - Epoch:   1 [    8959/  200000], loss: {'classification': 45.1953, 'neural_augmentation': 6.0506, 'total_loss': 51.2459}, LR: [0.000449, 0.000449], Avg. batch load time: 0.043, Elapsed time: 1665.56
2024-07-29 06:17:15 - [34m[1mLOGS   [0m - Epoch:   1 [    9022/  200000], loss: {'classification': 45.141, 'neural_augmentation': 6.0249, 'total_loss': 51.1659}, LR: [0.000452, 0.000452], Avg. batch load time: 0.042, Elapsed time: 1756.24
2024-07-29 06:18:46 - [34m[1mLOGS   [0m - Epoch:   1 [    9084/  200000], loss: {'classification': 45.0794, 'neural_augmentation': 6.0, 'total_loss': 51.0794}, LR: [0.000455, 0.000455], Avg. batch load time: 0.042, Elapsed time: 1847.78
2024-07-29 06:20:21 - [34m[1mLOGS   [0m - Epoch:   1 [    9147/  200000], loss: {'classification': 45.032, 'neural_augmentation': 5.9738, 'total_loss': 51.0058}, LR: [0.000458, 0.000458], Avg. batch load time: 0.042, Elapsed time: 1942.29
2024-07-29 06:21:52 - [34m[1mLOGS   [0m - Epoch:   1 [    9209/  200000], loss: {'classification': 44.9789, 'neural_augmentation': 5.9483, 'total_loss': 50.9272}, LR: [0.000461, 0.000461], Avg. batch load time: 0.041, Elapsed time: 2033.14
2024-07-29 06:23:18 - [34m[1mLOGS   [0m - Epoch:   1 [    9272/  200000], loss: {'classification': 44.9318, 'neural_augmentation': 5.9241, 'total_loss': 50.8558}, LR: [0.000464, 0.000464], Avg. batch load time: 0.041, Elapsed time: 2119.94
2024-07-29 06:24:48 - [34m[1mLOGS   [0m - Epoch:   1 [    9334/  200000], loss: {'classification': 44.8963, 'neural_augmentation': 5.8977, 'total_loss': 50.794}, LR: [0.000467, 0.000467], Avg. batch load time: 0.040, Elapsed time: 2209.66
2024-07-29 06:26:21 - [34m[1mLOGS   [0m - Epoch:   1 [    9397/  200000], loss: {'classification': 44.8421, 'neural_augmentation': 5.8714, 'total_loss': 50.7135}, LR: [0.00047, 0.00047], Avg. batch load time: 0.040, Elapsed time: 2302.45
2024-07-29 06:27:50 - [34m[1mLOGS   [0m - Epoch:   1 [    9459/  200000], loss: {'classification': 44.7985, 'neural_augmentation': 5.8444, 'total_loss': 50.6429}, LR: [0.000473, 0.000473], Avg. batch load time: 0.040, Elapsed time: 2391.61
2024-07-29 06:29:29 - [34m[1mLOGS   [0m - Epoch:   1 [    9522/  200000], loss: {'classification': 44.7438, 'neural_augmentation': 5.8175, 'total_loss': 50.5613}, LR: [0.000477, 0.000477], Avg. batch load time: 0.039, Elapsed time: 2490.52
2024-07-29 06:30:55 - [34m[1mLOGS   [0m - Epoch:   1 [    9584/  200000], loss: {'classification': 44.7042, 'neural_augmentation': 5.7897, 'total_loss': 50.4939}, LR: [0.00048, 0.00048], Avg. batch load time: 0.038, Elapsed time: 2576.25
2024-07-29 06:32:27 - [34m[1mLOGS   [0m - Epoch:   1 [    9647/  200000], loss: {'classification': 44.6616, 'neural_augmentation': 5.7623, 'total_loss': 50.4239}, LR: [0.000483, 0.000483], Avg. batch load time: 0.038, Elapsed time: 2668.89
2024-07-29 06:33:55 - [34m[1mLOGS   [0m - Epoch:   1 [    9709/  200000], loss: {'classification': 44.6231, 'neural_augmentation': 5.7343, 'total_loss': 50.3574}, LR: [0.000486, 0.000486], Avg. batch load time: 0.038, Elapsed time: 2756.17
2024-07-29 06:35:27 - [34m[1mLOGS   [0m - Epoch:   1 [    9772/  200000], loss: {'classification': 44.5844, 'neural_augmentation': 5.7065, 'total_loss': 50.291}, LR: [0.000489, 0.000489], Avg. batch load time: 0.038, Elapsed time: 2848.39
2024-07-29 06:37:07 - [34m[1mLOGS   [0m - Epoch:   1 [    9834/  200000], loss: {'classification': 44.5455, 'neural_augmentation': 5.6772, 'total_loss': 50.2227}, LR: [0.000492, 0.000492], Avg. batch load time: 0.038, Elapsed time: 2948.92
2024-07-29 06:38:35 - [34m[1mLOGS   [0m - Epoch:   1 [    9897/  200000], loss: {'classification': 44.5075, 'neural_augmentation': 5.6489, 'total_loss': 50.1564}, LR: [0.000495, 0.000495], Avg. batch load time: 0.038, Elapsed time: 3036.15
2024-07-29 06:40:07 - [34m[1mLOGS   [0m - Epoch:   1 [    9959/  200000], loss: {'classification': 44.486, 'neural_augmentation': 5.6207, 'total_loss': 50.1066}, LR: [0.000498, 0.000498], Avg. batch load time: 0.038, Elapsed time: 3128.37
2024-07-29 06:41:35 - [34m[1mLOGS   [0m - Epoch:   1 [   10022/  200000], loss: {'classification': 44.4485, 'neural_augmentation': 5.5912, 'total_loss': 50.0397}, LR: [0.000502, 0.000502], Avg. batch load time: 0.037, Elapsed time: 3216.13
2024-07-29 06:43:07 - [34m[1mLOGS   [0m - Epoch:   1 [   10084/  200000], loss: {'classification': 44.4144, 'neural_augmentation': 5.5626, 'total_loss': 49.977}, LR: [0.000505, 0.000505], Avg. batch load time: 0.037, Elapsed time: 3308.00
2024-07-29 06:44:48 - [34m[1mLOGS   [0m - Epoch:   1 [   10147/  200000], loss: {'classification': 44.3781, 'neural_augmentation': 5.5308, 'total_loss': 49.9088}, LR: [0.000508, 0.000508], Avg. batch load time: 0.037, Elapsed time: 3409.53
2024-07-29 06:46:20 - [34m[1mLOGS   [0m - Epoch:   1 [   10209/  200000], loss: {'classification': 44.3394, 'neural_augmentation': 5.5015, 'total_loss': 49.8408}, LR: [0.000511, 0.000511], Avg. batch load time: 0.037, Elapsed time: 3501.05
2024-07-29 06:48:02 - [34m[1mLOGS   [0m - Epoch:   1 [   10272/  200000], loss: {'classification': 44.3121, 'neural_augmentation': 5.4698, 'total_loss': 49.7819}, LR: [0.000514, 0.000514], Avg. batch load time: 0.037, Elapsed time: 3603.22
2024-07-29 06:49:30 - [34m[1mLOGS   [0m - Epoch:   1 [   10334/  200000], loss: {'classification': 44.2772, 'neural_augmentation': 5.4404, 'total_loss': 49.7176}, LR: [0.000517, 0.000517], Avg. batch load time: 0.037, Elapsed time: 3691.41
2024-07-29 06:51:00 - [34m[1mLOGS   [0m - Epoch:   1 [   10397/  200000], loss: {'classification': 44.241, 'neural_augmentation': 5.4104, 'total_loss': 49.6513}, LR: [0.00052, 0.00052], Avg. batch load time: 0.036, Elapsed time: 3781.85
2024-07-29 06:52:34 - [34m[1mLOGS   [0m - Epoch:   1 [   10459/  200000], loss: {'classification': 44.2036, 'neural_augmentation': 5.381, 'total_loss': 49.5846}, LR: [0.000523, 0.000523], Avg. batch load time: 0.037, Elapsed time: 3875.79
2024-07-29 06:54:04 - [34m[1mLOGS   [0m - Epoch:   1 [   10522/  200000], loss: {'classification': 44.1744, 'neural_augmentation': 5.3502, 'total_loss': 49.5246}, LR: [0.000527, 0.000527], Avg. batch load time: 0.036, Elapsed time: 3965.68
2024-07-29 06:55:39 - [34m[1mLOGS   [0m - Epoch:   1 [   10584/  200000], loss: {'classification': 44.1433, 'neural_augmentation': 5.3198, 'total_loss': 49.4631}, LR: [0.00053, 0.00053], Avg. batch load time: 0.036, Elapsed time: 4060.16
2024-07-29 06:57:11 - [34m[1mLOGS   [0m - Epoch:   1 [   10647/  200000], loss: {'classification': 44.1142, 'neural_augmentation': 5.2898, 'total_loss': 49.404}, LR: [0.000533, 0.000533], Avg. batch load time: 0.036, Elapsed time: 4152.38
2024-07-29 06:58:51 - [34m[1mLOGS   [0m - Epoch:   1 [   10709/  200000], loss: {'classification': 44.09, 'neural_augmentation': 5.2579, 'total_loss': 49.3479}, LR: [0.000536, 0.000536], Avg. batch load time: 0.037, Elapsed time: 4252.69
2024-07-29 07:00:22 - [34m[1mLOGS   [0m - Epoch:   1 [   10772/  200000], loss: {'classification': 44.0535, 'neural_augmentation': 5.2287, 'total_loss': 49.2822}, LR: [0.000539, 0.000539], Avg. batch load time: 0.036, Elapsed time: 4343.21
2024-07-29 07:01:59 - [34m[1mLOGS   [0m - Epoch:   1 [   10834/  200000], loss: {'classification': 44.0272, 'neural_augmentation': 5.1986, 'total_loss': 49.2258}, LR: [0.000542, 0.000542], Avg. batch load time: 0.036, Elapsed time: 4440.37
2024-07-29 07:03:30 - [34m[1mLOGS   [0m - Epoch:   1 [   10897/  200000], loss: {'classification': 44.0009, 'neural_augmentation': 5.1688, 'total_loss': 49.1697}, LR: [0.000545, 0.000545], Avg. batch load time: 0.036, Elapsed time: 4531.14
2024-07-29 07:05:01 - [34m[1mLOGS   [0m - Epoch:   1 [   10959/  200000], loss: {'classification': 43.9728, 'neural_augmentation': 5.139, 'total_loss': 49.1119}, LR: [0.000548, 0.000548], Avg. batch load time: 0.036, Elapsed time: 4622.37
2024-07-29 07:06:37 - [34m[1mLOGS   [0m - Epoch:   1 [   11022/  200000], loss: {'classification': 43.9436, 'neural_augmentation': 5.1077, 'total_loss': 49.0513}, LR: [0.000552, 0.000552], Avg. batch load time: 0.036, Elapsed time: 4718.88
2024-07-29 07:08:11 - [34m[1mLOGS   [0m - Epoch:   1 [   11084/  200000], loss: {'classification': 43.9209, 'neural_augmentation': 5.0785, 'total_loss': 48.9995}, LR: [0.000555, 0.000555], Avg. batch load time: 0.036, Elapsed time: 4812.41
2024-07-29 07:09:40 - [34m[1mLOGS   [0m - Epoch:   1 [   11147/  200000], loss: {'classification': 43.896, 'neural_augmentation': 5.0493, 'total_loss': 48.9452}, LR: [0.000558, 0.000558], Avg. batch load time: 0.036, Elapsed time: 4901.11
2024-07-29 07:11:18 - [34m[1mLOGS   [0m - Epoch:   1 [   11209/  200000], loss: {'classification': 43.8687, 'neural_augmentation': 5.0187, 'total_loss': 48.8874}, LR: [0.000561, 0.000561], Avg. batch load time: 0.035, Elapsed time: 4999.07
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 509, in Client
    deliver_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 740, in deliver_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
2024-07-29 07:12:24 - [34m[1mLOGS   [0m - Keyboard interruption. Exiting from early training
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 16 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
