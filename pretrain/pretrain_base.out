nohup: ignoring input
2024-07-25 17:08:38 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
base
dci
2024-07-25 17:08:39 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-25 17:08:39 - [34m[1mLOGS   [0m - [36mModel[0m
ViTamin(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (128,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=1024, out_features=6743, bias=True, channel_first=False)
)
[31m=================================================================[0m
                            ViTamin Summary
[31m=================================================================[0m
Total parameters     =  109.299 M
Total trainable parameters =  109.299 M

2024-07-25 17:08:39 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-25 17:08:39 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.109G                 | 13.405G    |
|  pos_embed                           |  (1, 1, 512)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  3.653M                |  5.52G     |
|   patch_embed.backbone.stem          |   0.151M               |   1.901G   |
|    patch_embed.backbone.stem.conv1   |    3.584K              |    43.352M |
|    patch_embed.backbone.stem.norm1   |    0.256K              |    8.028M  |
|    patch_embed.backbone.stem.conv2   |    0.148M              |    1.85G   |
|   patch_embed.backbone.stages        |   2.321M               |   3.387G   |
|    patch_embed.backbone.stages.0     |    0.274M              |    1.478G  |
|    patch_embed.backbone.stages.1     |    2.047M              |    1.909G  |
|   patch_embed.backbone.pool          |   1.181M               |   0.232G   |
|    patch_embed.backbone.pool.proj    |    1.18M               |    0.231G  |
|    patch_embed.backbone.pool.norm    |    0.512K              |    1.004M  |
|  blocks                              |  18.404M               |  3.607G    |
|   blocks.0                           |   2.629M               |   0.515G   |
|    blocks.0.norm1                    |    1.024K              |    0.502M  |
|    blocks.0.attn                     |    1.051M              |    0.206G  |
|    blocks.0.norm2                    |    1.024K              |    0.502M  |
|    blocks.0.mlp                      |    1.576M              |    0.309G  |
|   blocks.1                           |   2.629M               |   0.515G   |
|    blocks.1.norm1                    |    1.024K              |    0.502M  |
|    blocks.1.attn                     |    1.051M              |    0.206G  |
|    blocks.1.norm2                    |    1.024K              |    0.502M  |
|    blocks.1.mlp                      |    1.576M              |    0.309G  |
|   blocks.2                           |   2.629M               |   0.515G   |
|    blocks.2.norm1                    |    1.024K              |    0.502M  |
|    blocks.2.attn                     |    1.051M              |    0.206G  |
|    blocks.2.norm2                    |    1.024K              |    0.502M  |
|    blocks.2.mlp                      |    1.576M              |    0.309G  |
|   blocks.3                           |   2.629M               |   0.515G   |
|    blocks.3.norm1                    |    1.024K              |    0.502M  |
|    blocks.3.attn                     |    1.051M              |    0.206G  |
|    blocks.3.norm2                    |    1.024K              |    0.502M  |
|    blocks.3.mlp                      |    1.576M              |    0.309G  |
|   blocks.4                           |   2.629M               |   0.515G   |
|    blocks.4.norm1                    |    1.024K              |    0.502M  |
|    blocks.4.attn                     |    1.051M              |    0.206G  |
|    blocks.4.norm2                    |    1.024K              |    0.502M  |
|    blocks.4.mlp                      |    1.576M              |    0.309G  |
|   blocks.5                           |   2.629M               |   0.515G   |
|    blocks.5.norm1                    |    1.024K              |    0.502M  |
|    blocks.5.attn                     |    1.051M              |    0.206G  |
|    blocks.5.norm2                    |    1.024K              |    0.502M  |
|    blocks.5.mlp                      |    1.576M              |    0.309G  |
|   blocks.6                           |   2.629M               |   0.515G   |
|    blocks.6.norm1                    |    1.024K              |    0.502M  |
|    blocks.6.attn                     |    1.051M              |    0.206G  |
|    blocks.6.norm2                    |    1.024K              |    0.502M  |
|    blocks.6.mlp                      |    1.576M              |    0.309G  |
|  pool                                |  4.721M                |  0.463G    |
|   pool.proj                          |   4.72M                |   0.462G   |
|    pool.proj.weight                  |    (1024, 512, 3, 3)   |            |
|    pool.proj.bias                    |    (1024,)             |            |
|   pool.norm                          |   1.024K               |   1.004M   |
|    pool.norm.weight                  |    (512,)              |            |
|    pool.norm.bias                    |    (512,)              |            |
|  blocks1                             |  73.508M               |  3.602G    |
|   blocks1.0                          |   10.501M              |   0.515G   |
|    blocks1.0.norm1                   |    2.048K              |    0.251M  |
|    blocks1.0.attn                    |    4.198M              |    0.206G  |
|    blocks1.0.norm2                   |    2.048K              |    0.251M  |
|    blocks1.0.mlp                     |    6.299M              |    0.309G  |
|   blocks1.1                          |   10.501M              |   0.515G   |
|    blocks1.1.norm1                   |    2.048K              |    0.251M  |
|    blocks1.1.attn                    |    4.198M              |    0.206G  |
|    blocks1.1.norm2                   |    2.048K              |    0.251M  |
|    blocks1.1.mlp                     |    6.299M              |    0.309G  |
|   blocks1.2                          |   10.501M              |   0.515G   |
|    blocks1.2.norm1                   |    2.048K              |    0.251M  |
|    blocks1.2.attn                    |    4.198M              |    0.206G  |
|    blocks1.2.norm2                   |    2.048K              |    0.251M  |
|    blocks1.2.mlp                     |    6.299M              |    0.309G  |
|   blocks1.3                          |   10.501M              |   0.515G   |
|    blocks1.3.norm1                   |    2.048K              |    0.251M  |
|    blocks1.3.attn                    |    4.198M              |    0.206G  |
|    blocks1.3.norm2                   |    2.048K              |    0.251M  |
|    blocks1.3.mlp                     |    6.299M              |    0.309G  |
|   blocks1.4                          |   10.501M              |   0.515G   |
|    blocks1.4.norm1                   |    2.048K              |    0.251M  |
|    blocks1.4.attn                    |    4.198M              |    0.206G  |
|    blocks1.4.norm2                   |    2.048K              |    0.251M  |
|    blocks1.4.mlp                     |    6.299M              |    0.309G  |
|   blocks1.5                          |   10.501M              |   0.515G   |
|    blocks1.5.norm1                   |    2.048K              |    0.251M  |
|    blocks1.5.attn                    |    4.198M              |    0.206G  |
|    blocks1.5.norm2                   |    2.048K              |    0.251M  |
|    blocks1.5.mlp                     |    6.299M              |    0.309G  |
|   blocks1.6                          |   10.501M              |   0.515G   |
|    blocks1.6.norm1                   |    2.048K              |    0.251M  |
|    blocks1.6.attn                    |    4.198M              |    0.206G  |
|    blocks1.6.norm2                   |    2.048K              |    0.251M  |
|    blocks1.6.mlp                     |    6.299M              |    0.309G  |
|  mlp                                 |  2.099M                |  0.206G    |
|   mlp.0                              |   1.05M                |   0.103G   |
|    mlp.0.weight                      |    (1024, 1024)        |            |
|    mlp.0.bias                        |    (1024,)             |            |
|   mlp.2                              |   1.05M                |   0.103G   |
|    mlp.2.weight                      |    (1024, 1024)        |            |
|    mlp.2.bias                        |    (1024,)             |            |
|  fc_norm                             |  2.048K                |  5.12K     |
|   fc_norm.weight                     |   (1024,)              |            |
|   fc_norm.bias                       |   (1024,)              |            |
|  classifier                          |  6.912M                |  6.905M    |
|   classifier.weight                  |   (6743, 1024)         |            |
|   classifier.bias                    |   (6743,)              |            |
2024-07-25 17:08:39 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-25 17:08:39 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks1.1.attn.attn_drop', 'blocks1.3.drop_path2', 'blocks.1.attn.q_norm', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks.6.ls2', 'blocks.2.drop_path2', 'neural_augmentor.contrast.min_fn', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks1.0.ls2', 'blocks1.3.attn.attn_drop', 'blocks1.3.attn.q_norm', 'blocks.2.attn.q_norm', 'blocks1.3.ls2', 'blocks1.5.attn.q_norm', 'blocks.1.drop_path2', 'blocks.2.attn.attn_drop', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks.3.drop_path2', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks.2.attn.k_norm', 'blocks.1.ls2', 'patch_embed.backbone.stages.1.0.drop_path', 'neural_augmentor.noise.min_fn', 'blocks1.5.ls1', 'blocks1.6.attn.attn_drop', 'blocks1.1.drop_path2', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks1.4.ls1', 'neural_augmentor', 'blocks1.5.drop_path1', 'patch_embed.backbone.stages.0.0.down', 'blocks1.5.attn.attn_drop', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks1.2.ls1', 'blocks1.6.drop_path2', 'blocks1.6.ls1', 'blocks.5.drop_path1', 'patch_embed.backbone.stages.1.2.down', 'blocks.5.attn.attn_drop', 'blocks.0.ls2', 'blocks1.3.drop_path1', 'neural_augmentor.noise', 'patch_embed.backbone.stages.1.1.down', 'blocks.4.ls2', 'blocks1.1.ls1', 'neural_augmentor.noise.max_fn', 'blocks1.6.attn.q_norm', 'neural_augmentor.brightness.min_fn', 'neural_augmentor.brightness', 'blocks1.3.attn.k_norm', 'norm_pre', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks.5.ls1', 'blocks.3.attn.k_norm', 'blocks.4.attn.q_norm', 'blocks.6.drop_path1', 'blocks.3.drop_path1', 'blocks1.4.drop_path1', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'patch_embed.backbone.stages.1.0.down', 'blocks1.0.drop_path1', 'blocks.3.attn.attn_drop', 'patch_embed.backbone.stages.1.3.down', 'blocks.4.drop_path2', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks.1.ls1', 'blocks1.2.attn.attn_drop', 'blocks.6.attn.k_norm', 'blocks.6.ls1', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks1.0.ls1', 'patch_drop', 'blocks.4.attn.attn_drop', 'blocks.3.ls2', 'blocks.5.attn.q_norm', 'blocks1.1.drop_path1', 'patch_embed.backbone.stages.1.3.drop_path', 'neural_augmentor.contrast', 'blocks1.2.ls2', 'blocks1.0.attn.attn_drop', 'blocks1.6.drop_path1', 'blocks1.5.drop_path2', 'neural_augmentor.contrast.max_fn', 'blocks1.3.ls1', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks.0.attn.k_norm', 'neural_augmentor.brightness.max_fn', 'blocks1.1.ls2', 'blocks.5.ls2', 'blocks1.5.attn.k_norm', 'blocks1.1.attn.k_norm', 'blocks1.5.ls2', 'blocks.4.drop_path1', 'blocks.3.ls1', 'blocks.6.attn.attn_drop', 'blocks.0.attn.attn_drop', 'blocks.2.ls2', 'blocks.6.drop_path2', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'patch_embed.proj', 'patch_embed.backbone.stages.0.1.down', 'blocks.4.ls1', 'blocks1.4.ls2', 'blocks1.2.drop_path1', 'blocks.3.attn.q_norm', 'norm', 'blocks1.4.attn.k_norm', 'blocks.2.drop_path1', 'blocks1.0.drop_path2', 'blocks.5.drop_path2', 'blocks1.0.attn.q_norm', 'blocks.0.attn.q_norm', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks.1.attn.attn_drop', 'patch_embed.backbone.stem.norm1.drop', 'blocks1.4.attn.attn_drop', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks.0.drop_path2', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks.6.attn.q_norm', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks1.2.attn.k_norm', 'blocks1.6.attn.k_norm', 'blocks1.4.drop_path2', 'blocks1.0.attn.k_norm', 'blocks1.6.ls2', 'blocks.4.attn.k_norm', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks.1.drop_path1', 'blocks.0.ls1', 'blocks1.2.drop_path2', 'blocks1.4.attn.q_norm', 'blocks.5.attn.k_norm', 'blocks1.2.attn.q_norm', 'blocks.1.attn.k_norm', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks.0.drop_path1', 'blocks.2.ls1', 'blocks1.1.attn.q_norm'}
2024-07-25 17:08:39 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::avg_pool2d': 2, 'aten::sum': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-25 17:08:39 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-25 17:08:39 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-25 17:08:39 - [34m[1mLOGS   [0m - Available GPUs: 8
2024-07-25 17:08:39 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-25 17:08:39 - [34m[1mLOGS   [0m - Directory exists at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
2024-07-25 17:08:42 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:20000
2024-07-25 17:08:45 - [34m[1mLOGS   [0m - Training dataset details are given below
WordnetTaggedClassificationDataset(
	root= 
	is_training=True 
	num_samples=64290000
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	total_tar_files=6429
	max_files_per_tar=10000
	num_synsets=6743
)
2024-07-25 17:08:47 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=True
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=100
	 scales=[(128, 128, 306), (144, 144, 241), (160, 160, 196), (176, 176, 161), (192, 192, 136), (208, 208, 115), (224, 224, 100), (240, 240, 87), (256, 256, 76), (272, 272, 67), (288, 288, 60), (304, 304, 54), (320, 320, 49)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-25 17:08:47 - [34m[1mLOGS   [0m - Number of data workers: 64
base
dci
2024-07-25 17:08:48 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-25 17:08:48 - [34m[1mLOGS   [0m - [36mModel[0m
ViTamin(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (128,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=1024, out_features=6743, bias=True, channel_first=False)
)
[31m=================================================================[0m
                            ViTamin Summary
[31m=================================================================[0m
Total parameters     =  109.299 M
Total trainable parameters =  109.299 M

2024-07-25 17:08:49 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-25 17:08:49 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.109G                 | 13.405G    |
|  pos_embed                           |  (1, 1, 512)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  3.653M                |  5.52G     |
|   patch_embed.backbone.stem          |   0.151M               |   1.901G   |
|    patch_embed.backbone.stem.conv1   |    3.584K              |    43.352M |
|    patch_embed.backbone.stem.norm1   |    0.256K              |    8.028M  |
|    patch_embed.backbone.stem.conv2   |    0.148M              |    1.85G   |
|   patch_embed.backbone.stages        |   2.321M               |   3.387G   |
|    patch_embed.backbone.stages.0     |    0.274M              |    1.478G  |
|    patch_embed.backbone.stages.1     |    2.047M              |    1.909G  |
|   patch_embed.backbone.pool          |   1.181M               |   0.232G   |
|    patch_embed.backbone.pool.proj    |    1.18M               |    0.231G  |
|    patch_embed.backbone.pool.norm    |    0.512K              |    1.004M  |
|  blocks                              |  18.404M               |  3.607G    |
|   blocks.0                           |   2.629M               |   0.515G   |
|    blocks.0.norm1                    |    1.024K              |    0.502M  |
|    blocks.0.attn                     |    1.051M              |    0.206G  |
|    blocks.0.norm2                    |    1.024K              |    0.502M  |
|    blocks.0.mlp                      |    1.576M              |    0.309G  |
|   blocks.1                           |   2.629M               |   0.515G   |
|    blocks.1.norm1                    |    1.024K              |    0.502M  |
|    blocks.1.attn                     |    1.051M              |    0.206G  |
|    blocks.1.norm2                    |    1.024K              |    0.502M  |
|    blocks.1.mlp                      |    1.576M              |    0.309G  |
|   blocks.2                           |   2.629M               |   0.515G   |
|    blocks.2.norm1                    |    1.024K              |    0.502M  |
|    blocks.2.attn                     |    1.051M              |    0.206G  |
|    blocks.2.norm2                    |    1.024K              |    0.502M  |
|    blocks.2.mlp                      |    1.576M              |    0.309G  |
|   blocks.3                           |   2.629M               |   0.515G   |
|    blocks.3.norm1                    |    1.024K              |    0.502M  |
|    blocks.3.attn                     |    1.051M              |    0.206G  |
|    blocks.3.norm2                    |    1.024K              |    0.502M  |
|    blocks.3.mlp                      |    1.576M              |    0.309G  |
|   blocks.4                           |   2.629M               |   0.515G   |
|    blocks.4.norm1                    |    1.024K              |    0.502M  |
|    blocks.4.attn                     |    1.051M              |    0.206G  |
|    blocks.4.norm2                    |    1.024K              |    0.502M  |
|    blocks.4.mlp                      |    1.576M              |    0.309G  |
|   blocks.5                           |   2.629M               |   0.515G   |
|    blocks.5.norm1                    |    1.024K              |    0.502M  |
|    blocks.5.attn                     |    1.051M              |    0.206G  |
|    blocks.5.norm2                    |    1.024K              |    0.502M  |
|    blocks.5.mlp                      |    1.576M              |    0.309G  |
|   blocks.6                           |   2.629M               |   0.515G   |
|    blocks.6.norm1                    |    1.024K              |    0.502M  |
|    blocks.6.attn                     |    1.051M              |    0.206G  |
|    blocks.6.norm2                    |    1.024K              |    0.502M  |
|    blocks.6.mlp                      |    1.576M              |    0.309G  |
|  pool                                |  4.721M                |  0.463G    |
|   pool.proj                          |   4.72M                |   0.462G   |
|    pool.proj.weight                  |    (1024, 512, 3, 3)   |            |
|    pool.proj.bias                    |    (1024,)             |            |
|   pool.norm                          |   1.024K               |   1.004M   |
|    pool.norm.weight                  |    (512,)              |            |
|    pool.norm.bias                    |    (512,)              |            |
|  blocks1                             |  73.508M               |  3.602G    |
|   blocks1.0                          |   10.501M              |   0.515G   |
|    blocks1.0.norm1                   |    2.048K              |    0.251M  |
|    blocks1.0.attn                    |    4.198M              |    0.206G  |
|    blocks1.0.norm2                   |    2.048K              |    0.251M  |
|    blocks1.0.mlp                     |    6.299M              |    0.309G  |
|   blocks1.1                          |   10.501M              |   0.515G   |
|    blocks1.1.norm1                   |    2.048K              |    0.251M  |
|    blocks1.1.attn                    |    4.198M              |    0.206G  |
|    blocks1.1.norm2                   |    2.048K              |    0.251M  |
|    blocks1.1.mlp                     |    6.299M              |    0.309G  |
|   blocks1.2                          |   10.501M              |   0.515G   |
|    blocks1.2.norm1                   |    2.048K              |    0.251M  |
|    blocks1.2.attn                    |    4.198M              |    0.206G  |
|    blocks1.2.norm2                   |    2.048K              |    0.251M  |
|    blocks1.2.mlp                     |    6.299M              |    0.309G  |
|   blocks1.3                          |   10.501M              |   0.515G   |
|    blocks1.3.norm1                   |    2.048K              |    0.251M  |
|    blocks1.3.attn                    |    4.198M              |    0.206G  |
|    blocks1.3.norm2                   |    2.048K              |    0.251M  |
|    blocks1.3.mlp                     |    6.299M              |    0.309G  |
|   blocks1.4                          |   10.501M              |   0.515G   |
|    blocks1.4.norm1                   |    2.048K              |    0.251M  |
|    blocks1.4.attn                    |    4.198M              |    0.206G  |
|    blocks1.4.norm2                   |    2.048K              |    0.251M  |
|    blocks1.4.mlp                     |    6.299M              |    0.309G  |
|   blocks1.5                          |   10.501M              |   0.515G   |
|    blocks1.5.norm1                   |    2.048K              |    0.251M  |
|    blocks1.5.attn                    |    4.198M              |    0.206G  |
|    blocks1.5.norm2                   |    2.048K              |    0.251M  |
|    blocks1.5.mlp                     |    6.299M              |    0.309G  |
|   blocks1.6                          |   10.501M              |   0.515G   |
|    blocks1.6.norm1                   |    2.048K              |    0.251M  |
|    blocks1.6.attn                    |    4.198M              |    0.206G  |
|    blocks1.6.norm2                   |    2.048K              |    0.251M  |
|    blocks1.6.mlp                     |    6.299M              |    0.309G  |
|  mlp                                 |  2.099M                |  0.206G    |
|   mlp.0                              |   1.05M                |   0.103G   |
|    mlp.0.weight                      |    (1024, 1024)        |            |
|    mlp.0.bias                        |    (1024,)             |            |
|   mlp.2                              |   1.05M                |   0.103G   |
|    mlp.2.weight                      |    (1024, 1024)        |            |
|    mlp.2.bias                        |    (1024,)             |            |
|  fc_norm                             |  2.048K                |  5.12K     |
|   fc_norm.weight                     |   (1024,)              |            |
|   fc_norm.bias                       |   (1024,)              |            |
|  classifier                          |  6.912M                |  6.905M    |
|   classifier.weight                  |   (6743, 1024)         |            |
|   classifier.bias                    |   (6743,)              |            |
2024-07-25 17:08:50 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-25 17:08:50 - [33m[1mWARNING[0m - Uncalled Modules:
{'neural_augmentor.brightness', 'blocks1.4.ls1', 'blocks1.6.ls2', 'blocks1.5.attn.k_norm', 'blocks1.6.drop_path1', 'blocks.4.ls2', 'blocks1.0.attn.attn_drop', 'blocks1.4.drop_path2', 'blocks.3.drop_path2', 'blocks.3.ls1', 'blocks.1.attn.k_norm', 'blocks.0.attn.q_norm', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.2.drop_path1', 'blocks1.4.attn.attn_drop', 'blocks1.6.ls1', 'blocks1.4.attn.q_norm', 'blocks1.5.drop_path2', 'blocks1.2.attn.k_norm', 'blocks1.2.drop_path2', 'blocks.3.drop_path1', 'blocks1.0.attn.q_norm', 'blocks.2.ls1', 'blocks.5.attn.q_norm', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks1.4.attn.k_norm', 'blocks.1.ls2', 'neural_augmentor.contrast', 'blocks.3.attn.q_norm', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks1.1.ls1', 'blocks1.6.drop_path2', 'blocks1.2.ls1', 'neural_augmentor.contrast.max_fn', 'blocks.5.drop_path1', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks1.2.attn.q_norm', 'blocks1.3.ls1', 'blocks.5.ls1', 'blocks1.1.drop_path2', 'blocks.4.drop_path2', 'blocks.0.attn.k_norm', 'blocks1.6.attn.q_norm', 'patch_embed.proj', 'norm', 'blocks1.6.attn.k_norm', 'blocks.0.ls1', 'patch_embed.backbone.stages.1.3.drop_path', 'patch_embed.backbone.stages.0.0.down', 'blocks.6.attn.attn_drop', 'blocks1.1.drop_path1', 'blocks.6.ls1', 'patch_embed.backbone.stages.1.2.down', 'blocks1.0.drop_path2', 'blocks1.4.drop_path1', 'blocks.4.attn.k_norm', 'patch_embed.backbone.stages.0.0.drop_path', 'neural_augmentor.contrast.min_fn', 'blocks.1.drop_path2', 'blocks1.2.attn.attn_drop', 'blocks.5.attn.k_norm', 'blocks.1.attn.q_norm', 'blocks1.3.ls2', 'neural_augmentor.brightness.min_fn', 'norm_pre', 'blocks.3.ls2', 'blocks1.1.ls2', 'blocks1.0.drop_path1', 'blocks.6.attn.k_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks1.5.attn.attn_drop', 'blocks.1.ls1', 'blocks.0.drop_path1', 'blocks.4.attn.attn_drop', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks.1.drop_path1', 'blocks1.3.drop_path1', 'blocks1.1.attn.k_norm', 'blocks.6.ls2', 'blocks.2.attn.k_norm', 'blocks.4.drop_path1', 'blocks1.1.attn.attn_drop', 'neural_augmentor.noise', 'patch_embed.backbone.stages.0.1.down', 'blocks1.3.attn.k_norm', 'blocks1.0.attn.k_norm', 'blocks.4.ls1', 'blocks.3.attn.k_norm', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks1.3.attn.q_norm', 'patch_embed.backbone.stages.1.2.drop_path', 'patch_embed.backbone.stages.1.3.down', 'patch_embed.backbone.stages.0.1.shortcut', 'patch_embed.backbone.stem.norm1.drop', 'blocks1.0.ls2', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks.0.drop_path2', 'blocks1.1.attn.q_norm', 'blocks1.0.ls1', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks1.5.drop_path1', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks.3.attn.attn_drop', 'blocks.2.ls2', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks.2.drop_path1', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks1.3.drop_path2', 'blocks.4.attn.q_norm', 'blocks.6.drop_path1', 'blocks1.4.ls2', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks1.3.attn.attn_drop', 'blocks.5.drop_path2', 'patch_embed.backbone.stages.1.0.down', 'blocks.5.attn.attn_drop', 'blocks.6.attn.q_norm', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks.2.attn.attn_drop', 'blocks1.5.ls1', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'patch_drop', 'blocks.0.attn.attn_drop', 'blocks.6.drop_path2', 'neural_augmentor.noise.max_fn', 'blocks1.6.attn.attn_drop', 'blocks.5.ls2', 'blocks1.5.ls2', 'blocks.0.ls2', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks1.2.ls2', 'blocks1.5.attn.q_norm', 'blocks.1.attn.attn_drop', 'blocks.2.drop_path2', 'neural_augmentor.noise.min_fn', 'neural_augmentor.brightness.max_fn', 'neural_augmentor', 'blocks.2.attn.q_norm', 'patch_embed.backbone.stages.1.1.down'}
2024-07-25 17:08:50 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::avg_pool2d': 2, 'aten::sum': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-25 17:08:50 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-25 17:08:50 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-25 17:08:50 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-07-25 17:08:50 - [34m[1mLOGS   [0m - Max. iteration for training: 200000
2024-07-25 17:08:50 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=1e-05
 	 max_lr=0.001
 	 period=180001
 	 warmup_init_lr=1e-06
 	 warmup_iters=20000
 )
2024-07-25 17:08:52 - [34m[1mLOGS   [0m - Loaded checkpoint from /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 17:08:52 - [34m[1mLOGS   [0m - Resuming training for epoch 15
2024-07-25 17:08:52 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results_base_dci/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-25 17:08:54 - [32m[1mINFO   [0m - Training epoch 15
2024-07-25 17:08:43 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:20000
base
dci
2024-07-25 17:08:43 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:20000
base
dci
2024-07-25 17:08:42 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:20000
base
dci
2024-07-25 17:08:42 - [32m[1mINFO   [0m - distributed init (rank 6): tcp://localhost:20000
base
dci
2024-07-25 17:08:43 - [32m[1mINFO   [0m - distributed init (rank 7): tcp://localhost:20000
base
dci
2024-07-25 17:08:42 - [32m[1mINFO   [0m - distributed init (rank 5): tcp://localhost:20000
base
dci
2024-07-25 17:08:42 - [32m[1mINFO   [0m - distributed init (rank 4): tcp://localhost:20000
base
dci
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-25 17:13:22 - [34m[1mLOGS   [0m - Epoch:  15 [  118716/  200000], loss: {'classification': 32.7857, 'neural_augmentation': 0.4684, 'total_loss': 33.2541}, LR: [0.00043, 0.00043], Avg. batch load time: 252.638, Elapsed time: 267.58
2024-07-25 17:15:31 - [34m[1mLOGS   [0m - Epoch:  15 [  118778/  200000], loss: {'classification': 32.7196, 'neural_augmentation': 0.4678, 'total_loss': 33.1875}, LR: [0.000429, 0.000429], Avg. batch load time: 0.507, Elapsed time: 397.17
2024-07-25 17:17:37 - [34m[1mLOGS   [0m - Epoch:  15 [  118841/  200000], loss: {'classification': 32.7823, 'neural_augmentation': 0.4667, 'total_loss': 33.249}, LR: [0.000429, 0.000429], Avg. batch load time: 0.254, Elapsed time: 523.49
2024-07-25 17:19:43 - [34m[1mLOGS   [0m - Epoch:  15 [  118903/  200000], loss: {'classification': 32.8578, 'neural_augmentation': 0.4657, 'total_loss': 33.3235}, LR: [0.000428, 0.000428], Avg. batch load time: 0.170, Elapsed time: 649.29
2024-07-25 17:21:49 - [34m[1mLOGS   [0m - Epoch:  15 [  118966/  200000], loss: {'classification': 32.9417, 'neural_augmentation': 0.4651, 'total_loss': 33.4068}, LR: [0.000428, 0.000428], Avg. batch load time: 0.128, Elapsed time: 774.83
2024-07-25 17:23:55 - [34m[1mLOGS   [0m - Epoch:  15 [  119028/  200000], loss: {'classification': 32.9971, 'neural_augmentation': 0.4644, 'total_loss': 33.4615}, LR: [0.000427, 0.000427], Avg. batch load time: 0.102, Elapsed time: 900.62
2024-07-25 17:26:00 - [34m[1mLOGS   [0m - Epoch:  15 [  119091/  200000], loss: {'classification': 33.0349, 'neural_augmentation': 0.4639, 'total_loss': 33.4988}, LR: [0.000427, 0.000427], Avg. batch load time: 0.085, Elapsed time: 1026.34
2024-07-25 17:28:06 - [34m[1mLOGS   [0m - Epoch:  15 [  119153/  200000], loss: {'classification': 33.0735, 'neural_augmentation': 0.4634, 'total_loss': 33.5369}, LR: [0.000426, 0.000426], Avg. batch load time: 0.073, Elapsed time: 1151.82
2024-07-25 17:30:11 - [34m[1mLOGS   [0m - Epoch:  15 [  119216/  200000], loss: {'classification': 33.0975, 'neural_augmentation': 0.4631, 'total_loss': 33.5606}, LR: [0.000426, 0.000426], Avg. batch load time: 0.064, Elapsed time: 1277.42
2024-07-25 17:32:17 - [34m[1mLOGS   [0m - Epoch:  15 [  119278/  200000], loss: {'classification': 33.1224, 'neural_augmentation': 0.4628, 'total_loss': 33.5852}, LR: [0.000425, 0.000425], Avg. batch load time: 0.057, Elapsed time: 1403.00
2024-07-25 17:34:24 - [34m[1mLOGS   [0m - Epoch:  15 [  119341/  200000], loss: {'classification': 33.143, 'neural_augmentation': 0.4626, 'total_loss': 33.6056}, LR: [0.000425, 0.000425], Avg. batch load time: 0.052, Elapsed time: 1530.00
2024-07-25 17:36:31 - [34m[1mLOGS   [0m - Epoch:  15 [  119403/  200000], loss: {'classification': 33.1582, 'neural_augmentation': 0.4625, 'total_loss': 33.6206}, LR: [0.000424, 0.000424], Avg. batch load time: 0.047, Elapsed time: 1657.05
2024-07-25 17:38:36 - [34m[1mLOGS   [0m - Epoch:  15 [  119466/  200000], loss: {'classification': 33.173, 'neural_augmentation': 0.4623, 'total_loss': 33.6354}, LR: [0.000424, 0.000424], Avg. batch load time: 0.043, Elapsed time: 1782.32
2024-07-25 17:40:42 - [34m[1mLOGS   [0m - Epoch:  15 [  119528/  200000], loss: {'classification': 33.1946, 'neural_augmentation': 0.4623, 'total_loss': 33.6568}, LR: [0.000423, 0.000423], Avg. batch load time: 0.040, Elapsed time: 1907.79
2024-07-25 17:42:47 - [34m[1mLOGS   [0m - Epoch:  15 [  119591/  200000], loss: {'classification': 33.216, 'neural_augmentation': 0.4622, 'total_loss': 33.6782}, LR: [0.000423, 0.000423], Avg. batch load time: 0.037, Elapsed time: 2033.42
2024-07-25 17:44:53 - [34m[1mLOGS   [0m - Epoch:  15 [  119653/  200000], loss: {'classification': 33.231, 'neural_augmentation': 0.4622, 'total_loss': 33.6931}, LR: [0.000422, 0.000422], Avg. batch load time: 0.035, Elapsed time: 2158.86
2024-07-25 17:46:58 - [34m[1mLOGS   [0m - Epoch:  15 [  119716/  200000], loss: {'classification': 33.2418, 'neural_augmentation': 0.4622, 'total_loss': 33.704}, LR: [0.000421, 0.000421], Avg. batch load time: 0.033, Elapsed time: 2284.16
2024-07-25 17:49:04 - [34m[1mLOGS   [0m - Epoch:  15 [  119778/  200000], loss: {'classification': 33.2643, 'neural_augmentation': 0.4622, 'total_loss': 33.7265}, LR: [0.000421, 0.000421], Avg. batch load time: 0.031, Elapsed time: 2410.10
2024-07-25 17:51:10 - [34m[1mLOGS   [0m - Epoch:  15 [  119841/  200000], loss: {'classification': 33.276, 'neural_augmentation': 0.4622, 'total_loss': 33.7382}, LR: [0.00042, 0.00042], Avg. batch load time: 0.029, Elapsed time: 2535.75
2024-07-25 17:53:15 - [34m[1mLOGS   [0m - Epoch:  15 [  119903/  200000], loss: {'classification': 33.2925, 'neural_augmentation': 0.4623, 'total_loss': 33.7547}, LR: [0.00042, 0.00042], Avg. batch load time: 0.028, Elapsed time: 2661.46
2024-07-25 17:55:21 - [34m[1mLOGS   [0m - Epoch:  15 [  119966/  200000], loss: {'classification': 33.3021, 'neural_augmentation': 0.4623, 'total_loss': 33.7644}, LR: [0.000419, 0.000419], Avg. batch load time: 0.026, Elapsed time: 2786.92
2024-07-25 17:56:29 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 17:56:29 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-25 17:56:31 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:31 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:31 - [32m[1mINFO   [0m - Checkpoints saved after 119999 updates at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
[31m======================================================================================================================================================[0m
2024-07-25 17:56:32 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 17:56:33 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-25 17:56:34 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:34 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:34 - [32m[1mINFO   [0m - Checkpoints saved after 119999 updates at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
[31m======================================================================================================================================================[0m
2024-07-25 17:56:36 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 17:56:36 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-25 17:56:37 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:38 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:38 - [32m[1mINFO   [0m - Checkpoints saved after 119999 updates at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
[31m======================================================================================================================================================[0m
2024-07-25 17:56:39 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 17:56:39 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-25 17:56:41 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:41 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:41 - [32m[1mINFO   [0m - Checkpoints saved after 119999 updates at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
[31m======================================================================================================================================================[0m
2024-07-25 17:56:44 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 17:56:44 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-25 17:56:46 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:46 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:46 - [32m[1mINFO   [0m - Checkpoints saved after 119999 updates at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
[31m======================================================================================================================================================[0m
2024-07-25 17:56:47 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 17:56:48 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-25 17:56:49 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:49 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:49 - [32m[1mINFO   [0m - Checkpoints saved after 119999 updates at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
[31m======================================================================================================================================================[0m
2024-07-25 17:56:51 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 17:56:51 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-25 17:56:53 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:53 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:53 - [32m[1mINFO   [0m - Checkpoints saved after 119999 updates at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
[31m======================================================================================================================================================[0m
2024-07-25 17:56:55 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 17:56:55 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-25 17:56:56 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:56 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 119999 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_15_iter_119999.pt
2024-07-25 17:56:56 - [32m[1mINFO   [0m - Checkpoints saved after 119999 updates at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
[31m======================================================================================================================================================[0m
2024-07-25 17:57:54 - [34m[1mLOGS   [0m - Epoch:  15 [  120028/  200000], loss: {'classification': 33.3161, 'neural_augmentation': 0.4624, 'total_loss': 33.7785}, LR: [0.000419, 0.000419], Avg. batch load time: 0.025, Elapsed time: 2940.15
2024-07-25 18:00:00 - [34m[1mLOGS   [0m - Epoch:  15 [  120091/  200000], loss: {'classification': 33.3318, 'neural_augmentation': 0.4624, 'total_loss': 33.7942}, LR: [0.000418, 0.000418], Avg. batch load time: 0.024, Elapsed time: 3065.83
2024-07-25 18:02:10 - [34m[1mLOGS   [0m - Epoch:  15 [  120153/  200000], loss: {'classification': 33.3423, 'neural_augmentation': 0.4625, 'total_loss': 33.8049}, LR: [0.000418, 0.000418], Avg. batch load time: 0.023, Elapsed time: 3196.39
2024-07-25 18:04:16 - [34m[1mLOGS   [0m - Epoch:  15 [  120216/  200000], loss: {'classification': 33.3509, 'neural_augmentation': 0.4626, 'total_loss': 33.8135}, LR: [0.000417, 0.000417], Avg. batch load time: 0.022, Elapsed time: 3321.92
2024-07-25 18:06:21 - [34m[1mLOGS   [0m - Epoch:  15 [  120278/  200000], loss: {'classification': 33.3622, 'neural_augmentation': 0.4627, 'total_loss': 33.8249}, LR: [0.000417, 0.000417], Avg. batch load time: 0.021, Elapsed time: 3447.49
2024-07-25 18:08:27 - [34m[1mLOGS   [0m - Epoch:  15 [  120341/  200000], loss: {'classification': 33.3751, 'neural_augmentation': 0.4628, 'total_loss': 33.8379}, LR: [0.000416, 0.000416], Avg. batch load time: 0.020, Elapsed time: 3573.06
2024-07-25 18:10:33 - [34m[1mLOGS   [0m - Epoch:  15 [  120403/  200000], loss: {'classification': 33.3862, 'neural_augmentation': 0.4629, 'total_loss': 33.8491}, LR: [0.000416, 0.000416], Avg. batch load time: 0.020, Elapsed time: 3698.81
2024-07-25 18:12:39 - [34m[1mLOGS   [0m - Epoch:  15 [  120466/  200000], loss: {'classification': 33.3923, 'neural_augmentation': 0.463, 'total_loss': 33.8553}, LR: [0.000415, 0.000415], Avg. batch load time: 0.019, Elapsed time: 3824.60
2024-07-25 18:14:44 - [34m[1mLOGS   [0m - Epoch:  15 [  120528/  200000], loss: {'classification': 33.4014, 'neural_augmentation': 0.4631, 'total_loss': 33.8645}, LR: [0.000415, 0.000415], Avg. batch load time: 0.018, Elapsed time: 3950.21
2024-07-25 18:16:50 - [34m[1mLOGS   [0m - Epoch:  15 [  120591/  200000], loss: {'classification': 33.4073, 'neural_augmentation': 0.4632, 'total_loss': 33.8705}, LR: [0.000414, 0.000414], Avg. batch load time: 0.018, Elapsed time: 4075.73
2024-07-25 18:18:55 - [34m[1mLOGS   [0m - Epoch:  15 [  120653/  200000], loss: {'classification': 33.4155, 'neural_augmentation': 0.4633, 'total_loss': 33.8788}, LR: [0.000414, 0.000414], Avg. batch load time: 0.017, Elapsed time: 4200.97
2024-07-25 18:21:02 - [34m[1mLOGS   [0m - Epoch:  15 [  120716/  200000], loss: {'classification': 33.423, 'neural_augmentation': 0.4635, 'total_loss': 33.8864}, LR: [0.000413, 0.000413], Avg. batch load time: 0.017, Elapsed time: 4328.26
2024-07-25 18:23:08 - [34m[1mLOGS   [0m - Epoch:  15 [  120778/  200000], loss: {'classification': 33.4306, 'neural_augmentation': 0.4636, 'total_loss': 33.8942}, LR: [0.000412, 0.000412], Avg. batch load time: 0.016, Elapsed time: 4453.87
2024-07-25 18:25:14 - [34m[1mLOGS   [0m - Epoch:  15 [  120841/  200000], loss: {'classification': 33.4381, 'neural_augmentation': 0.4637, 'total_loss': 33.9018}, LR: [0.000412, 0.000412], Avg. batch load time: 0.016, Elapsed time: 4579.61
2024-07-25 18:27:25 - [34m[1mLOGS   [0m - Epoch:  15 [  120903/  200000], loss: {'classification': 33.4439, 'neural_augmentation': 0.4639, 'total_loss': 33.9078}, LR: [0.000411, 0.000411], Avg. batch load time: 0.015, Elapsed time: 4711.50
2024-07-25 18:29:31 - [34m[1mLOGS   [0m - Epoch:  15 [  120966/  200000], loss: {'classification': 33.4499, 'neural_augmentation': 0.464, 'total_loss': 33.9139}, LR: [0.000411, 0.000411], Avg. batch load time: 0.015, Elapsed time: 4837.10
2024-07-25 18:31:37 - [34m[1mLOGS   [0m - Epoch:  15 [  121028/  200000], loss: {'classification': 33.454, 'neural_augmentation': 0.4641, 'total_loss': 33.9182}, LR: [0.00041, 0.00041], Avg. batch load time: 0.015, Elapsed time: 4962.76
2024-07-25 18:33:42 - [34m[1mLOGS   [0m - Epoch:  15 [  121091/  200000], loss: {'classification': 33.459, 'neural_augmentation': 0.4643, 'total_loss': 33.9232}, LR: [0.00041, 0.00041], Avg. batch load time: 0.014, Elapsed time: 5088.48
2024-07-25 18:35:48 - [34m[1mLOGS   [0m - Epoch:  15 [  121153/  200000], loss: {'classification': 33.4624, 'neural_augmentation': 0.4644, 'total_loss': 33.9268}, LR: [0.000409, 0.000409], Avg. batch load time: 0.014, Elapsed time: 5214.10
2024-07-25 18:37:53 - [34m[1mLOGS   [0m - Epoch:  15 [  121216/  200000], loss: {'classification': 33.4698, 'neural_augmentation': 0.4645, 'total_loss': 33.9343}, LR: [0.000409, 0.000409], Avg. batch load time: 0.014, Elapsed time: 5339.51
2024-07-25 18:39:59 - [34m[1mLOGS   [0m - Epoch:  15 [  121278/  200000], loss: {'classification': 33.4757, 'neural_augmentation': 0.4647, 'total_loss': 33.9404}, LR: [0.000408, 0.000408], Avg. batch load time: 0.013, Elapsed time: 5465.33
2024-07-25 18:42:05 - [34m[1mLOGS   [0m - Epoch:  15 [  121341/  200000], loss: {'classification': 33.4815, 'neural_augmentation': 0.4648, 'total_loss': 33.9463}, LR: [0.000408, 0.000408], Avg. batch load time: 0.013, Elapsed time: 5590.83
2024-07-25 18:44:11 - [34m[1mLOGS   [0m - Epoch:  15 [  121403/  200000], loss: {'classification': 33.4872, 'neural_augmentation': 0.465, 'total_loss': 33.9522}, LR: [0.000407, 0.000407], Avg. batch load time: 0.013, Elapsed time: 5716.56
2024-07-25 18:46:16 - [34m[1mLOGS   [0m - Epoch:  15 [  121466/  200000], loss: {'classification': 33.4914, 'neural_augmentation': 0.4651, 'total_loss': 33.9565}, LR: [0.000407, 0.000407], Avg. batch load time: 0.012, Elapsed time: 5842.34
2024-07-25 18:48:23 - [34m[1mLOGS   [0m - Epoch:  15 [  121528/  200000], loss: {'classification': 33.494, 'neural_augmentation': 0.4652, 'total_loss': 33.9593}, LR: [0.000406, 0.000406], Avg. batch load time: 0.012, Elapsed time: 5969.50
2024-07-25 18:50:29 - [34m[1mLOGS   [0m - Epoch:  15 [  121591/  200000], loss: {'classification': 33.4994, 'neural_augmentation': 0.4654, 'total_loss': 33.9648}, LR: [0.000406, 0.000406], Avg. batch load time: 0.012, Elapsed time: 6095.14
2024-07-25 18:52:38 - [34m[1mLOGS   [0m - Epoch:  15 [  121653/  200000], loss: {'classification': 33.5043, 'neural_augmentation': 0.4655, 'total_loss': 33.9698}, LR: [0.000405, 0.000405], Avg. batch load time: 0.012, Elapsed time: 6224.16
2024-07-25 18:54:47 - [34m[1mLOGS   [0m - Epoch:  15 [  121716/  200000], loss: {'classification': 33.5073, 'neural_augmentation': 0.4657, 'total_loss': 33.973}, LR: [0.000404, 0.000404], Avg. batch load time: 0.012, Elapsed time: 6353.07
2024-07-25 18:56:52 - [34m[1mLOGS   [0m - Epoch:  15 [  121778/  200000], loss: {'classification': 33.5089, 'neural_augmentation': 0.4658, 'total_loss': 33.9747}, LR: [0.000404, 0.000404], Avg. batch load time: 0.011, Elapsed time: 6478.28
2024-07-25 18:58:58 - [34m[1mLOGS   [0m - Epoch:  15 [  121841/  200000], loss: {'classification': 33.5127, 'neural_augmentation': 0.466, 'total_loss': 33.9787}, LR: [0.000403, 0.000403], Avg. batch load time: 0.011, Elapsed time: 6604.01
2024-07-25 19:01:03 - [34m[1mLOGS   [0m - Epoch:  15 [  121903/  200000], loss: {'classification': 33.5158, 'neural_augmentation': 0.4661, 'total_loss': 33.982}, LR: [0.000403, 0.000403], Avg. batch load time: 0.011, Elapsed time: 6729.46
2024-07-25 19:03:09 - [34m[1mLOGS   [0m - Epoch:  15 [  121966/  200000], loss: {'classification': 33.5182, 'neural_augmentation': 0.4663, 'total_loss': 33.9845}, LR: [0.000402, 0.000402], Avg. batch load time: 0.011, Elapsed time: 6855.01
2024-07-25 19:05:15 - [34m[1mLOGS   [0m - Epoch:  15 [  122028/  200000], loss: {'classification': 33.5211, 'neural_augmentation': 0.4664, 'total_loss': 33.9875}, LR: [0.000402, 0.000402], Avg. batch load time: 0.011, Elapsed time: 6980.77
2024-07-25 19:07:20 - [34m[1mLOGS   [0m - Epoch:  15 [  122091/  200000], loss: {'classification': 33.5228, 'neural_augmentation': 0.4666, 'total_loss': 33.9894}, LR: [0.000401, 0.000401], Avg. batch load time: 0.010, Elapsed time: 7106.46
2024-07-25 19:09:26 - [34m[1mLOGS   [0m - Epoch:  15 [  122153/  200000], loss: {'classification': 33.5271, 'neural_augmentation': 0.4667, 'total_loss': 33.9939}, LR: [0.000401, 0.000401], Avg. batch load time: 0.010, Elapsed time: 7232.00
2024-07-25 19:11:31 - [34m[1mLOGS   [0m - Epoch:  15 [  122216/  200000], loss: {'classification': 33.5305, 'neural_augmentation': 0.4669, 'total_loss': 33.9973}, LR: [0.0004, 0.0004], Avg. batch load time: 0.010, Elapsed time: 7357.54
2024-07-25 19:13:39 - [34m[1mLOGS   [0m - Epoch:  15 [  122278/  200000], loss: {'classification': 33.5322, 'neural_augmentation': 0.467, 'total_loss': 33.9992}, LR: [0.0004, 0.0004], Avg. batch load time: 0.010, Elapsed time: 7484.87
2024-07-25 19:15:45 - [34m[1mLOGS   [0m - Epoch:  15 [  122341/  200000], loss: {'classification': 33.5339, 'neural_augmentation': 0.4672, 'total_loss': 34.001}, LR: [0.000399, 0.000399], Avg. batch load time: 0.010, Elapsed time: 7610.77
2024-07-25 19:17:50 - [34m[1mLOGS   [0m - Epoch:  15 [  122403/  200000], loss: {'classification': 33.5364, 'neural_augmentation': 0.4673, 'total_loss': 34.0038}, LR: [0.000399, 0.000399], Avg. batch load time: 0.010, Elapsed time: 7736.25
2024-07-25 19:20:02 - [34m[1mLOGS   [0m - Epoch:  15 [  122466/  200000], loss: {'classification': 33.5375, 'neural_augmentation': 0.4675, 'total_loss': 34.005}, LR: [0.000398, 0.000398], Avg. batch load time: 0.009, Elapsed time: 7868.34
2024-07-25 19:22:08 - [34m[1mLOGS   [0m - Epoch:  15 [  122528/  200000], loss: {'classification': 33.5378, 'neural_augmentation': 0.4676, 'total_loss': 34.0055}, LR: [0.000398, 0.000398], Avg. batch load time: 0.009, Elapsed time: 7993.57
2024-07-25 19:24:13 - [34m[1mLOGS   [0m - Epoch:  15 [  122591/  200000], loss: {'classification': 33.5387, 'neural_augmentation': 0.4678, 'total_loss': 34.0065}, LR: [0.000397, 0.000397], Avg. batch load time: 0.009, Elapsed time: 8119.20
2024-07-25 19:26:19 - [34m[1mLOGS   [0m - Epoch:  15 [  122653/  200000], loss: {'classification': 33.5413, 'neural_augmentation': 0.4679, 'total_loss': 34.0093}, LR: [0.000397, 0.000397], Avg. batch load time: 0.009, Elapsed time: 8244.63
2024-07-25 19:28:24 - [34m[1mLOGS   [0m - Epoch:  15 [  122716/  200000], loss: {'classification': 33.5434, 'neural_augmentation': 0.4681, 'total_loss': 34.0115}, LR: [0.000396, 0.000396], Avg. batch load time: 0.009, Elapsed time: 8370.47
2024-07-25 19:30:30 - [34m[1mLOGS   [0m - Epoch:  15 [  122778/  200000], loss: {'classification': 33.5448, 'neural_augmentation': 0.4682, 'total_loss': 34.013}, LR: [0.000396, 0.000396], Avg. batch load time: 0.009, Elapsed time: 8496.15
2024-07-25 19:32:36 - [34m[1mLOGS   [0m - Epoch:  15 [  122841/  200000], loss: {'classification': 33.5463, 'neural_augmentation': 0.4684, 'total_loss': 34.0147}, LR: [0.000395, 0.000395], Avg. batch load time: 0.009, Elapsed time: 8621.80
2024-07-25 19:34:41 - [34m[1mLOGS   [0m - Epoch:  15 [  122903/  200000], loss: {'classification': 33.5469, 'neural_augmentation': 0.4685, 'total_loss': 34.0154}, LR: [0.000394, 0.000394], Avg. batch load time: 0.009, Elapsed time: 8747.21
2024-07-25 19:36:47 - [34m[1mLOGS   [0m - Epoch:  15 [  122966/  200000], loss: {'classification': 33.5489, 'neural_augmentation': 0.4687, 'total_loss': 34.0176}, LR: [0.000394, 0.000394], Avg. batch load time: 0.008, Elapsed time: 8872.68
2024-07-25 19:38:52 - [34m[1mLOGS   [0m - Epoch:  15 [  123028/  200000], loss: {'classification': 33.5509, 'neural_augmentation': 0.4688, 'total_loss': 34.0197}, LR: [0.000393, 0.000393], Avg. batch load time: 0.008, Elapsed time: 8998.29
2024-07-25 19:40:59 - [34m[1mLOGS   [0m - Epoch:  15 [  123091/  200000], loss: {'classification': 33.553, 'neural_augmentation': 0.469, 'total_loss': 34.022}, LR: [0.000393, 0.000393], Avg. batch load time: 0.008, Elapsed time: 9125.37
2024-07-25 19:43:05 - [34m[1mLOGS   [0m - Epoch:  15 [  123153/  200000], loss: {'classification': 33.5552, 'neural_augmentation': 0.4691, 'total_loss': 34.0243}, LR: [0.000392, 0.000392], Avg. batch load time: 0.008, Elapsed time: 9251.02
2024-07-25 19:45:17 - [34m[1mLOGS   [0m - Epoch:  15 [  123216/  200000], loss: {'classification': 33.5565, 'neural_augmentation': 0.4693, 'total_loss': 34.0257}, LR: [0.000392, 0.000392], Avg. batch load time: 0.008, Elapsed time: 9383.08
2024-07-25 19:47:23 - [34m[1mLOGS   [0m - Epoch:  15 [  123278/  200000], loss: {'classification': 33.559, 'neural_augmentation': 0.4694, 'total_loss': 34.0285}, LR: [0.000391, 0.000391], Avg. batch load time: 0.008, Elapsed time: 9508.67
2024-07-25 19:49:28 - [34m[1mLOGS   [0m - Epoch:  15 [  123341/  200000], loss: {'classification': 33.5606, 'neural_augmentation': 0.4696, 'total_loss': 34.0301}, LR: [0.000391, 0.000391], Avg. batch load time: 0.008, Elapsed time: 9634.37
2024-07-25 19:51:34 - [34m[1mLOGS   [0m - Epoch:  15 [  123403/  200000], loss: {'classification': 33.561, 'neural_augmentation': 0.4698, 'total_loss': 34.0308}, LR: [0.00039, 0.00039], Avg. batch load time: 0.008, Elapsed time: 9760.06
2024-07-25 19:53:40 - [34m[1mLOGS   [0m - Epoch:  15 [  123466/  200000], loss: {'classification': 33.5624, 'neural_augmentation': 0.4699, 'total_loss': 34.0324}, LR: [0.00039, 0.00039], Avg. batch load time: 0.008, Elapsed time: 9885.68
2024-07-25 19:55:45 - [34m[1mLOGS   [0m - Epoch:  15 [  123528/  200000], loss: {'classification': 33.5635, 'neural_augmentation': 0.4701, 'total_loss': 34.0335}, LR: [0.000389, 0.000389], Avg. batch load time: 0.008, Elapsed time: 10011.15
2024-07-25 19:57:51 - [34m[1mLOGS   [0m - Epoch:  15 [  123591/  200000], loss: {'classification': 33.5661, 'neural_augmentation': 0.4702, 'total_loss': 34.0363}, LR: [0.000389, 0.000389], Avg. batch load time: 0.007, Elapsed time: 10137.06
2024-07-25 19:59:56 - [34m[1mLOGS   [0m - Epoch:  15 [  123653/  200000], loss: {'classification': 33.5663, 'neural_augmentation': 0.4704, 'total_loss': 34.0367}, LR: [0.000388, 0.000388], Avg. batch load time: 0.007, Elapsed time: 10262.49
2024-07-25 20:02:02 - [34m[1mLOGS   [0m - Epoch:  15 [  123716/  200000], loss: {'classification': 33.5671, 'neural_augmentation': 0.4705, 'total_loss': 34.0377}, LR: [0.000388, 0.000388], Avg. batch load time: 0.007, Elapsed time: 10388.20
2024-07-25 20:04:08 - [34m[1mLOGS   [0m - Epoch:  15 [  123778/  200000], loss: {'classification': 33.568, 'neural_augmentation': 0.4707, 'total_loss': 34.0387}, LR: [0.000387, 0.000387], Avg. batch load time: 0.007, Elapsed time: 10513.92
2024-07-25 20:06:15 - [34m[1mLOGS   [0m - Epoch:  15 [  123841/  200000], loss: {'classification': 33.5693, 'neural_augmentation': 0.4708, 'total_loss': 34.0401}, LR: [0.000387, 0.000387], Avg. batch load time: 0.007, Elapsed time: 10641.07
2024-07-25 20:08:21 - [34m[1mLOGS   [0m - Epoch:  15 [  123903/  200000], loss: {'classification': 33.5704, 'neural_augmentation': 0.471, 'total_loss': 34.0414}, LR: [0.000386, 0.000386], Avg. batch load time: 0.007, Elapsed time: 10766.79
2024-07-25 20:10:32 - [34m[1mLOGS   [0m - Epoch:  15 [  123966/  200000], loss: {'classification': 33.5716, 'neural_augmentation': 0.4711, 'total_loss': 34.0427}, LR: [0.000386, 0.000386], Avg. batch load time: 0.007, Elapsed time: 10897.66
2024-07-25 20:12:37 - [34m[1mLOGS   [0m - Epoch:  15 [  124028/  200000], loss: {'classification': 33.5716, 'neural_augmentation': 0.4713, 'total_loss': 34.0429}, LR: [0.000385, 0.000385], Avg. batch load time: 0.007, Elapsed time: 11023.39
2024-07-25 20:14:43 - [34m[1mLOGS   [0m - Epoch:  15 [  124091/  200000], loss: {'classification': 33.5706, 'neural_augmentation': 0.4715, 'total_loss': 34.0421}, LR: [0.000384, 0.000384], Avg. batch load time: 0.007, Elapsed time: 11148.91
2024-07-25 20:16:49 - [34m[1mLOGS   [0m - Epoch:  15 [  124153/  200000], loss: {'classification': 33.5708, 'neural_augmentation': 0.4716, 'total_loss': 34.0424}, LR: [0.000384, 0.000384], Avg. batch load time: 0.007, Elapsed time: 11274.64
2024-07-25 20:18:54 - [34m[1mLOGS   [0m - Epoch:  15 [  124216/  200000], loss: {'classification': 33.5712, 'neural_augmentation': 0.4718, 'total_loss': 34.043}, LR: [0.000383, 0.000383], Avg. batch load time: 0.007, Elapsed time: 11400.33
2024-07-25 20:21:00 - [34m[1mLOGS   [0m - Epoch:  15 [  124278/  200000], loss: {'classification': 33.5711, 'neural_augmentation': 0.4719, 'total_loss': 34.043}, LR: [0.000383, 0.000383], Avg. batch load time: 0.007, Elapsed time: 11525.99
2024-07-25 20:23:06 - [34m[1mLOGS   [0m - Epoch:  15 [  124341/  200000], loss: {'classification': 33.5707, 'neural_augmentation': 0.4721, 'total_loss': 34.0427}, LR: [0.000382, 0.000382], Avg. batch load time: 0.007, Elapsed time: 11651.58
2024-07-25 20:25:11 - [34m[1mLOGS   [0m - Epoch:  15 [  124403/  200000], loss: {'classification': 33.5725, 'neural_augmentation': 0.4722, 'total_loss': 34.0447}, LR: [0.000382, 0.000382], Avg. batch load time: 0.007, Elapsed time: 11777.48
2024-07-25 20:27:17 - [34m[1mLOGS   [0m - Epoch:  15 [  124466/  200000], loss: {'classification': 33.5737, 'neural_augmentation': 0.4724, 'total_loss': 34.0461}, LR: [0.000381, 0.000381], Avg. batch load time: 0.006, Elapsed time: 11903.32
2024-07-25 20:29:23 - [34m[1mLOGS   [0m - Epoch:  15 [  124528/  200000], loss: {'classification': 33.5742, 'neural_augmentation': 0.4725, 'total_loss': 34.0467}, LR: [0.000381, 0.000381], Avg. batch load time: 0.006, Elapsed time: 12029.27
2024-07-25 20:31:30 - [34m[1mLOGS   [0m - Epoch:  15 [  124591/  200000], loss: {'classification': 33.5738, 'neural_augmentation': 0.4727, 'total_loss': 34.0464}, LR: [0.00038, 0.00038], Avg. batch load time: 0.006, Elapsed time: 12156.13
2024-07-25 20:33:36 - [34m[1mLOGS   [0m - Epoch:  15 [  124653/  200000], loss: {'classification': 33.5747, 'neural_augmentation': 0.4728, 'total_loss': 34.0476}, LR: [0.00038, 0.00038], Avg. batch load time: 0.006, Elapsed time: 12281.65
2024-07-25 20:35:41 - [34m[1mLOGS   [0m - Epoch:  15 [  124716/  200000], loss: {'classification': 33.5744, 'neural_augmentation': 0.473, 'total_loss': 34.0474}, LR: [0.000379, 0.000379], Avg. batch load time: 0.006, Elapsed time: 12407.27
2024-07-25 20:37:54 - [34m[1mLOGS   [0m - Epoch:  15 [  124778/  200000], loss: {'classification': 33.5741, 'neural_augmentation': 0.4731, 'total_loss': 34.0472}, LR: [0.000379, 0.000379], Avg. batch load time: 0.006, Elapsed time: 12539.70
2024-07-25 20:39:59 - [34m[1mLOGS   [0m - Epoch:  15 [  124841/  200000], loss: {'classification': 33.5737, 'neural_augmentation': 0.4733, 'total_loss': 34.047}, LR: [0.000378, 0.000378], Avg. batch load time: 0.006, Elapsed time: 12665.37
2024-07-25 20:42:05 - [34m[1mLOGS   [0m - Epoch:  15 [  124903/  200000], loss: {'classification': 33.5738, 'neural_augmentation': 0.4734, 'total_loss': 34.0473}, LR: [0.000378, 0.000378], Avg. batch load time: 0.006, Elapsed time: 12791.06
2024-07-25 20:44:11 - [34m[1mLOGS   [0m - Epoch:  15 [  124966/  200000], loss: {'classification': 33.5738, 'neural_augmentation': 0.4736, 'total_loss': 34.0474}, LR: [0.000377, 0.000377], Avg. batch load time: 0.006, Elapsed time: 12916.93
2024-07-25 20:46:17 - [34m[1mLOGS   [0m - Epoch:  15 [  125028/  200000], loss: {'classification': 33.5737, 'neural_augmentation': 0.4738, 'total_loss': 34.0475}, LR: [0.000377, 0.000377], Avg. batch load time: 0.006, Elapsed time: 13042.62
2024-07-25 20:48:22 - [34m[1mLOGS   [0m - Epoch:  15 [  125091/  200000], loss: {'classification': 33.5727, 'neural_augmentation': 0.4739, 'total_loss': 34.0466}, LR: [0.000376, 0.000376], Avg. batch load time: 0.006, Elapsed time: 13168.00
2024-07-25 20:50:27 - [34m[1mLOGS   [0m - Epoch:  15 [  125153/  200000], loss: {'classification': 33.5723, 'neural_augmentation': 0.474, 'total_loss': 34.0464}, LR: [0.000376, 0.000376], Avg. batch load time: 0.006, Elapsed time: 13293.43
2024-07-25 20:52:33 - [34m[1mLOGS   [0m - Epoch:  15 [  125216/  200000], loss: {'classification': 33.5724, 'neural_augmentation': 0.4742, 'total_loss': 34.0467}, LR: [0.000375, 0.000375], Avg. batch load time: 0.006, Elapsed time: 13419.24
2024-07-25 20:54:39 - [34m[1mLOGS   [0m - Epoch:  15 [  125278/  200000], loss: {'classification': 33.5721, 'neural_augmentation': 0.4744, 'total_loss': 34.0464}, LR: [0.000375, 0.000375], Avg. batch load time: 0.006, Elapsed time: 13545.18
2024-07-25 20:56:45 - [34m[1mLOGS   [0m - Epoch:  15 [  125341/  200000], loss: {'classification': 33.5718, 'neural_augmentation': 0.4745, 'total_loss': 34.0463}, LR: [0.000374, 0.000374], Avg. batch load time: 0.006, Elapsed time: 13670.98
2024-07-25 20:58:52 - [34m[1mLOGS   [0m - Epoch:  15 [  125403/  200000], loss: {'classification': 33.5716, 'neural_augmentation': 0.4747, 'total_loss': 34.0463}, LR: [0.000374, 0.000374], Avg. batch load time: 0.006, Elapsed time: 13798.37
2024-07-25 21:00:58 - [34m[1mLOGS   [0m - Epoch:  15 [  125466/  200000], loss: {'classification': 33.5711, 'neural_augmentation': 0.4748, 'total_loss': 34.0459}, LR: [0.000373, 0.000373], Avg. batch load time: 0.006, Elapsed time: 13924.17
2024-07-25 21:03:13 - [34m[1mLOGS   [0m - Epoch:  15 [  125528/  200000], loss: {'classification': 33.5694, 'neural_augmentation': 0.475, 'total_loss': 34.0444}, LR: [0.000372, 0.000372], Avg. batch load time: 0.006, Elapsed time: 14059.51
2024-07-25 21:05:19 - [34m[1mLOGS   [0m - Epoch:  15 [  125591/  200000], loss: {'classification': 33.5688, 'neural_augmentation': 0.4752, 'total_loss': 34.0439}, LR: [0.000372, 0.000372], Avg. batch load time: 0.006, Elapsed time: 14184.96
2024-07-25 21:07:25 - [34m[1mLOGS   [0m - Epoch:  15 [  125653/  200000], loss: {'classification': 33.5678, 'neural_augmentation': 0.4753, 'total_loss': 34.0431}, LR: [0.000371, 0.000371], Avg. batch load time: 0.006, Elapsed time: 14310.58
2024-07-25 21:09:30 - [34m[1mLOGS   [0m - Epoch:  15 [  125716/  200000], loss: {'classification': 33.5672, 'neural_augmentation': 0.4754, 'total_loss': 34.0426}, LR: [0.000371, 0.000371], Avg. batch load time: 0.005, Elapsed time: 14435.97
2024-07-25 21:11:36 - [34m[1mLOGS   [0m - Epoch:  15 [  125778/  200000], loss: {'classification': 33.5674, 'neural_augmentation': 0.4756, 'total_loss': 34.043}, LR: [0.00037, 0.00037], Avg. batch load time: 0.005, Elapsed time: 14561.58
2024-07-25 21:13:41 - [34m[1mLOGS   [0m - Epoch:  15 [  125841/  200000], loss: {'classification': 33.5671, 'neural_augmentation': 0.4758, 'total_loss': 34.0429}, LR: [0.00037, 0.00037], Avg. batch load time: 0.005, Elapsed time: 14687.46
2024-07-25 21:15:47 - [34m[1mLOGS   [0m - Epoch:  15 [  125903/  200000], loss: {'classification': 33.5667, 'neural_augmentation': 0.4759, 'total_loss': 34.0426}, LR: [0.000369, 0.000369], Avg. batch load time: 0.005, Elapsed time: 14813.10
2024-07-25 21:17:53 - [34m[1mLOGS   [0m - Epoch:  15 [  125966/  200000], loss: {'classification': 33.5663, 'neural_augmentation': 0.4761, 'total_loss': 34.0424}, LR: [0.000369, 0.000369], Avg. batch load time: 0.005, Elapsed time: 14938.98
2024-07-25 21:19:59 - [34m[1mLOGS   [0m - Epoch:  15 [  126028/  200000], loss: {'classification': 33.567, 'neural_augmentation': 0.4762, 'total_loss': 34.0433}, LR: [0.000368, 0.000368], Avg. batch load time: 0.005, Elapsed time: 15064.85
2024-07-25 21:22:04 - [34m[1mLOGS   [0m - Epoch:  15 [  126091/  200000], loss: {'classification': 33.5672, 'neural_augmentation': 0.4764, 'total_loss': 34.0436}, LR: [0.000368, 0.000368], Avg. batch load time: 0.005, Elapsed time: 15190.36
2024-07-25 21:24:11 - [34m[1mLOGS   [0m - Epoch:  15 [  126153/  200000], loss: {'classification': 33.5668, 'neural_augmentation': 0.4765, 'total_loss': 34.0434}, LR: [0.000367, 0.000367], Avg. batch load time: 0.005, Elapsed time: 15317.50
2024-07-25 21:26:17 - [34m[1mLOGS   [0m - Epoch:  15 [  126216/  200000], loss: {'classification': 33.5666, 'neural_augmentation': 0.4767, 'total_loss': 34.0433}, LR: [0.000367, 0.000367], Avg. batch load time: 0.005, Elapsed time: 15442.86
2024-07-25 21:28:29 - [34m[1mLOGS   [0m - Epoch:  15 [  126278/  200000], loss: {'classification': 33.5661, 'neural_augmentation': 0.4768, 'total_loss': 34.0429}, LR: [0.000366, 0.000366], Avg. batch load time: 0.005, Elapsed time: 15574.85
2024-07-25 21:30:38 - [34m[1mLOGS   [0m - Epoch:  15 [  126341/  200000], loss: {'classification': 33.5653, 'neural_augmentation': 0.477, 'total_loss': 34.0422}, LR: [0.000366, 0.000366], Avg. batch load time: 0.005, Elapsed time: 15703.93
2024-07-25 21:32:44 - [34m[1mLOGS   [0m - Epoch:  15 [  126403/  200000], loss: {'classification': 33.5646, 'neural_augmentation': 0.4771, 'total_loss': 34.0417}, LR: [0.000365, 0.000365], Avg. batch load time: 0.005, Elapsed time: 15829.66
2024-07-25 21:34:49 - [34m[1mLOGS   [0m - Epoch:  15 [  126466/  200000], loss: {'classification': 33.5646, 'neural_augmentation': 0.4773, 'total_loss': 34.0419}, LR: [0.000365, 0.000365], Avg. batch load time: 0.005, Elapsed time: 15955.07
2024-07-25 21:36:55 - [34m[1mLOGS   [0m - Epoch:  15 [  126528/  200000], loss: {'classification': 33.5638, 'neural_augmentation': 0.4774, 'total_loss': 34.0412}, LR: [0.000364, 0.000364], Avg. batch load time: 0.005, Elapsed time: 16080.56
2024-07-25 21:39:00 - [34m[1mLOGS   [0m - Epoch:  15 [  126591/  200000], loss: {'classification': 33.5628, 'neural_augmentation': 0.4776, 'total_loss': 34.0403}, LR: [0.000364, 0.000364], Avg. batch load time: 0.005, Elapsed time: 16206.05
2024-07-25 21:40:37 - [34m[1mLOGS   [0m - *** Training summary for epoch 15
	 loss={'classification': 33.5617, 'neural_augmentation': 0.4777, 'total_loss': 34.0394}
2024-07-25 21:40:40 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_best.pt
2024-07-25 21:40:41 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-25 21:40:42 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-25 21:40:43 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 126639 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_15_iter_126639.pt
2024-07-25 21:40:44 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 126639 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_15_iter_126639.pt
[31m===========================================================================[0m
2024-07-25 21:40:46 - [32m[1mINFO   [0m - Training epoch 16
2024-07-25 21:41:54 - [34m[1mLOGS   [0m - Epoch:  16 [  126639/  200000], loss: {'classification': 33.3641, 'neural_augmentation': 0.4878, 'total_loss': 33.8519}, LR: [0.000363, 0.000363], Avg. batch load time: 66.059, Elapsed time: 68.46
2024-07-25 21:44:03 - [34m[1mLOGS   [0m - Epoch:  16 [  126701/  200000], loss: {'classification': 33.4995, 'neural_augmentation': 0.4975, 'total_loss': 33.997}, LR: [0.000363, 0.000363], Avg. batch load time: 0.134, Elapsed time: 197.73
2024-07-25 21:46:09 - [34m[1mLOGS   [0m - Epoch:  16 [  126764/  200000], loss: {'classification': 33.4392, 'neural_augmentation': 0.4974, 'total_loss': 33.9366}, LR: [0.000362, 0.000362], Avg. batch load time: 0.068, Elapsed time: 322.99
2024-07-25 21:48:14 - [34m[1mLOGS   [0m - Epoch:  16 [  126826/  200000], loss: {'classification': 33.4027, 'neural_augmentation': 0.4975, 'total_loss': 33.9003}, LR: [0.000362, 0.000362], Avg. batch load time: 0.045, Elapsed time: 448.49
2024-07-25 21:50:19 - [34m[1mLOGS   [0m - Epoch:  16 [  126889/  200000], loss: {'classification': 33.4167, 'neural_augmentation': 0.4975, 'total_loss': 33.9142}, LR: [0.000361, 0.000361], Avg. batch load time: 0.034, Elapsed time: 573.85
2024-07-25 21:52:25 - [34m[1mLOGS   [0m - Epoch:  16 [  126951/  200000], loss: {'classification': 33.4435, 'neural_augmentation': 0.4976, 'total_loss': 33.9411}, LR: [0.000361, 0.000361], Avg. batch load time: 0.028, Elapsed time: 699.18
2024-07-25 21:54:30 - [34m[1mLOGS   [0m - Epoch:  16 [  127014/  200000], loss: {'classification': 33.4318, 'neural_augmentation': 0.4978, 'total_loss': 33.9296}, LR: [0.00036, 0.00036], Avg. batch load time: 0.023, Elapsed time: 824.78
2024-07-25 21:56:36 - [34m[1mLOGS   [0m - Epoch:  16 [  127076/  200000], loss: {'classification': 33.4475, 'neural_augmentation': 0.498, 'total_loss': 33.9455}, LR: [0.00036, 0.00036], Avg. batch load time: 0.020, Elapsed time: 950.43
2024-07-25 21:58:42 - [34m[1mLOGS   [0m - Epoch:  16 [  127139/  200000], loss: {'classification': 33.4455, 'neural_augmentation': 0.4982, 'total_loss': 33.9437}, LR: [0.000359, 0.000359], Avg. batch load time: 0.018, Elapsed time: 1076.14
2024-07-25 22:00:47 - [34m[1mLOGS   [0m - Epoch:  16 [  127201/  200000], loss: {'classification': 33.4369, 'neural_augmentation': 0.4984, 'total_loss': 33.9353}, LR: [0.000359, 0.000359], Avg. batch load time: 0.016, Elapsed time: 1201.62
2024-07-25 22:02:53 - [34m[1mLOGS   [0m - Epoch:  16 [  127264/  200000], loss: {'classification': 33.4343, 'neural_augmentation': 0.4985, 'total_loss': 33.9328}, LR: [0.000358, 0.000358], Avg. batch load time: 0.014, Elapsed time: 1327.21
2024-07-25 22:04:58 - [34m[1mLOGS   [0m - Epoch:  16 [  127326/  200000], loss: {'classification': 33.4318, 'neural_augmentation': 0.4987, 'total_loss': 33.9305}, LR: [0.000358, 0.000358], Avg. batch load time: 0.013, Elapsed time: 1452.35
2024-07-25 22:07:06 - [34m[1mLOGS   [0m - Epoch:  16 [  127389/  200000], loss: {'classification': 33.4322, 'neural_augmentation': 0.4988, 'total_loss': 33.9311}, LR: [0.000357, 0.000357], Avg. batch load time: 0.012, Elapsed time: 1580.68
2024-07-25 22:09:13 - [34m[1mLOGS   [0m - Epoch:  16 [  127451/  200000], loss: {'classification': 33.4316, 'neural_augmentation': 0.499, 'total_loss': 33.9306}, LR: [0.000357, 0.000357], Avg. batch load time: 0.011, Elapsed time: 1707.45
2024-07-25 22:11:18 - [34m[1mLOGS   [0m - Epoch:  16 [  127514/  200000], loss: {'classification': 33.4273, 'neural_augmentation': 0.4991, 'total_loss': 33.9264}, LR: [0.000356, 0.000356], Avg. batch load time: 0.010, Elapsed time: 1832.89
2024-07-25 22:13:24 - [34m[1mLOGS   [0m - Epoch:  16 [  127576/  200000], loss: {'classification': 33.424, 'neural_augmentation': 0.4993, 'total_loss': 33.9233}, LR: [0.000356, 0.000356], Avg. batch load time: 0.010, Elapsed time: 1958.13
2024-07-25 22:15:29 - [34m[1mLOGS   [0m - Epoch:  16 [  127639/  200000], loss: {'classification': 33.4238, 'neural_augmentation': 0.4994, 'total_loss': 33.9233}, LR: [0.000355, 0.000355], Avg. batch load time: 0.009, Elapsed time: 2083.55
2024-07-25 22:17:35 - [34m[1mLOGS   [0m - Epoch:  16 [  127701/  200000], loss: {'classification': 33.418, 'neural_augmentation': 0.4996, 'total_loss': 33.9176}, LR: [0.000354, 0.000354], Avg. batch load time: 0.009, Elapsed time: 2209.15
2024-07-25 22:19:40 - [34m[1mLOGS   [0m - Epoch:  16 [  127764/  200000], loss: {'classification': 33.4154, 'neural_augmentation': 0.4997, 'total_loss': 33.9151}, LR: [0.000354, 0.000354], Avg. batch load time: 0.008, Elapsed time: 2334.53
2024-07-25 22:21:46 - [34m[1mLOGS   [0m - Epoch:  16 [  127826/  200000], loss: {'classification': 33.4171, 'neural_augmentation': 0.4999, 'total_loss': 33.9169}, LR: [0.000353, 0.000353], Avg. batch load time: 0.008, Elapsed time: 2459.91
2024-07-25 22:23:51 - [34m[1mLOGS   [0m - Epoch:  16 [  127889/  200000], loss: {'classification': 33.4111, 'neural_augmentation': 0.5, 'total_loss': 33.9111}, LR: [0.000353, 0.000353], Avg. batch load time: 0.008, Elapsed time: 2585.29
2024-07-25 22:25:57 - [34m[1mLOGS   [0m - Epoch:  16 [  127951/  200000], loss: {'classification': 33.4183, 'neural_augmentation': 0.5002, 'total_loss': 33.9185}, LR: [0.000352, 0.000352], Avg. batch load time: 0.007, Elapsed time: 2710.97
2024-07-25 22:28:02 - [34m[1mLOGS   [0m - Epoch:  16 [  128014/  200000], loss: {'classification': 33.4156, 'neural_augmentation': 0.5003, 'total_loss': 33.9159}, LR: [0.000352, 0.000352], Avg. batch load time: 0.007, Elapsed time: 2836.34
2024-07-25 22:30:07 - [34m[1mLOGS   [0m - Epoch:  16 [  128076/  200000], loss: {'classification': 33.4137, 'neural_augmentation': 0.5005, 'total_loss': 33.9142}, LR: [0.000351, 0.000351], Avg. batch load time: 0.007, Elapsed time: 2961.66
2024-07-25 22:32:14 - [34m[1mLOGS   [0m - Epoch:  16 [  128139/  200000], loss: {'classification': 33.4092, 'neural_augmentation': 0.5006, 'total_loss': 33.9098}, LR: [0.000351, 0.000351], Avg. batch load time: 0.006, Elapsed time: 3088.68
2024-07-25 22:34:23 - [34m[1mLOGS   [0m - Epoch:  16 [  128201/  200000], loss: {'classification': 33.4086, 'neural_augmentation': 0.5008, 'total_loss': 33.9094}, LR: [0.00035, 0.00035], Avg. batch load time: 0.006, Elapsed time: 3217.10
2024-07-25 22:36:28 - [34m[1mLOGS   [0m - Epoch:  16 [  128264/  200000], loss: {'classification': 33.4089, 'neural_augmentation': 0.5009, 'total_loss': 33.9098}, LR: [0.00035, 0.00035], Avg. batch load time: 0.006, Elapsed time: 3342.56
2024-07-25 22:38:34 - [34m[1mLOGS   [0m - Epoch:  16 [  128326/  200000], loss: {'classification': 33.4093, 'neural_augmentation': 0.501, 'total_loss': 33.9104}, LR: [0.000349, 0.000349], Avg. batch load time: 0.006, Elapsed time: 3467.94
2024-07-25 22:40:39 - [34m[1mLOGS   [0m - Epoch:  16 [  128389/  200000], loss: {'classification': 33.4075, 'neural_augmentation': 0.5012, 'total_loss': 33.9087}, LR: [0.000349, 0.000349], Avg. batch load time: 0.006, Elapsed time: 3593.34
2024-07-25 22:42:44 - [34m[1mLOGS   [0m - Epoch:  16 [  128451/  200000], loss: {'classification': 33.4056, 'neural_augmentation': 0.5013, 'total_loss': 33.907}, LR: [0.000348, 0.000348], Avg. batch load time: 0.005, Elapsed time: 3718.36
2024-07-25 22:44:50 - [34m[1mLOGS   [0m - Epoch:  16 [  128514/  200000], loss: {'classification': 33.4103, 'neural_augmentation': 0.5015, 'total_loss': 33.9118}, LR: [0.000348, 0.000348], Avg. batch load time: 0.005, Elapsed time: 3843.91
2024-07-25 22:46:55 - [34m[1mLOGS   [0m - Epoch:  16 [  128576/  200000], loss: {'classification': 33.408, 'neural_augmentation': 0.5016, 'total_loss': 33.9097}, LR: [0.000347, 0.000347], Avg. batch load time: 0.005, Elapsed time: 3969.49
2024-07-25 22:49:01 - [34m[1mLOGS   [0m - Epoch:  16 [  128639/  200000], loss: {'classification': 33.4056, 'neural_augmentation': 0.5018, 'total_loss': 33.9074}, LR: [0.000347, 0.000347], Avg. batch load time: 0.005, Elapsed time: 4094.98
2024-07-25 22:51:06 - [34m[1mLOGS   [0m - Epoch:  16 [  128701/  200000], loss: {'classification': 33.4036, 'neural_augmentation': 0.5019, 'total_loss': 33.9055}, LR: [0.000346, 0.000346], Avg. batch load time: 0.005, Elapsed time: 4220.58
2024-07-25 22:53:12 - [34m[1mLOGS   [0m - Epoch:  16 [  128764/  200000], loss: {'classification': 33.4045, 'neural_augmentation': 0.5021, 'total_loss': 33.9065}, LR: [0.000346, 0.000346], Avg. batch load time: 0.005, Elapsed time: 4345.99
2024-07-25 22:55:17 - [34m[1mLOGS   [0m - Epoch:  16 [  128826/  200000], loss: {'classification': 33.4034, 'neural_augmentation': 0.5022, 'total_loss': 33.9056}, LR: [0.000345, 0.000345], Avg. batch load time: 0.005, Elapsed time: 4471.25
2024-07-25 22:57:22 - [34m[1mLOGS   [0m - Epoch:  16 [  128889/  200000], loss: {'classification': 33.4005, 'neural_augmentation': 0.5024, 'total_loss': 33.9029}, LR: [0.000345, 0.000345], Avg. batch load time: 0.005, Elapsed time: 4596.55
2024-07-25 22:59:34 - [34m[1mLOGS   [0m - Epoch:  16 [  128951/  200000], loss: {'classification': 33.4, 'neural_augmentation': 0.5025, 'total_loss': 33.9025}, LR: [0.000344, 0.000344], Avg. batch load time: 0.004, Elapsed time: 4728.16
2024-07-25 23:01:39 - [34m[1mLOGS   [0m - Epoch:  16 [  129014/  200000], loss: {'classification': 33.4001, 'neural_augmentation': 0.5026, 'total_loss': 33.9027}, LR: [0.000344, 0.000344], Avg. batch load time: 0.004, Elapsed time: 4853.65
2024-07-25 23:03:44 - [34m[1mLOGS   [0m - Epoch:  16 [  129076/  200000], loss: {'classification': 33.3983, 'neural_augmentation': 0.5028, 'total_loss': 33.9011}, LR: [0.000343, 0.000343], Avg. batch load time: 0.004, Elapsed time: 4978.82
2024-07-25 23:05:50 - [34m[1mLOGS   [0m - Epoch:  16 [  129139/  200000], loss: {'classification': 33.3967, 'neural_augmentation': 0.5029, 'total_loss': 33.8996}, LR: [0.000343, 0.000343], Avg. batch load time: 0.004, Elapsed time: 5104.30
2024-07-25 23:07:55 - [34m[1mLOGS   [0m - Epoch:  16 [  129201/  200000], loss: {'classification': 33.3954, 'neural_augmentation': 0.5031, 'total_loss': 33.8985}, LR: [0.000342, 0.000342], Avg. batch load time: 0.004, Elapsed time: 5229.79
2024-07-25 23:10:01 - [34m[1mLOGS   [0m - Epoch:  16 [  129264/  200000], loss: {'classification': 33.3934, 'neural_augmentation': 0.5032, 'total_loss': 33.8966}, LR: [0.000342, 0.000342], Avg. batch load time: 0.004, Elapsed time: 5355.26
2024-07-25 23:12:06 - [34m[1mLOGS   [0m - Epoch:  16 [  129326/  200000], loss: {'classification': 33.3918, 'neural_augmentation': 0.5034, 'total_loss': 33.8952}, LR: [0.000341, 0.000341], Avg. batch load time: 0.004, Elapsed time: 5480.86
2024-07-25 23:14:12 - [34m[1mLOGS   [0m - Epoch:  16 [  129389/  200000], loss: {'classification': 33.3894, 'neural_augmentation': 0.5035, 'total_loss': 33.8929}, LR: [0.000341, 0.000341], Avg. batch load time: 0.004, Elapsed time: 5606.24
2024-07-25 23:16:17 - [34m[1mLOGS   [0m - Epoch:  16 [  129451/  200000], loss: {'classification': 33.3893, 'neural_augmentation': 0.5037, 'total_loss': 33.893}, LR: [0.00034, 0.00034], Avg. batch load time: 0.004, Elapsed time: 5731.47
2024-07-25 23:18:22 - [34m[1mLOGS   [0m - Epoch:  16 [  129514/  200000], loss: {'classification': 33.3892, 'neural_augmentation': 0.5038, 'total_loss': 33.8931}, LR: [0.00034, 0.00034], Avg. batch load time: 0.004, Elapsed time: 5856.87
2024-07-25 23:20:28 - [34m[1mLOGS   [0m - Epoch:  16 [  129576/  200000], loss: {'classification': 33.3877, 'neural_augmentation': 0.504, 'total_loss': 33.8917}, LR: [0.000339, 0.000339], Avg. batch load time: 0.004, Elapsed time: 5982.20
2024-07-25 23:22:33 - [34m[1mLOGS   [0m - Epoch:  16 [  129639/  200000], loss: {'classification': 33.3826, 'neural_augmentation': 0.5041, 'total_loss': 33.8867}, LR: [0.000339, 0.000339], Avg. batch load time: 0.004, Elapsed time: 6107.34
2024-07-25 23:24:41 - [34m[1mLOGS   [0m - Epoch:  16 [  129701/  200000], loss: {'classification': 33.3806, 'neural_augmentation': 0.5043, 'total_loss': 33.8849}, LR: [0.000338, 0.000338], Avg. batch load time: 0.004, Elapsed time: 6235.79
2024-07-25 23:26:50 - [34m[1mLOGS   [0m - Epoch:  16 [  129764/  200000], loss: {'classification': 33.3788, 'neural_augmentation': 0.5044, 'total_loss': 33.8832}, LR: [0.000338, 0.000338], Avg. batch load time: 0.004, Elapsed time: 6364.20
2024-07-25 23:28:55 - [34m[1mLOGS   [0m - Epoch:  16 [  129826/  200000], loss: {'classification': 33.3766, 'neural_augmentation': 0.5046, 'total_loss': 33.8812}, LR: [0.000337, 0.000337], Avg. batch load time: 0.004, Elapsed time: 6489.44
2024-07-25 23:31:01 - [34m[1mLOGS   [0m - Epoch:  16 [  129889/  200000], loss: {'classification': 33.3761, 'neural_augmentation': 0.5047, 'total_loss': 33.8808}, LR: [0.000337, 0.000337], Avg. batch load time: 0.003, Elapsed time: 6615.00
2024-07-25 23:33:06 - [34m[1mLOGS   [0m - Epoch:  16 [  129951/  200000], loss: {'classification': 33.3747, 'neural_augmentation': 0.5049, 'total_loss': 33.8796}, LR: [0.000336, 0.000336], Avg. batch load time: 0.003, Elapsed time: 6740.40
2024-07-25 23:35:12 - [34m[1mLOGS   [0m - Epoch:  16 [  130014/  200000], loss: {'classification': 33.3743, 'neural_augmentation': 0.505, 'total_loss': 33.8793}, LR: [0.000336, 0.000336], Avg. batch load time: 0.003, Elapsed time: 6866.02
2024-07-25 23:37:17 - [34m[1mLOGS   [0m - Epoch:  16 [  130076/  200000], loss: {'classification': 33.375, 'neural_augmentation': 0.5052, 'total_loss': 33.8802}, LR: [0.000335, 0.000335], Avg. batch load time: 0.003, Elapsed time: 6991.41
2024-07-25 23:39:23 - [34m[1mLOGS   [0m - Epoch:  16 [  130139/  200000], loss: {'classification': 33.3753, 'neural_augmentation': 0.5053, 'total_loss': 33.8806}, LR: [0.000335, 0.000335], Avg. batch load time: 0.003, Elapsed time: 7117.02
2024-07-25 23:41:28 - [34m[1mLOGS   [0m - Epoch:  16 [  130201/  200000], loss: {'classification': 33.3713, 'neural_augmentation': 0.5055, 'total_loss': 33.8768}, LR: [0.000334, 0.000334], Avg. batch load time: 0.003, Elapsed time: 7242.26
2024-07-25 23:43:33 - [34m[1mLOGS   [0m - Epoch:  16 [  130264/  200000], loss: {'classification': 33.3701, 'neural_augmentation': 0.5056, 'total_loss': 33.8757}, LR: [0.000334, 0.000334], Avg. batch load time: 0.003, Elapsed time: 7367.73
2024-07-25 23:45:38 - [34m[1mLOGS   [0m - Epoch:  16 [  130326/  200000], loss: {'classification': 33.37, 'neural_augmentation': 0.5058, 'total_loss': 33.8758}, LR: [0.000333, 0.000333], Avg. batch load time: 0.003, Elapsed time: 7492.90
2024-07-25 23:47:44 - [34m[1mLOGS   [0m - Epoch:  16 [  130389/  200000], loss: {'classification': 33.3702, 'neural_augmentation': 0.5059, 'total_loss': 33.8761}, LR: [0.000333, 0.000333], Avg. batch load time: 0.003, Elapsed time: 7618.53
2024-07-25 23:49:51 - [34m[1mLOGS   [0m - Epoch:  16 [  130451/  200000], loss: {'classification': 33.369, 'neural_augmentation': 0.5061, 'total_loss': 33.8751}, LR: [0.000332, 0.000332], Avg. batch load time: 0.003, Elapsed time: 7745.34
2024-07-25 23:52:00 - [34m[1mLOGS   [0m - Epoch:  16 [  130514/  200000], loss: {'classification': 33.3688, 'neural_augmentation': 0.5062, 'total_loss': 33.875}, LR: [0.000332, 0.000332], Avg. batch load time: 0.003, Elapsed time: 7874.03
2024-07-25 23:54:05 - [34m[1mLOGS   [0m - Epoch:  16 [  130576/  200000], loss: {'classification': 33.3679, 'neural_augmentation': 0.5064, 'total_loss': 33.8743}, LR: [0.000331, 0.000331], Avg. batch load time: 0.003, Elapsed time: 7999.49
2024-07-25 23:56:11 - [34m[1mLOGS   [0m - Epoch:  16 [  130639/  200000], loss: {'classification': 33.3658, 'neural_augmentation': 0.5065, 'total_loss': 33.8723}, LR: [0.000331, 0.000331], Avg. batch load time: 0.003, Elapsed time: 8125.01
2024-07-25 23:58:16 - [34m[1mLOGS   [0m - Epoch:  16 [  130701/  200000], loss: {'classification': 33.3645, 'neural_augmentation': 0.5067, 'total_loss': 33.8712}, LR: [0.00033, 0.00033], Avg. batch load time: 0.003, Elapsed time: 8250.31
2024-07-26 00:00:21 - [34m[1mLOGS   [0m - Epoch:  16 [  130764/  200000], loss: {'classification': 33.3633, 'neural_augmentation': 0.5068, 'total_loss': 33.8701}, LR: [0.00033, 0.00033], Avg. batch load time: 0.003, Elapsed time: 8375.69
2024-07-26 00:02:27 - [34m[1mLOGS   [0m - Epoch:  16 [  130826/  200000], loss: {'classification': 33.3628, 'neural_augmentation': 0.507, 'total_loss': 33.8697}, LR: [0.000329, 0.000329], Avg. batch load time: 0.003, Elapsed time: 8501.03
2024-07-26 00:04:32 - [34m[1mLOGS   [0m - Epoch:  16 [  130889/  200000], loss: {'classification': 33.3598, 'neural_augmentation': 0.5071, 'total_loss': 33.8669}, LR: [0.000329, 0.000329], Avg. batch load time: 0.003, Elapsed time: 8626.22
2024-07-26 00:06:37 - [34m[1mLOGS   [0m - Epoch:  16 [  130951/  200000], loss: {'classification': 33.3596, 'neural_augmentation': 0.5073, 'total_loss': 33.8669}, LR: [0.000328, 0.000328], Avg. batch load time: 0.003, Elapsed time: 8751.77
2024-07-26 00:08:43 - [34m[1mLOGS   [0m - Epoch:  16 [  131014/  200000], loss: {'classification': 33.3582, 'neural_augmentation': 0.5074, 'total_loss': 33.8656}, LR: [0.000327, 0.000327], Avg. batch load time: 0.003, Elapsed time: 8877.24
2024-07-26 00:10:48 - [34m[1mLOGS   [0m - Epoch:  16 [  131076/  200000], loss: {'classification': 33.3565, 'neural_augmentation': 0.5076, 'total_loss': 33.8641}, LR: [0.000327, 0.000327], Avg. batch load time: 0.003, Elapsed time: 9002.73
2024-07-26 00:12:54 - [34m[1mLOGS   [0m - Epoch:  16 [  131139/  200000], loss: {'classification': 33.3549, 'neural_augmentation': 0.5077, 'total_loss': 33.8626}, LR: [0.000326, 0.000326], Avg. batch load time: 0.003, Elapsed time: 9128.28
2024-07-26 00:14:59 - [34m[1mLOGS   [0m - Epoch:  16 [  131201/  200000], loss: {'classification': 33.3524, 'neural_augmentation': 0.5079, 'total_loss': 33.8603}, LR: [0.000326, 0.000326], Avg. batch load time: 0.003, Elapsed time: 9253.62
2024-07-26 00:17:09 - [34m[1mLOGS   [0m - Epoch:  16 [  131264/  200000], loss: {'classification': 33.3512, 'neural_augmentation': 0.508, 'total_loss': 33.8592}, LR: [0.000325, 0.000325], Avg. batch load time: 0.003, Elapsed time: 9383.64
2024-07-26 00:19:14 - [34m[1mLOGS   [0m - Epoch:  16 [  131326/  200000], loss: {'classification': 33.3501, 'neural_augmentation': 0.5082, 'total_loss': 33.8583}, LR: [0.000325, 0.000325], Avg. batch load time: 0.003, Elapsed time: 9508.86
2024-07-26 00:21:20 - [34m[1mLOGS   [0m - Epoch:  16 [  131389/  200000], loss: {'classification': 33.3487, 'neural_augmentation': 0.5083, 'total_loss': 33.8571}, LR: [0.000324, 0.000324], Avg. batch load time: 0.003, Elapsed time: 9634.52
2024-07-26 00:23:25 - [34m[1mLOGS   [0m - Epoch:  16 [  131451/  200000], loss: {'classification': 33.3466, 'neural_augmentation': 0.5085, 'total_loss': 33.8551}, LR: [0.000324, 0.000324], Avg. batch load time: 0.003, Elapsed time: 9759.87
2024-07-26 00:25:31 - [34m[1mLOGS   [0m - Epoch:  16 [  131514/  200000], loss: {'classification': 33.3452, 'neural_augmentation': 0.5086, 'total_loss': 33.8538}, LR: [0.000323, 0.000323], Avg. batch load time: 0.003, Elapsed time: 9885.07
2024-07-26 00:27:36 - [34m[1mLOGS   [0m - Epoch:  16 [  131576/  200000], loss: {'classification': 33.344, 'neural_augmentation': 0.5088, 'total_loss': 33.8528}, LR: [0.000323, 0.000323], Avg. batch load time: 0.003, Elapsed time: 10010.67
2024-07-26 00:29:42 - [34m[1mLOGS   [0m - Epoch:  16 [  131639/  200000], loss: {'classification': 33.343, 'neural_augmentation': 0.5089, 'total_loss': 33.8519}, LR: [0.000322, 0.000322], Avg. batch load time: 0.003, Elapsed time: 10136.36
2024-07-26 00:31:47 - [34m[1mLOGS   [0m - Epoch:  16 [  131701/  200000], loss: {'classification': 33.3419, 'neural_augmentation': 0.5091, 'total_loss': 33.851}, LR: [0.000322, 0.000322], Avg. batch load time: 0.003, Elapsed time: 10261.88
2024-07-26 00:33:53 - [34m[1mLOGS   [0m - Epoch:  16 [  131764/  200000], loss: {'classification': 33.343, 'neural_augmentation': 0.5092, 'total_loss': 33.8522}, LR: [0.000321, 0.000321], Avg. batch load time: 0.003, Elapsed time: 10387.59
2024-07-26 00:35:59 - [34m[1mLOGS   [0m - Epoch:  16 [  131826/  200000], loss: {'classification': 33.3419, 'neural_augmentation': 0.5094, 'total_loss': 33.8513}, LR: [0.000321, 0.000321], Avg. batch load time: 0.002, Elapsed time: 10512.95
2024-07-26 00:38:04 - [34m[1mLOGS   [0m - Epoch:  16 [  131889/  200000], loss: {'classification': 33.341, 'neural_augmentation': 0.5095, 'total_loss': 33.8506}, LR: [0.00032, 0.00032], Avg. batch load time: 0.002, Elapsed time: 10638.45
2024-07-26 00:40:10 - [34m[1mLOGS   [0m - Epoch:  16 [  131951/  200000], loss: {'classification': 33.339, 'neural_augmentation': 0.5097, 'total_loss': 33.8487}, LR: [0.00032, 0.00032], Avg. batch load time: 0.002, Elapsed time: 10764.02
2024-07-26 00:42:18 - [34m[1mLOGS   [0m - Epoch:  16 [  132014/  200000], loss: {'classification': 33.3377, 'neural_augmentation': 0.5098, 'total_loss': 33.8475}, LR: [0.000319, 0.000319], Avg. batch load time: 0.002, Elapsed time: 10892.75
2024-07-26 00:44:24 - [34m[1mLOGS   [0m - Epoch:  16 [  132076/  200000], loss: {'classification': 33.337, 'neural_augmentation': 0.5099, 'total_loss': 33.847}, LR: [0.000319, 0.000319], Avg. batch load time: 0.002, Elapsed time: 11018.40
2024-07-26 00:46:29 - [34m[1mLOGS   [0m - Epoch:  16 [  132139/  200000], loss: {'classification': 33.3359, 'neural_augmentation': 0.5101, 'total_loss': 33.8459}, LR: [0.000318, 0.000318], Avg. batch load time: 0.002, Elapsed time: 11143.55
2024-07-26 00:48:35 - [34m[1mLOGS   [0m - Epoch:  16 [  132201/  200000], loss: {'classification': 33.3346, 'neural_augmentation': 0.5102, 'total_loss': 33.8449}, LR: [0.000318, 0.000318], Avg. batch load time: 0.002, Elapsed time: 11268.98
2024-07-26 00:50:40 - [34m[1mLOGS   [0m - Epoch:  16 [  132264/  200000], loss: {'classification': 33.3343, 'neural_augmentation': 0.5104, 'total_loss': 33.8447}, LR: [0.000317, 0.000317], Avg. batch load time: 0.002, Elapsed time: 11394.73
2024-07-26 00:52:46 - [34m[1mLOGS   [0m - Epoch:  16 [  132326/  200000], loss: {'classification': 33.3311, 'neural_augmentation': 0.5105, 'total_loss': 33.8417}, LR: [0.000317, 0.000317], Avg. batch load time: 0.002, Elapsed time: 11520.07
2024-07-26 00:54:51 - [34m[1mLOGS   [0m - Epoch:  16 [  132389/  200000], loss: {'classification': 33.3294, 'neural_augmentation': 0.5107, 'total_loss': 33.8401}, LR: [0.000316, 0.000316], Avg. batch load time: 0.002, Elapsed time: 11645.33
2024-07-26 00:56:56 - [34m[1mLOGS   [0m - Epoch:  16 [  132451/  200000], loss: {'classification': 33.3287, 'neural_augmentation': 0.5108, 'total_loss': 33.8395}, LR: [0.000316, 0.000316], Avg. batch load time: 0.002, Elapsed time: 11770.60
2024-07-26 00:59:02 - [34m[1mLOGS   [0m - Epoch:  16 [  132514/  200000], loss: {'classification': 33.3276, 'neural_augmentation': 0.511, 'total_loss': 33.8386}, LR: [0.000315, 0.000315], Avg. batch load time: 0.002, Elapsed time: 11895.99
2024-07-26 01:01:07 - [34m[1mLOGS   [0m - Epoch:  16 [  132576/  200000], loss: {'classification': 33.3262, 'neural_augmentation': 0.5111, 'total_loss': 33.8373}, LR: [0.000315, 0.000315], Avg. batch load time: 0.002, Elapsed time: 12021.32
2024-07-26 01:03:12 - [34m[1mLOGS   [0m - Epoch:  16 [  132639/  200000], loss: {'classification': 33.3252, 'neural_augmentation': 0.5112, 'total_loss': 33.8364}, LR: [0.000314, 0.000314], Avg. batch load time: 0.002, Elapsed time: 12146.70
2024-07-26 01:05:18 - [34m[1mLOGS   [0m - Epoch:  16 [  132701/  200000], loss: {'classification': 33.3239, 'neural_augmentation': 0.5114, 'total_loss': 33.8353}, LR: [0.000314, 0.000314], Avg. batch load time: 0.002, Elapsed time: 12272.20
2024-07-26 01:07:23 - [34m[1mLOGS   [0m - Epoch:  16 [  132764/  200000], loss: {'classification': 33.3228, 'neural_augmentation': 0.5115, 'total_loss': 33.8344}, LR: [0.000313, 0.000313], Avg. batch load time: 0.002, Elapsed time: 12397.57
2024-07-26 01:09:33 - [34m[1mLOGS   [0m - Epoch:  16 [  132826/  200000], loss: {'classification': 33.3222, 'neural_augmentation': 0.5117, 'total_loss': 33.8339}, LR: [0.000313, 0.000313], Avg. batch load time: 0.002, Elapsed time: 12527.71
2024-07-26 01:11:39 - [34m[1mLOGS   [0m - Epoch:  16 [  132889/  200000], loss: {'classification': 33.3193, 'neural_augmentation': 0.5118, 'total_loss': 33.8311}, LR: [0.000312, 0.000312], Avg. batch load time: 0.002, Elapsed time: 12653.03
2024-07-26 01:13:44 - [34m[1mLOGS   [0m - Epoch:  16 [  132951/  200000], loss: {'classification': 33.3171, 'neural_augmentation': 0.512, 'total_loss': 33.8291}, LR: [0.000312, 0.000312], Avg. batch load time: 0.002, Elapsed time: 12778.21
2024-07-26 01:15:49 - [34m[1mLOGS   [0m - Epoch:  16 [  133014/  200000], loss: {'classification': 33.3152, 'neural_augmentation': 0.5121, 'total_loss': 33.8273}, LR: [0.000311, 0.000311], Avg. batch load time: 0.002, Elapsed time: 12903.74
2024-07-26 01:17:55 - [34m[1mLOGS   [0m - Epoch:  16 [  133076/  200000], loss: {'classification': 33.3144, 'neural_augmentation': 0.5122, 'total_loss': 33.8266}, LR: [0.000311, 0.000311], Avg. batch load time: 0.002, Elapsed time: 13029.22
2024-07-26 01:20:00 - [34m[1mLOGS   [0m - Epoch:  16 [  133139/  200000], loss: {'classification': 33.3127, 'neural_augmentation': 0.5124, 'total_loss': 33.8251}, LR: [0.00031, 0.00031], Avg. batch load time: 0.002, Elapsed time: 13154.60
2024-07-26 01:22:06 - [34m[1mLOGS   [0m - Epoch:  16 [  133201/  200000], loss: {'classification': 33.3117, 'neural_augmentation': 0.5125, 'total_loss': 33.8243}, LR: [0.00031, 0.00031], Avg. batch load time: 0.002, Elapsed time: 13280.02
2024-07-26 01:24:11 - [34m[1mLOGS   [0m - Epoch:  16 [  133264/  200000], loss: {'classification': 33.3106, 'neural_augmentation': 0.5127, 'total_loss': 33.8232}, LR: [0.000309, 0.000309], Avg. batch load time: 0.002, Elapsed time: 13405.39
2024-07-26 01:26:16 - [34m[1mLOGS   [0m - Epoch:  16 [  133326/  200000], loss: {'classification': 33.3096, 'neural_augmentation': 0.5128, 'total_loss': 33.8224}, LR: [0.000309, 0.000309], Avg. batch load time: 0.002, Elapsed time: 13530.79
2024-07-26 01:28:22 - [34m[1mLOGS   [0m - Epoch:  16 [  133389/  200000], loss: {'classification': 33.3082, 'neural_augmentation': 0.513, 'total_loss': 33.8212}, LR: [0.000309, 0.000309], Avg. batch load time: 0.002, Elapsed time: 13656.13
2024-07-26 01:30:27 - [34m[1mLOGS   [0m - Epoch:  16 [  133451/  200000], loss: {'classification': 33.3072, 'neural_augmentation': 0.5131, 'total_loss': 33.8203}, LR: [0.000308, 0.000308], Avg. batch load time: 0.002, Elapsed time: 13781.45
2024-07-26 01:32:33 - [34m[1mLOGS   [0m - Epoch:  16 [  133514/  200000], loss: {'classification': 33.3066, 'neural_augmentation': 0.5133, 'total_loss': 33.8198}, LR: [0.000308, 0.000308], Avg. batch load time: 0.002, Elapsed time: 13907.05
2024-07-26 01:34:41 - [34m[1mLOGS   [0m - Epoch:  16 [  133576/  200000], loss: {'classification': 33.3051, 'neural_augmentation': 0.5134, 'total_loss': 33.8185}, LR: [0.000307, 0.000307], Avg. batch load time: 0.002, Elapsed time: 14035.46
2024-07-26 01:36:46 - [34m[1mLOGS   [0m - Epoch:  16 [  133639/  200000], loss: {'classification': 33.3028, 'neural_augmentation': 0.5136, 'total_loss': 33.8164}, LR: [0.000307, 0.000307], Avg. batch load time: 0.002, Elapsed time: 14160.77
2024-07-26 01:38:52 - [34m[1mLOGS   [0m - Epoch:  16 [  133701/  200000], loss: {'classification': 33.3017, 'neural_augmentation': 0.5137, 'total_loss': 33.8154}, LR: [0.000306, 0.000306], Avg. batch load time: 0.002, Elapsed time: 14286.30
2024-07-26 01:40:57 - [34m[1mLOGS   [0m - Epoch:  16 [  133764/  200000], loss: {'classification': 33.2998, 'neural_augmentation': 0.5138, 'total_loss': 33.8137}, LR: [0.000306, 0.000306], Avg. batch load time: 0.002, Elapsed time: 14411.66
2024-07-26 01:43:02 - [34m[1mLOGS   [0m - Epoch:  16 [  133826/  200000], loss: {'classification': 33.2988, 'neural_augmentation': 0.514, 'total_loss': 33.8128}, LR: [0.000305, 0.000305], Avg. batch load time: 0.002, Elapsed time: 14536.86
2024-07-26 01:45:08 - [34m[1mLOGS   [0m - Epoch:  16 [  133889/  200000], loss: {'classification': 33.2975, 'neural_augmentation': 0.5141, 'total_loss': 33.8116}, LR: [0.000305, 0.000305], Avg. batch load time: 0.002, Elapsed time: 14662.20
2024-07-26 01:47:13 - [34m[1mLOGS   [0m - Epoch:  16 [  133951/  200000], loss: {'classification': 33.2968, 'neural_augmentation': 0.5143, 'total_loss': 33.8111}, LR: [0.000304, 0.000304], Avg. batch load time: 0.002, Elapsed time: 14787.56
2024-07-26 01:49:18 - [34m[1mLOGS   [0m - Epoch:  16 [  134014/  200000], loss: {'classification': 33.295, 'neural_augmentation': 0.5144, 'total_loss': 33.8094}, LR: [0.000304, 0.000304], Avg. batch load time: 0.002, Elapsed time: 14912.83
2024-07-26 01:51:24 - [34m[1mLOGS   [0m - Epoch:  16 [  134076/  200000], loss: {'classification': 33.2933, 'neural_augmentation': 0.5146, 'total_loss': 33.8079}, LR: [0.000303, 0.000303], Avg. batch load time: 0.002, Elapsed time: 15038.10
2024-07-26 01:53:29 - [34m[1mLOGS   [0m - Epoch:  16 [  134139/  200000], loss: {'classification': 33.2913, 'neural_augmentation': 0.5147, 'total_loss': 33.806}, LR: [0.000303, 0.000303], Avg. batch load time: 0.002, Elapsed time: 15163.49
2024-07-26 01:55:35 - [34m[1mLOGS   [0m - Epoch:  16 [  134201/  200000], loss: {'classification': 33.2897, 'neural_augmentation': 0.5149, 'total_loss': 33.8046}, LR: [0.000302, 0.000302], Avg. batch load time: 0.002, Elapsed time: 15289.05
2024-07-26 01:57:40 - [34m[1mLOGS   [0m - Epoch:  16 [  134264/  200000], loss: {'classification': 33.2873, 'neural_augmentation': 0.515, 'total_loss': 33.8023}, LR: [0.000302, 0.000302], Avg. batch load time: 0.002, Elapsed time: 15414.40
2024-07-26 01:59:49 - [34m[1mLOGS   [0m - Epoch:  16 [  134326/  200000], loss: {'classification': 33.2858, 'neural_augmentation': 0.5151, 'total_loss': 33.801}, LR: [0.000301, 0.000301], Avg. batch load time: 0.002, Elapsed time: 15543.19
2024-07-26 02:01:56 - [34m[1mLOGS   [0m - Epoch:  16 [  134389/  200000], loss: {'classification': 33.2846, 'neural_augmentation': 0.5153, 'total_loss': 33.7999}, LR: [0.000301, 0.000301], Avg. batch load time: 0.002, Elapsed time: 15670.33
2024-07-26 02:04:02 - [34m[1mLOGS   [0m - Epoch:  16 [  134451/  200000], loss: {'classification': 33.2834, 'neural_augmentation': 0.5154, 'total_loss': 33.7988}, LR: [0.0003, 0.0003], Avg. batch load time: 0.002, Elapsed time: 15796.08
2024-07-26 02:06:07 - [34m[1mLOGS   [0m - Epoch:  16 [  134514/  200000], loss: {'classification': 33.2822, 'neural_augmentation': 0.5156, 'total_loss': 33.7978}, LR: [0.0003, 0.0003], Avg. batch load time: 0.002, Elapsed time: 15921.56
2024-07-26 02:08:12 - [34m[1mLOGS   [0m - Epoch:  16 [  134576/  200000], loss: {'classification': 33.2811, 'neural_augmentation': 0.5157, 'total_loss': 33.7968}, LR: [0.000299, 0.000299], Avg. batch load time: 0.002, Elapsed time: 16046.83
2024-07-26 02:09:16 - [34m[1mLOGS   [0m - *** Training summary for epoch 16
	 loss={'classification': 33.281, 'neural_augmentation': 0.5158, 'total_loss': 33.7968}
2024-07-26 02:09:19 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_best.pt
2024-07-26 02:09:21 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-26 02:09:21 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-26 02:09:22 - [34m[1mLOGS   [0m - Training checkpoint for epoch 16/iteration 134608 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_16_iter_134608.pt
2024-07-26 02:09:23 - [34m[1mLOGS   [0m - Model state for epoch 16/iteration 134608 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_16_iter_134608.pt
[31m===========================================================================[0m
2024-07-26 02:09:25 - [32m[1mINFO   [0m - Training epoch 17
2024-07-26 02:10:35 - [34m[1mLOGS   [0m - Epoch:  17 [  134608/  200000], loss: {'classification': 34.4868, 'neural_augmentation': 0.5351, 'total_loss': 35.0218}, LR: [0.000299, 0.000299], Avg. batch load time: 70.290, Elapsed time: 70.54
2024-07-26 02:12:41 - [34m[1mLOGS   [0m - Epoch:  17 [  134670/  200000], loss: {'classification': 32.9818, 'neural_augmentation': 0.5348, 'total_loss': 33.5166}, LR: [0.000298, 0.000298], Avg. batch load time: 0.143, Elapsed time: 196.77
2024-07-26 02:14:47 - [34m[1mLOGS   [0m - Epoch:  17 [  134733/  200000], loss: {'classification': 33.0382, 'neural_augmentation': 0.5346, 'total_loss': 33.5728}, LR: [0.000298, 0.000298], Avg. batch load time: 0.072, Elapsed time: 322.24
2024-07-26 02:16:52 - [34m[1mLOGS   [0m - Epoch:  17 [  134795/  200000], loss: {'classification': 33.0538, 'neural_augmentation': 0.5348, 'total_loss': 33.5886}, LR: [0.000297, 0.000297], Avg. batch load time: 0.048, Elapsed time: 447.49
2024-07-26 02:18:57 - [34m[1mLOGS   [0m - Epoch:  17 [  134858/  200000], loss: {'classification': 33.0728, 'neural_augmentation': 0.5348, 'total_loss': 33.6076}, LR: [0.000297, 0.000297], Avg. batch load time: 0.037, Elapsed time: 572.73
2024-07-26 02:21:03 - [34m[1mLOGS   [0m - Epoch:  17 [  134920/  200000], loss: {'classification': 33.0551, 'neural_augmentation': 0.5349, 'total_loss': 33.59}, LR: [0.000296, 0.000296], Avg. batch load time: 0.029, Elapsed time: 698.30
2024-07-26 02:23:08 - [34m[1mLOGS   [0m - Epoch:  17 [  134983/  200000], loss: {'classification': 33.0529, 'neural_augmentation': 0.535, 'total_loss': 33.5879}, LR: [0.000296, 0.000296], Avg. batch load time: 0.025, Elapsed time: 823.75
2024-07-26 02:25:14 - [34m[1mLOGS   [0m - Epoch:  17 [  135045/  200000], loss: {'classification': 33.0528, 'neural_augmentation': 0.5352, 'total_loss': 33.588}, LR: [0.000295, 0.000295], Avg. batch load time: 0.021, Elapsed time: 949.02
2024-07-26 02:27:19 - [34m[1mLOGS   [0m - Epoch:  17 [  135108/  200000], loss: {'classification': 33.0506, 'neural_augmentation': 0.5354, 'total_loss': 33.5859}, LR: [0.000295, 0.000295], Avg. batch load time: 0.019, Elapsed time: 1074.62
2024-07-26 02:29:24 - [34m[1mLOGS   [0m - Epoch:  17 [  135170/  200000], loss: {'classification': 33.0505, 'neural_augmentation': 0.5355, 'total_loss': 33.5859}, LR: [0.000294, 0.000294], Avg. batch load time: 0.017, Elapsed time: 1199.83
2024-07-26 02:31:30 - [34m[1mLOGS   [0m - Epoch:  17 [  135233/  200000], loss: {'classification': 33.0493, 'neural_augmentation': 0.5356, 'total_loss': 33.5849}, LR: [0.000294, 0.000294], Avg. batch load time: 0.015, Elapsed time: 1324.97
2024-07-26 02:33:35 - [34m[1mLOGS   [0m - Epoch:  17 [  135295/  200000], loss: {'classification': 33.0535, 'neural_augmentation': 0.5357, 'total_loss': 33.5892}, LR: [0.000293, 0.000293], Avg. batch load time: 0.014, Elapsed time: 1450.35
2024-07-26 02:35:43 - [34m[1mLOGS   [0m - Epoch:  17 [  135358/  200000], loss: {'classification': 33.0518, 'neural_augmentation': 0.5358, 'total_loss': 33.5876}, LR: [0.000293, 0.000293], Avg. batch load time: 0.013, Elapsed time: 1578.89
2024-07-26 02:37:52 - [34m[1mLOGS   [0m - Epoch:  17 [  135420/  200000], loss: {'classification': 33.0472, 'neural_augmentation': 0.536, 'total_loss': 33.5831}, LR: [0.000293, 0.000293], Avg. batch load time: 0.012, Elapsed time: 1707.19
2024-07-26 02:39:57 - [34m[1mLOGS   [0m - Epoch:  17 [  135483/  200000], loss: {'classification': 33.0527, 'neural_augmentation': 0.5361, 'total_loss': 33.5888}, LR: [0.000292, 0.000292], Avg. batch load time: 0.011, Elapsed time: 1832.56
2024-07-26 02:42:03 - [34m[1mLOGS   [0m - Epoch:  17 [  135545/  200000], loss: {'classification': 33.052, 'neural_augmentation': 0.5363, 'total_loss': 33.5883}, LR: [0.000292, 0.000292], Avg. batch load time: 0.010, Elapsed time: 1958.00
2024-07-26 02:44:08 - [34m[1mLOGS   [0m - Epoch:  17 [  135608/  200000], loss: {'classification': 33.0498, 'neural_augmentation': 0.5365, 'total_loss': 33.5863}, LR: [0.000291, 0.000291], Avg. batch load time: 0.010, Elapsed time: 2083.18
2024-07-26 02:46:13 - [34m[1mLOGS   [0m - Epoch:  17 [  135670/  200000], loss: {'classification': 33.0503, 'neural_augmentation': 0.5366, 'total_loss': 33.5869}, LR: [0.000291, 0.000291], Avg. batch load time: 0.009, Elapsed time: 2208.72
2024-07-26 02:48:19 - [34m[1mLOGS   [0m - Epoch:  17 [  135733/  200000], loss: {'classification': 33.0468, 'neural_augmentation': 0.5367, 'total_loss': 33.5836}, LR: [0.00029, 0.00029], Avg. batch load time: 0.009, Elapsed time: 2334.15
2024-07-26 02:50:24 - [34m[1mLOGS   [0m - Epoch:  17 [  135795/  200000], loss: {'classification': 33.039, 'neural_augmentation': 0.5369, 'total_loss': 33.5759}, LR: [0.00029, 0.00029], Avg. batch load time: 0.008, Elapsed time: 2459.82
2024-07-26 02:52:30 - [34m[1mLOGS   [0m - Epoch:  17 [  135858/  200000], loss: {'classification': 33.0408, 'neural_augmentation': 0.537, 'total_loss': 33.5779}, LR: [0.000289, 0.000289], Avg. batch load time: 0.008, Elapsed time: 2585.34
2024-07-26 02:54:35 - [34m[1mLOGS   [0m - Epoch:  17 [  135920/  200000], loss: {'classification': 33.0403, 'neural_augmentation': 0.5372, 'total_loss': 33.5775}, LR: [0.000289, 0.000289], Avg. batch load time: 0.008, Elapsed time: 2710.80
2024-07-26 02:56:41 - [34m[1mLOGS   [0m - Epoch:  17 [  135983/  200000], loss: {'classification': 33.0372, 'neural_augmentation': 0.5373, 'total_loss': 33.5745}, LR: [0.000288, 0.000288], Avg. batch load time: 0.007, Elapsed time: 2836.08
2024-07-26 02:58:46 - [34m[1mLOGS   [0m - Epoch:  17 [  136045/  200000], loss: {'classification': 33.0366, 'neural_augmentation': 0.5374, 'total_loss': 33.574}, LR: [0.000288, 0.000288], Avg. batch load time: 0.007, Elapsed time: 2961.51
2024-07-26 03:00:52 - [34m[1mLOGS   [0m - Epoch:  17 [  136108/  200000], loss: {'classification': 33.0357, 'neural_augmentation': 0.5376, 'total_loss': 33.5733}, LR: [0.000287, 0.000287], Avg. batch load time: 0.007, Elapsed time: 3087.22
2024-07-26 03:03:05 - [34m[1mLOGS   [0m - Epoch:  17 [  136170/  200000], loss: {'classification': 33.0352, 'neural_augmentation': 0.5377, 'total_loss': 33.5729}, LR: [0.000287, 0.000287], Avg. batch load time: 0.007, Elapsed time: 3220.45
2024-07-26 03:05:10 - [34m[1mLOGS   [0m - Epoch:  17 [  136233/  200000], loss: {'classification': 33.0347, 'neural_augmentation': 0.5378, 'total_loss': 33.5725}, LR: [0.000286, 0.000286], Avg. batch load time: 0.006, Elapsed time: 3345.80
2024-07-26 03:07:16 - [34m[1mLOGS   [0m - Epoch:  17 [  136295/  200000], loss: {'classification': 33.0345, 'neural_augmentation': 0.538, 'total_loss': 33.5725}, LR: [0.000286, 0.000286], Avg. batch load time: 0.006, Elapsed time: 3471.24
2024-07-26 03:09:21 - [34m[1mLOGS   [0m - Epoch:  17 [  136358/  200000], loss: {'classification': 33.0354, 'neural_augmentation': 0.5381, 'total_loss': 33.5735}, LR: [0.000285, 0.000285], Avg. batch load time: 0.006, Elapsed time: 3596.55
2024-07-26 03:11:27 - [34m[1mLOGS   [0m - Epoch:  17 [  136420/  200000], loss: {'classification': 33.0339, 'neural_augmentation': 0.5382, 'total_loss': 33.5721}, LR: [0.000285, 0.000285], Avg. batch load time: 0.006, Elapsed time: 3722.05
2024-07-26 03:13:32 - [34m[1mLOGS   [0m - Epoch:  17 [  136483/  200000], loss: {'classification': 33.0319, 'neural_augmentation': 0.5384, 'total_loss': 33.5702}, LR: [0.000284, 0.000284], Avg. batch load time: 0.006, Elapsed time: 3847.56
2024-07-26 03:15:38 - [34m[1mLOGS   [0m - Epoch:  17 [  136545/  200000], loss: {'classification': 33.0333, 'neural_augmentation': 0.5385, 'total_loss': 33.5718}, LR: [0.000284, 0.000284], Avg. batch load time: 0.005, Elapsed time: 3973.04
2024-07-26 03:17:43 - [34m[1mLOGS   [0m - Epoch:  17 [  136608/  200000], loss: {'classification': 33.0314, 'neural_augmentation': 0.5386, 'total_loss': 33.5701}, LR: [0.000283, 0.000283], Avg. batch load time: 0.005, Elapsed time: 4098.37
2024-07-26 03:19:49 - [34m[1mLOGS   [0m - Epoch:  17 [  136670/  200000], loss: {'classification': 33.0287, 'neural_augmentation': 0.5388, 'total_loss': 33.5674}, LR: [0.000283, 0.000283], Avg. batch load time: 0.005, Elapsed time: 4224.04
2024-07-26 03:21:54 - [34m[1mLOGS   [0m - Epoch:  17 [  136733/  200000], loss: {'classification': 33.0273, 'neural_augmentation': 0.5389, 'total_loss': 33.5662}, LR: [0.000282, 0.000282], Avg. batch load time: 0.005, Elapsed time: 4349.60
2024-07-26 03:24:00 - [34m[1mLOGS   [0m - Epoch:  17 [  136795/  200000], loss: {'classification': 33.023, 'neural_augmentation': 0.5391, 'total_loss': 33.562}, LR: [0.000282, 0.000282], Avg. batch load time: 0.005, Elapsed time: 4475.24
2024-07-26 03:26:05 - [34m[1mLOGS   [0m - Epoch:  17 [  136858/  200000], loss: {'classification': 33.0238, 'neural_augmentation': 0.5392, 'total_loss': 33.563}, LR: [0.000281, 0.000281], Avg. batch load time: 0.005, Elapsed time: 4600.67
2024-07-26 03:28:17 - [34m[1mLOGS   [0m - Epoch:  17 [  136920/  200000], loss: {'classification': 33.0237, 'neural_augmentation': 0.5394, 'total_loss': 33.5631}, LR: [0.000281, 0.000281], Avg. batch load time: 0.005, Elapsed time: 4732.39
2024-07-26 03:30:22 - [34m[1mLOGS   [0m - Epoch:  17 [  136983/  200000], loss: {'classification': 33.0232, 'neural_augmentation': 0.5395, 'total_loss': 33.5628}, LR: [0.00028, 0.00028], Avg. batch load time: 0.005, Elapsed time: 4857.57
2024-07-26 03:32:27 - [34m[1mLOGS   [0m - Epoch:  17 [  137045/  200000], loss: {'classification': 33.0207, 'neural_augmentation': 0.5396, 'total_loss': 33.5604}, LR: [0.00028, 0.00028], Avg. batch load time: 0.005, Elapsed time: 4982.80
2024-07-26 03:34:33 - [34m[1mLOGS   [0m - Epoch:  17 [  137108/  200000], loss: {'classification': 33.0228, 'neural_augmentation': 0.5398, 'total_loss': 33.5626}, LR: [0.000279, 0.000279], Avg. batch load time: 0.004, Elapsed time: 5108.55
2024-07-26 03:36:38 - [34m[1mLOGS   [0m - Epoch:  17 [  137170/  200000], loss: {'classification': 33.0232, 'neural_augmentation': 0.5399, 'total_loss': 33.5631}, LR: [0.000279, 0.000279], Avg. batch load time: 0.004, Elapsed time: 5233.90
2024-07-26 03:38:44 - [34m[1mLOGS   [0m - Epoch:  17 [  137233/  200000], loss: {'classification': 33.0257, 'neural_augmentation': 0.5401, 'total_loss': 33.5658}, LR: [0.000278, 0.000278], Avg. batch load time: 0.004, Elapsed time: 5359.38
2024-07-26 03:40:50 - [34m[1mLOGS   [0m - Epoch:  17 [  137295/  200000], loss: {'classification': 33.024, 'neural_augmentation': 0.5402, 'total_loss': 33.5642}, LR: [0.000278, 0.000278], Avg. batch load time: 0.004, Elapsed time: 5485.01
2024-07-26 03:42:55 - [34m[1mLOGS   [0m - Epoch:  17 [  137358/  200000], loss: {'classification': 33.0234, 'neural_augmentation': 0.5404, 'total_loss': 33.5638}, LR: [0.000278, 0.000278], Avg. batch load time: 0.004, Elapsed time: 5610.42
2024-07-26 03:45:01 - [34m[1mLOGS   [0m - Epoch:  17 [  137420/  200000], loss: {'classification': 33.0223, 'neural_augmentation': 0.5405, 'total_loss': 33.5628}, LR: [0.000277, 0.000277], Avg. batch load time: 0.004, Elapsed time: 5736.04
2024-07-26 03:47:06 - [34m[1mLOGS   [0m - Epoch:  17 [  137483/  200000], loss: {'classification': 33.0225, 'neural_augmentation': 0.5406, 'total_loss': 33.5632}, LR: [0.000277, 0.000277], Avg. batch load time: 0.004, Elapsed time: 5861.50
2024-07-26 03:49:12 - [34m[1mLOGS   [0m - Epoch:  17 [  137545/  200000], loss: {'classification': 33.0221, 'neural_augmentation': 0.5408, 'total_loss': 33.5629}, LR: [0.000276, 0.000276], Avg. batch load time: 0.004, Elapsed time: 5987.16
2024-07-26 03:51:18 - [34m[1mLOGS   [0m - Epoch:  17 [  137608/  200000], loss: {'classification': 33.0203, 'neural_augmentation': 0.5409, 'total_loss': 33.5612}, LR: [0.000276, 0.000276], Avg. batch load time: 0.004, Elapsed time: 6113.00
2024-07-26 03:53:26 - [34m[1mLOGS   [0m - Epoch:  17 [  137670/  200000], loss: {'classification': 33.0197, 'neural_augmentation': 0.5411, 'total_loss': 33.5608}, LR: [0.000275, 0.000275], Avg. batch load time: 0.004, Elapsed time: 6241.38
2024-07-26 03:55:34 - [34m[1mLOGS   [0m - Epoch:  17 [  137733/  200000], loss: {'classification': 33.0198, 'neural_augmentation': 0.5412, 'total_loss': 33.561}, LR: [0.000275, 0.000275], Avg. batch load time: 0.004, Elapsed time: 6369.72
2024-07-26 03:57:40 - [34m[1mLOGS   [0m - Epoch:  17 [  137795/  200000], loss: {'classification': 33.02, 'neural_augmentation': 0.5413, 'total_loss': 33.5613}, LR: [0.000274, 0.000274], Avg. batch load time: 0.004, Elapsed time: 6495.16
2024-07-26 03:59:45 - [34m[1mLOGS   [0m - Epoch:  17 [  137858/  200000], loss: {'classification': 33.0204, 'neural_augmentation': 0.5415, 'total_loss': 33.5618}, LR: [0.000274, 0.000274], Avg. batch load time: 0.004, Elapsed time: 6620.46
2024-07-26 04:01:50 - [34m[1mLOGS   [0m - Epoch:  17 [  137920/  200000], loss: {'classification': 33.0202, 'neural_augmentation': 0.5416, 'total_loss': 33.5618}, LR: [0.000273, 0.000273], Avg. batch load time: 0.004, Elapsed time: 6745.73
2024-07-26 04:03:56 - [34m[1mLOGS   [0m - Epoch:  17 [  137983/  200000], loss: {'classification': 33.0194, 'neural_augmentation': 0.5417, 'total_loss': 33.5612}, LR: [0.000273, 0.000273], Avg. batch load time: 0.004, Elapsed time: 6871.15
2024-07-26 04:06:02 - [34m[1mLOGS   [0m - Epoch:  17 [  138045/  200000], loss: {'classification': 33.0196, 'neural_augmentation': 0.5419, 'total_loss': 33.5615}, LR: [0.000272, 0.000272], Avg. batch load time: 0.003, Elapsed time: 6996.93
2024-07-26 04:08:07 - [34m[1mLOGS   [0m - Epoch:  17 [  138108/  200000], loss: {'classification': 33.0187, 'neural_augmentation': 0.542, 'total_loss': 33.5607}, LR: [0.000272, 0.000272], Avg. batch load time: 0.003, Elapsed time: 7122.17
2024-07-26 04:10:12 - [34m[1mLOGS   [0m - Epoch:  17 [  138170/  200000], loss: {'classification': 33.0178, 'neural_augmentation': 0.5421, 'total_loss': 33.56}, LR: [0.000271, 0.000271], Avg. batch load time: 0.003, Elapsed time: 7247.67
2024-07-26 04:12:17 - [34m[1mLOGS   [0m - Epoch:  17 [  138233/  200000], loss: {'classification': 33.0162, 'neural_augmentation': 0.5423, 'total_loss': 33.5585}, LR: [0.000271, 0.000271], Avg. batch load time: 0.003, Elapsed time: 7372.85
2024-07-26 04:14:23 - [34m[1mLOGS   [0m - Epoch:  17 [  138295/  200000], loss: {'classification': 33.0159, 'neural_augmentation': 0.5424, 'total_loss': 33.5583}, LR: [0.00027, 0.00027], Avg. batch load time: 0.003, Elapsed time: 7498.36
2024-07-26 04:16:29 - [34m[1mLOGS   [0m - Epoch:  17 [  138358/  200000], loss: {'classification': 33.0157, 'neural_augmentation': 0.5425, 'total_loss': 33.5582}, LR: [0.00027, 0.00027], Avg. batch load time: 0.003, Elapsed time: 7623.93
2024-07-26 04:18:34 - [34m[1mLOGS   [0m - Epoch:  17 [  138420/  200000], loss: {'classification': 33.0141, 'neural_augmentation': 0.5427, 'total_loss': 33.5567}, LR: [0.000269, 0.000269], Avg. batch load time: 0.003, Elapsed time: 7749.85
2024-07-26 04:20:48 - [34m[1mLOGS   [0m - Epoch:  17 [  138483/  200000], loss: {'classification': 33.0138, 'neural_augmentation': 0.5428, 'total_loss': 33.5566}, LR: [0.000269, 0.000269], Avg. batch load time: 0.003, Elapsed time: 7883.23
2024-07-26 04:22:53 - [34m[1mLOGS   [0m - Epoch:  17 [  138545/  200000], loss: {'classification': 33.0111, 'neural_augmentation': 0.543, 'total_loss': 33.554}, LR: [0.000268, 0.000268], Avg. batch load time: 0.003, Elapsed time: 8008.76
2024-07-26 04:24:59 - [34m[1mLOGS   [0m - Epoch:  17 [  138608/  200000], loss: {'classification': 33.0106, 'neural_augmentation': 0.5431, 'total_loss': 33.5537}, LR: [0.000268, 0.000268], Avg. batch load time: 0.003, Elapsed time: 8134.26
2024-07-26 04:27:04 - [34m[1mLOGS   [0m - Epoch:  17 [  138670/  200000], loss: {'classification': 33.0096, 'neural_augmentation': 0.5432, 'total_loss': 33.5529}, LR: [0.000268, 0.000268], Avg. batch load time: 0.003, Elapsed time: 8259.67
2024-07-26 04:29:10 - [34m[1mLOGS   [0m - Epoch:  17 [  138733/  200000], loss: {'classification': 33.0099, 'neural_augmentation': 0.5434, 'total_loss': 33.5533}, LR: [0.000267, 0.000267], Avg. batch load time: 0.003, Elapsed time: 8385.16
2024-07-26 04:31:15 - [34m[1mLOGS   [0m - Epoch:  17 [  138795/  200000], loss: {'classification': 33.0098, 'neural_augmentation': 0.5435, 'total_loss': 33.5533}, LR: [0.000267, 0.000267], Avg. batch load time: 0.003, Elapsed time: 8510.49
2024-07-26 04:33:21 - [34m[1mLOGS   [0m - Epoch:  17 [  138858/  200000], loss: {'classification': 33.009, 'neural_augmentation': 0.5437, 'total_loss': 33.5527}, LR: [0.000266, 0.000266], Avg. batch load time: 0.003, Elapsed time: 8636.11
2024-07-26 04:35:26 - [34m[1mLOGS   [0m - Epoch:  17 [  138920/  200000], loss: {'classification': 33.0072, 'neural_augmentation': 0.5438, 'total_loss': 33.551}, LR: [0.000266, 0.000266], Avg. batch load time: 0.003, Elapsed time: 8761.73
2024-07-26 04:37:32 - [34m[1mLOGS   [0m - Epoch:  17 [  138983/  200000], loss: {'classification': 33.0061, 'neural_augmentation': 0.5439, 'total_loss': 33.55}, LR: [0.000265, 0.000265], Avg. batch load time: 0.003, Elapsed time: 8887.16
2024-07-26 04:39:37 - [34m[1mLOGS   [0m - Epoch:  17 [  139045/  200000], loss: {'classification': 33.0065, 'neural_augmentation': 0.5441, 'total_loss': 33.5505}, LR: [0.000265, 0.000265], Avg. batch load time: 0.003, Elapsed time: 9012.67
2024-07-26 04:41:43 - [34m[1mLOGS   [0m - Epoch:  17 [  139108/  200000], loss: {'classification': 33.0058, 'neural_augmentation': 0.5442, 'total_loss': 33.55}, LR: [0.000264, 0.000264], Avg. batch load time: 0.003, Elapsed time: 9138.18
2024-07-26 04:43:48 - [34m[1mLOGS   [0m - Epoch:  17 [  139170/  200000], loss: {'classification': 33.0043, 'neural_augmentation': 0.5443, 'total_loss': 33.5487}, LR: [0.000264, 0.000264], Avg. batch load time: 0.003, Elapsed time: 9263.78
2024-07-26 04:46:00 - [34m[1mLOGS   [0m - Epoch:  17 [  139233/  200000], loss: {'classification': 33.005, 'neural_augmentation': 0.5445, 'total_loss': 33.5495}, LR: [0.000263, 0.000263], Avg. batch load time: 0.003, Elapsed time: 9395.83
2024-07-26 04:48:06 - [34m[1mLOGS   [0m - Epoch:  17 [  139295/  200000], loss: {'classification': 33.0042, 'neural_augmentation': 0.5446, 'total_loss': 33.5488}, LR: [0.000263, 0.000263], Avg. batch load time: 0.003, Elapsed time: 9520.99
2024-07-26 04:50:11 - [34m[1mLOGS   [0m - Epoch:  17 [  139358/  200000], loss: {'classification': 33.0044, 'neural_augmentation': 0.5447, 'total_loss': 33.5492}, LR: [0.000262, 0.000262], Avg. batch load time: 0.003, Elapsed time: 9646.64
2024-07-26 04:52:17 - [34m[1mLOGS   [0m - Epoch:  17 [  139420/  200000], loss: {'classification': 33.003, 'neural_augmentation': 0.5449, 'total_loss': 33.5479}, LR: [0.000262, 0.000262], Avg. batch load time: 0.003, Elapsed time: 9772.06
2024-07-26 04:54:22 - [34m[1mLOGS   [0m - Epoch:  17 [  139483/  200000], loss: {'classification': 33.0032, 'neural_augmentation': 0.545, 'total_loss': 33.5483}, LR: [0.000261, 0.000261], Avg. batch load time: 0.003, Elapsed time: 9897.54
2024-07-26 04:56:28 - [34m[1mLOGS   [0m - Epoch:  17 [  139545/  200000], loss: {'classification': 33.0029, 'neural_augmentation': 0.5452, 'total_loss': 33.548}, LR: [0.000261, 0.000261], Avg. batch load time: 0.003, Elapsed time: 10023.02
2024-07-26 04:58:33 - [34m[1mLOGS   [0m - Epoch:  17 [  139608/  200000], loss: {'classification': 33.0024, 'neural_augmentation': 0.5453, 'total_loss': 33.5477}, LR: [0.00026, 0.00026], Avg. batch load time: 0.003, Elapsed time: 10148.36
2024-07-26 05:00:38 - [34m[1mLOGS   [0m - Epoch:  17 [  139670/  200000], loss: {'classification': 33.0018, 'neural_augmentation': 0.5454, 'total_loss': 33.5472}, LR: [0.00026, 0.00026], Avg. batch load time: 0.003, Elapsed time: 10273.83
2024-07-26 05:02:44 - [34m[1mLOGS   [0m - Epoch:  17 [  139733/  200000], loss: {'classification': 33.0011, 'neural_augmentation': 0.5456, 'total_loss': 33.5467}, LR: [0.00026, 0.00026], Avg. batch load time: 0.003, Elapsed time: 10399.21
2024-07-26 05:04:49 - [34m[1mLOGS   [0m - Epoch:  17 [  139795/  200000], loss: {'classification': 33.0008, 'neural_augmentation': 0.5457, 'total_loss': 33.5465}, LR: [0.000259, 0.000259], Avg. batch load time: 0.003, Elapsed time: 10524.77
2024-07-26 05:06:55 - [34m[1mLOGS   [0m - Epoch:  17 [  139858/  200000], loss: {'classification': 33.0002, 'neural_augmentation': 0.5458, 'total_loss': 33.546}, LR: [0.000259, 0.000259], Avg. batch load time: 0.003, Elapsed time: 10650.22
2024-07-26 05:09:00 - [34m[1mLOGS   [0m - Epoch:  17 [  139920/  200000], loss: {'classification': 32.9984, 'neural_augmentation': 0.546, 'total_loss': 33.5444}, LR: [0.000258, 0.000258], Avg. batch load time: 0.003, Elapsed time: 10775.59
2024-07-26 05:11:11 - [34m[1mLOGS   [0m - Epoch:  17 [  139983/  200000], loss: {'classification': 32.9985, 'neural_augmentation': 0.5461, 'total_loss': 33.5446}, LR: [0.000258, 0.000258], Avg. batch load time: 0.003, Elapsed time: 10906.00
2024-07-26 05:13:17 - [34m[1mLOGS   [0m - Epoch:  17 [  140045/  200000], loss: {'classification': 32.9969, 'neural_augmentation': 0.5462, 'total_loss': 33.5431}, LR: [0.000257, 0.000257], Avg. batch load time: 0.003, Elapsed time: 11032.90
2024-07-26 05:15:23 - [34m[1mLOGS   [0m - Epoch:  17 [  140108/  200000], loss: {'classification': 32.9961, 'neural_augmentation': 0.5464, 'total_loss': 33.5424}, LR: [0.000257, 0.000257], Avg. batch load time: 0.003, Elapsed time: 11158.32
2024-07-26 05:17:28 - [34m[1mLOGS   [0m - Epoch:  17 [  140170/  200000], loss: {'classification': 32.9955, 'neural_augmentation': 0.5465, 'total_loss': 33.542}, LR: [0.000256, 0.000256], Avg. batch load time: 0.002, Elapsed time: 11283.81
2024-07-26 05:19:34 - [34m[1mLOGS   [0m - Epoch:  17 [  140233/  200000], loss: {'classification': 32.9952, 'neural_augmentation': 0.5467, 'total_loss': 33.5419}, LR: [0.000256, 0.000256], Avg. batch load time: 0.002, Elapsed time: 11409.37
2024-07-26 05:21:40 - [34m[1mLOGS   [0m - Epoch:  17 [  140295/  200000], loss: {'classification': 32.994, 'neural_augmentation': 0.5468, 'total_loss': 33.5407}, LR: [0.000255, 0.000255], Avg. batch load time: 0.002, Elapsed time: 11534.95
2024-07-26 05:23:45 - [34m[1mLOGS   [0m - Epoch:  17 [  140358/  200000], loss: {'classification': 32.9932, 'neural_augmentation': 0.5469, 'total_loss': 33.5402}, LR: [0.000255, 0.000255], Avg. batch load time: 0.002, Elapsed time: 11660.56
2024-07-26 05:25:51 - [34m[1mLOGS   [0m - Epoch:  17 [  140420/  200000], loss: {'classification': 32.9916, 'neural_augmentation': 0.5471, 'total_loss': 33.5387}, LR: [0.000254, 0.000254], Avg. batch load time: 0.002, Elapsed time: 11786.20
2024-07-26 05:27:56 - [34m[1mLOGS   [0m - Epoch:  17 [  140483/  200000], loss: {'classification': 32.9898, 'neural_augmentation': 0.5472, 'total_loss': 33.537}, LR: [0.000254, 0.000254], Avg. batch load time: 0.002, Elapsed time: 11911.78
2024-07-26 05:30:02 - [34m[1mLOGS   [0m - Epoch:  17 [  140545/  200000], loss: {'classification': 32.9875, 'neural_augmentation': 0.5473, 'total_loss': 33.5348}, LR: [0.000253, 0.000253], Avg. batch load time: 0.002, Elapsed time: 12037.23
2024-07-26 05:32:07 - [34m[1mLOGS   [0m - Epoch:  17 [  140608/  200000], loss: {'classification': 32.9867, 'neural_augmentation': 0.5475, 'total_loss': 33.5342}, LR: [0.000253, 0.000253], Avg. batch load time: 0.002, Elapsed time: 12162.80
2024-07-26 05:34:13 - [34m[1mLOGS   [0m - Epoch:  17 [  140670/  200000], loss: {'classification': 32.9857, 'neural_augmentation': 0.5476, 'total_loss': 33.5333}, LR: [0.000253, 0.000253], Avg. batch load time: 0.002, Elapsed time: 12288.41
2024-07-26 05:36:18 - [34m[1mLOGS   [0m - Epoch:  17 [  140733/  200000], loss: {'classification': 32.9842, 'neural_augmentation': 0.5477, 'total_loss': 33.5319}, LR: [0.000252, 0.000252], Avg. batch load time: 0.002, Elapsed time: 12413.67
2024-07-26 05:38:30 - [34m[1mLOGS   [0m - Epoch:  17 [  140795/  200000], loss: {'classification': 32.9832, 'neural_augmentation': 0.5479, 'total_loss': 33.531}, LR: [0.000252, 0.000252], Avg. batch load time: 0.002, Elapsed time: 12545.27
2024-07-26 05:40:35 - [34m[1mLOGS   [0m - Epoch:  17 [  140858/  200000], loss: {'classification': 32.9828, 'neural_augmentation': 0.548, 'total_loss': 33.5308}, LR: [0.000251, 0.000251], Avg. batch load time: 0.002, Elapsed time: 12670.48
2024-07-26 05:42:41 - [34m[1mLOGS   [0m - Epoch:  17 [  140920/  200000], loss: {'classification': 32.981, 'neural_augmentation': 0.5481, 'total_loss': 33.5291}, LR: [0.000251, 0.000251], Avg. batch load time: 0.002, Elapsed time: 12796.07
2024-07-26 05:44:46 - [34m[1mLOGS   [0m - Epoch:  17 [  140983/  200000], loss: {'classification': 32.9799, 'neural_augmentation': 0.5483, 'total_loss': 33.5282}, LR: [0.00025, 0.00025], Avg. batch load time: 0.002, Elapsed time: 12921.62
2024-07-26 05:46:52 - [34m[1mLOGS   [0m - Epoch:  17 [  141045/  200000], loss: {'classification': 32.9781, 'neural_augmentation': 0.5484, 'total_loss': 33.5265}, LR: [0.00025, 0.00025], Avg. batch load time: 0.002, Elapsed time: 13047.32
2024-07-26 05:48:57 - [34m[1mLOGS   [0m - Epoch:  17 [  141108/  200000], loss: {'classification': 32.9765, 'neural_augmentation': 0.5485, 'total_loss': 33.525}, LR: [0.000249, 0.000249], Avg. batch load time: 0.002, Elapsed time: 13172.74
2024-07-26 05:51:03 - [34m[1mLOGS   [0m - Epoch:  17 [  141170/  200000], loss: {'classification': 32.974, 'neural_augmentation': 0.5487, 'total_loss': 33.5227}, LR: [0.000249, 0.000249], Avg. batch load time: 0.002, Elapsed time: 13298.19
2024-07-26 05:53:08 - [34m[1mLOGS   [0m - Epoch:  17 [  141233/  200000], loss: {'classification': 32.9731, 'neural_augmentation': 0.5488, 'total_loss': 33.5219}, LR: [0.000248, 0.000248], Avg. batch load time: 0.002, Elapsed time: 13423.79
2024-07-26 05:55:14 - [34m[1mLOGS   [0m - Epoch:  17 [  141295/  200000], loss: {'classification': 32.9722, 'neural_augmentation': 0.549, 'total_loss': 33.5211}, LR: [0.000248, 0.000248], Avg. batch load time: 0.002, Elapsed time: 13549.17
2024-07-26 05:57:20 - [34m[1mLOGS   [0m - Epoch:  17 [  141358/  200000], loss: {'classification': 32.9706, 'neural_augmentation': 0.5491, 'total_loss': 33.5197}, LR: [0.000247, 0.000247], Avg. batch load time: 0.002, Elapsed time: 13675.04
2024-07-26 05:59:25 - [34m[1mLOGS   [0m - Epoch:  17 [  141420/  200000], loss: {'classification': 32.9689, 'neural_augmentation': 0.5492, 'total_loss': 33.5182}, LR: [0.000247, 0.000247], Avg. batch load time: 0.002, Elapsed time: 13800.42
2024-07-26 06:01:31 - [34m[1mLOGS   [0m - Epoch:  17 [  141483/  200000], loss: {'classification': 32.9683, 'neural_augmentation': 0.5494, 'total_loss': 33.5177}, LR: [0.000246, 0.000246], Avg. batch load time: 0.002, Elapsed time: 13926.01
2024-07-26 06:03:42 - [34m[1mLOGS   [0m - Epoch:  17 [  141545/  200000], loss: {'classification': 32.9674, 'neural_augmentation': 0.5495, 'total_loss': 33.5169}, LR: [0.000246, 0.000246], Avg. batch load time: 0.002, Elapsed time: 14057.74
2024-07-26 06:05:48 - [34m[1mLOGS   [0m - Epoch:  17 [  141608/  200000], loss: {'classification': 32.9662, 'neural_augmentation': 0.5496, 'total_loss': 33.5158}, LR: [0.000246, 0.000246], Avg. batch load time: 0.002, Elapsed time: 14183.44
2024-07-26 06:07:53 - [34m[1mLOGS   [0m - Epoch:  17 [  141670/  200000], loss: {'classification': 32.9646, 'neural_augmentation': 0.5498, 'total_loss': 33.5143}, LR: [0.000245, 0.000245], Avg. batch load time: 0.002, Elapsed time: 14308.78
2024-07-26 06:09:59 - [34m[1mLOGS   [0m - Epoch:  17 [  141733/  200000], loss: {'classification': 32.9634, 'neural_augmentation': 0.5499, 'total_loss': 33.5133}, LR: [0.000245, 0.000245], Avg. batch load time: 0.002, Elapsed time: 14434.04
2024-07-26 06:12:04 - [34m[1mLOGS   [0m - Epoch:  17 [  141795/  200000], loss: {'classification': 32.9625, 'neural_augmentation': 0.55, 'total_loss': 33.5125}, LR: [0.000244, 0.000244], Avg. batch load time: 0.002, Elapsed time: 14559.45
2024-07-26 06:14:09 - [34m[1mLOGS   [0m - Epoch:  17 [  141858/  200000], loss: {'classification': 32.9616, 'neural_augmentation': 0.5502, 'total_loss': 33.5117}, LR: [0.000244, 0.000244], Avg. batch load time: 0.002, Elapsed time: 14684.84
2024-07-26 06:16:15 - [34m[1mLOGS   [0m - Epoch:  17 [  141920/  200000], loss: {'classification': 32.9605, 'neural_augmentation': 0.5503, 'total_loss': 33.5108}, LR: [0.000243, 0.000243], Avg. batch load time: 0.002, Elapsed time: 14810.38
2024-07-26 06:18:20 - [34m[1mLOGS   [0m - Epoch:  17 [  141983/  200000], loss: {'classification': 32.9592, 'neural_augmentation': 0.5504, 'total_loss': 33.5096}, LR: [0.000243, 0.000243], Avg. batch load time: 0.002, Elapsed time: 14935.88
2024-07-26 06:20:26 - [34m[1mLOGS   [0m - Epoch:  17 [  142045/  200000], loss: {'classification': 32.9582, 'neural_augmentation': 0.5506, 'total_loss': 33.5088}, LR: [0.000242, 0.000242], Avg. batch load time: 0.002, Elapsed time: 15061.24
2024-07-26 06:22:31 - [34m[1mLOGS   [0m - Epoch:  17 [  142108/  200000], loss: {'classification': 32.9576, 'neural_augmentation': 0.5507, 'total_loss': 33.5083}, LR: [0.000242, 0.000242], Avg. batch load time: 0.002, Elapsed time: 15186.56
2024-07-26 06:24:37 - [34m[1mLOGS   [0m - Epoch:  17 [  142170/  200000], loss: {'classification': 32.9563, 'neural_augmentation': 0.5509, 'total_loss': 33.5072}, LR: [0.000241, 0.000241], Avg. batch load time: 0.002, Elapsed time: 15312.05
2024-07-26 06:26:42 - [34m[1mLOGS   [0m - Epoch:  17 [  142233/  200000], loss: {'classification': 32.9556, 'neural_augmentation': 0.551, 'total_loss': 33.5066}, LR: [0.000241, 0.000241], Avg. batch load time: 0.002, Elapsed time: 15437.50
2024-07-26 06:28:51 - [34m[1mLOGS   [0m - Epoch:  17 [  142295/  200000], loss: {'classification': 32.9549, 'neural_augmentation': 0.5511, 'total_loss': 33.506}, LR: [0.000241, 0.000241], Avg. batch load time: 0.002, Elapsed time: 15566.23
2024-07-26 06:30:59 - [34m[1mLOGS   [0m - Epoch:  17 [  142358/  200000], loss: {'classification': 32.9536, 'neural_augmentation': 0.5513, 'total_loss': 33.5048}, LR: [0.00024, 0.00024], Avg. batch load time: 0.002, Elapsed time: 15694.80
2024-07-26 06:33:05 - [34m[1mLOGS   [0m - Epoch:  17 [  142420/  200000], loss: {'classification': 32.9526, 'neural_augmentation': 0.5514, 'total_loss': 33.504}, LR: [0.00024, 0.00024], Avg. batch load time: 0.002, Elapsed time: 15820.00
2024-07-26 06:35:10 - [34m[1mLOGS   [0m - Epoch:  17 [  142483/  200000], loss: {'classification': 32.9508, 'neural_augmentation': 0.5515, 'total_loss': 33.5023}, LR: [0.000239, 0.000239], Avg. batch load time: 0.002, Elapsed time: 15945.70
2024-07-26 06:37:16 - [34m[1mLOGS   [0m - Epoch:  17 [  142545/  200000], loss: {'classification': 32.9496, 'neural_augmentation': 0.5517, 'total_loss': 33.5013}, LR: [0.000239, 0.000239], Avg. batch load time: 0.002, Elapsed time: 16071.60
2024-07-26 06:37:33 - [34m[1mLOGS   [0m - *** Training summary for epoch 17
	 loss={'classification': 32.9493, 'neural_augmentation': 0.5517, 'total_loss': 33.501}
2024-07-26 06:37:36 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_best.pt
2024-07-26 06:37:38 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-26 06:37:38 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-26 06:37:40 - [34m[1mLOGS   [0m - Training checkpoint for epoch 17/iteration 142554 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_17_iter_142554.pt
2024-07-26 06:37:40 - [34m[1mLOGS   [0m - Model state for epoch 17/iteration 142554 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_17_iter_142554.pt
[31m===========================================================================[0m
2024-07-26 06:37:42 - [32m[1mINFO   [0m - Training epoch 18
2024-07-26 06:38:53 - [34m[1mLOGS   [0m - Epoch:  18 [  142554/  200000], loss: {'classification': 29.5557, 'neural_augmentation': 0.5656, 'total_loss': 30.1213}, LR: [0.000239, 0.000239], Avg. batch load time: 65.984, Elapsed time: 70.97
2024-07-26 06:41:02 - [34m[1mLOGS   [0m - Epoch:  18 [  142616/  200000], loss: {'classification': 32.7744, 'neural_augmentation': 0.5683, 'total_loss': 33.3428}, LR: [0.000238, 0.000238], Avg. batch load time: 0.133, Elapsed time: 199.77
2024-07-26 06:43:08 - [34m[1mLOGS   [0m - Epoch:  18 [  142679/  200000], loss: {'classification': 32.7865, 'neural_augmentation': 0.5685, 'total_loss': 33.355}, LR: [0.000238, 0.000238], Avg. batch load time: 0.067, Elapsed time: 325.43
2024-07-26 06:45:13 - [34m[1mLOGS   [0m - Epoch:  18 [  142741/  200000], loss: {'classification': 32.7791, 'neural_augmentation': 0.5688, 'total_loss': 33.3479}, LR: [0.000237, 0.000237], Avg. batch load time: 0.045, Elapsed time: 450.71
2024-07-26 06:47:18 - [34m[1mLOGS   [0m - Epoch:  18 [  142804/  200000], loss: {'classification': 32.7748, 'neural_augmentation': 0.569, 'total_loss': 33.3437}, LR: [0.000237, 0.000237], Avg. batch load time: 0.034, Elapsed time: 576.10
2024-07-26 06:49:24 - [34m[1mLOGS   [0m - Epoch:  18 [  142866/  200000], loss: {'classification': 32.7934, 'neural_augmentation': 0.5691, 'total_loss': 33.3624}, LR: [0.000236, 0.000236], Avg. batch load time: 0.027, Elapsed time: 701.73
2024-07-26 06:51:29 - [34m[1mLOGS   [0m - Epoch:  18 [  142929/  200000], loss: {'classification': 32.7848, 'neural_augmentation': 0.5692, 'total_loss': 33.3541}, LR: [0.000236, 0.000236], Avg. batch load time: 0.023, Elapsed time: 827.25
2024-07-26 06:53:35 - [34m[1mLOGS   [0m - Epoch:  18 [  142991/  200000], loss: {'classification': 32.784, 'neural_augmentation': 0.5694, 'total_loss': 33.3534}, LR: [0.000235, 0.000235], Avg. batch load time: 0.020, Elapsed time: 952.54
2024-07-26 06:55:40 - [34m[1mLOGS   [0m - Epoch:  18 [  143054/  200000], loss: {'classification': 32.7745, 'neural_augmentation': 0.5695, 'total_loss': 33.3441}, LR: [0.000235, 0.000235], Avg. batch load time: 0.017, Elapsed time: 1078.04
2024-07-26 06:57:45 - [34m[1mLOGS   [0m - Epoch:  18 [  143116/  200000], loss: {'classification': 32.7715, 'neural_augmentation': 0.5697, 'total_loss': 33.3412}, LR: [0.000235, 0.000235], Avg. batch load time: 0.016, Elapsed time: 1203.21
2024-07-26 06:59:51 - [34m[1mLOGS   [0m - Epoch:  18 [  143179/  200000], loss: {'classification': 32.7646, 'neural_augmentation': 0.5698, 'total_loss': 33.3344}, LR: [0.000234, 0.000234], Avg. batch load time: 0.014, Elapsed time: 1328.63
2024-07-26 07:01:56 - [34m[1mLOGS   [0m - Epoch:  18 [  143241/  200000], loss: {'classification': 32.7638, 'neural_augmentation': 0.5699, 'total_loss': 33.3337}, LR: [0.000234, 0.000234], Avg. batch load time: 0.013, Elapsed time: 1453.92
2024-07-26 07:04:03 - [34m[1mLOGS   [0m - Epoch:  18 [  143304/  200000], loss: {'classification': 32.7612, 'neural_augmentation': 0.57, 'total_loss': 33.3313}, LR: [0.000233, 0.000233], Avg. batch load time: 0.012, Elapsed time: 1581.00
2024-07-26 07:06:15 - [34m[1mLOGS   [0m - Epoch:  18 [  143366/  200000], loss: {'classification': 32.759, 'neural_augmentation': 0.5702, 'total_loss': 33.3292}, LR: [0.000233, 0.000233], Avg. batch load time: 0.011, Elapsed time: 1712.58
2024-07-26 07:08:20 - [34m[1mLOGS   [0m - Epoch:  18 [  143429/  200000], loss: {'classification': 32.7596, 'neural_augmentation': 0.5703, 'total_loss': 33.3299}, LR: [0.000232, 0.000232], Avg. batch load time: 0.010, Elapsed time: 1837.91
2024-07-26 07:10:25 - [34m[1mLOGS   [0m - Epoch:  18 [  143491/  200000], loss: {'classification': 32.7631, 'neural_augmentation': 0.5704, 'total_loss': 33.3335}, LR: [0.000232, 0.000232], Avg. batch load time: 0.010, Elapsed time: 1963.24
2024-07-26 07:12:31 - [34m[1mLOGS   [0m - Epoch:  18 [  143554/  200000], loss: {'classification': 32.7562, 'neural_augmentation': 0.5705, 'total_loss': 33.3267}, LR: [0.000231, 0.000231], Avg. batch load time: 0.009, Elapsed time: 2088.65
2024-07-26 07:14:36 - [34m[1mLOGS   [0m - Epoch:  18 [  143616/  200000], loss: {'classification': 32.7563, 'neural_augmentation': 0.5706, 'total_loss': 33.3269}, LR: [0.000231, 0.000231], Avg. batch load time: 0.009, Elapsed time: 2214.20
2024-07-26 07:16:42 - [34m[1mLOGS   [0m - Epoch:  18 [  143679/  200000], loss: {'classification': 32.7527, 'neural_augmentation': 0.5708, 'total_loss': 33.3235}, LR: [0.000231, 0.000231], Avg. batch load time: 0.008, Elapsed time: 2339.76
2024-07-26 07:18:48 - [34m[1mLOGS   [0m - Epoch:  18 [  143741/  200000], loss: {'classification': 32.7578, 'neural_augmentation': 0.5709, 'total_loss': 33.3287}, LR: [0.00023, 0.00023], Avg. batch load time: 0.008, Elapsed time: 2465.38
2024-07-26 07:20:53 - [34m[1mLOGS   [0m - Epoch:  18 [  143804/  200000], loss: {'classification': 32.7533, 'neural_augmentation': 0.571, 'total_loss': 33.3243}, LR: [0.00023, 0.00023], Avg. batch load time: 0.007, Elapsed time: 2590.96
2024-07-26 07:22:59 - [34m[1mLOGS   [0m - Epoch:  18 [  143866/  200000], loss: {'classification': 32.7563, 'neural_augmentation': 0.5711, 'total_loss': 33.3274}, LR: [0.000229, 0.000229], Avg. batch load time: 0.007, Elapsed time: 2716.50
2024-07-26 07:25:04 - [34m[1mLOGS   [0m - Epoch:  18 [  143929/  200000], loss: {'classification': 32.7574, 'neural_augmentation': 0.5712, 'total_loss': 33.3286}, LR: [0.000229, 0.000229], Avg. batch load time: 0.007, Elapsed time: 2842.05
2024-07-26 07:27:10 - [34m[1mLOGS   [0m - Epoch:  18 [  143991/  200000], loss: {'classification': 32.7555, 'neural_augmentation': 0.5714, 'total_loss': 33.3269}, LR: [0.000228, 0.000228], Avg. batch load time: 0.007, Elapsed time: 2967.49
2024-07-26 07:29:17 - [34m[1mLOGS   [0m - Epoch:  18 [  144054/  200000], loss: {'classification': 32.7553, 'neural_augmentation': 0.5715, 'total_loss': 33.3268}, LR: [0.000228, 0.000228], Avg. batch load time: 0.006, Elapsed time: 3094.43
2024-07-26 07:31:27 - [34m[1mLOGS   [0m - Epoch:  18 [  144116/  200000], loss: {'classification': 32.7545, 'neural_augmentation': 0.5716, 'total_loss': 33.3261}, LR: [0.000227, 0.000227], Avg. batch load time: 0.006, Elapsed time: 3224.40
2024-07-26 07:33:32 - [34m[1mLOGS   [0m - Epoch:  18 [  144179/  200000], loss: {'classification': 32.7534, 'neural_augmentation': 0.5718, 'total_loss': 33.3251}, LR: [0.000227, 0.000227], Avg. batch load time: 0.006, Elapsed time: 3349.97
2024-07-26 07:35:38 - [34m[1mLOGS   [0m - Epoch:  18 [  144241/  200000], loss: {'classification': 32.7526, 'neural_augmentation': 0.5719, 'total_loss': 33.3245}, LR: [0.000226, 0.000226], Avg. batch load time: 0.006, Elapsed time: 3475.47
2024-07-26 07:37:43 - [34m[1mLOGS   [0m - Epoch:  18 [  144304/  200000], loss: {'classification': 32.7526, 'neural_augmentation': 0.572, 'total_loss': 33.3246}, LR: [0.000226, 0.000226], Avg. batch load time: 0.006, Elapsed time: 3601.00
2024-07-26 07:39:48 - [34m[1mLOGS   [0m - Epoch:  18 [  144366/  200000], loss: {'classification': 32.7489, 'neural_augmentation': 0.5722, 'total_loss': 33.3211}, LR: [0.000226, 0.000226], Avg. batch load time: 0.005, Elapsed time: 3726.25
2024-07-26 07:41:54 - [34m[1mLOGS   [0m - Epoch:  18 [  144429/  200000], loss: {'classification': 32.7449, 'neural_augmentation': 0.5723, 'total_loss': 33.3172}, LR: [0.000225, 0.000225], Avg. batch load time: 0.005, Elapsed time: 3851.58
2024-07-26 07:43:59 - [34m[1mLOGS   [0m - Epoch:  18 [  144491/  200000], loss: {'classification': 32.7456, 'neural_augmentation': 0.5724, 'total_loss': 33.318}, LR: [0.000225, 0.000225], Avg. batch load time: 0.005, Elapsed time: 3977.04
2024-07-26 07:46:05 - [34m[1mLOGS   [0m - Epoch:  18 [  144554/  200000], loss: {'classification': 32.7452, 'neural_augmentation': 0.5726, 'total_loss': 33.3178}, LR: [0.000224, 0.000224], Avg. batch load time: 0.005, Elapsed time: 4102.56
2024-07-26 07:48:10 - [34m[1mLOGS   [0m - Epoch:  18 [  144616/  200000], loss: {'classification': 32.7427, 'neural_augmentation': 0.5727, 'total_loss': 33.3154}, LR: [0.000224, 0.000224], Avg. batch load time: 0.005, Elapsed time: 4228.30
2024-07-26 07:50:16 - [34m[1mLOGS   [0m - Epoch:  18 [  144679/  200000], loss: {'classification': 32.7442, 'neural_augmentation': 0.5728, 'total_loss': 33.317}, LR: [0.000223, 0.000223], Avg. batch load time: 0.005, Elapsed time: 4353.65
2024-07-26 07:52:21 - [34m[1mLOGS   [0m - Epoch:  18 [  144741/  200000], loss: {'classification': 32.7441, 'neural_augmentation': 0.573, 'total_loss': 33.3171}, LR: [0.000223, 0.000223], Avg. batch load time: 0.005, Elapsed time: 4479.09
2024-07-26 07:54:28 - [34m[1mLOGS   [0m - Epoch:  18 [  144804/  200000], loss: {'classification': 32.7427, 'neural_augmentation': 0.5731, 'total_loss': 33.3158}, LR: [0.000222, 0.000222], Avg. batch load time: 0.005, Elapsed time: 4606.11
2024-07-26 07:56:40 - [34m[1mLOGS   [0m - Epoch:  18 [  144866/  200000], loss: {'classification': 32.7399, 'neural_augmentation': 0.5732, 'total_loss': 33.3132}, LR: [0.000222, 0.000222], Avg. batch load time: 0.004, Elapsed time: 4737.53
2024-07-26 07:58:45 - [34m[1mLOGS   [0m - Epoch:  18 [  144929/  200000], loss: {'classification': 32.7392, 'neural_augmentation': 0.5733, 'total_loss': 33.3126}, LR: [0.000222, 0.000222], Avg. batch load time: 0.004, Elapsed time: 4862.72
2024-07-26 08:00:50 - [34m[1mLOGS   [0m - Epoch:  18 [  144991/  200000], loss: {'classification': 32.7363, 'neural_augmentation': 0.5735, 'total_loss': 33.3097}, LR: [0.000221, 0.000221], Avg. batch load time: 0.004, Elapsed time: 4987.89
2024-07-26 08:02:56 - [34m[1mLOGS   [0m - Epoch:  18 [  145054/  200000], loss: {'classification': 32.7352, 'neural_augmentation': 0.5736, 'total_loss': 33.3087}, LR: [0.000221, 0.000221], Avg. batch load time: 0.004, Elapsed time: 5114.18
2024-07-26 08:05:02 - [34m[1mLOGS   [0m - Epoch:  18 [  145116/  200000], loss: {'classification': 32.7329, 'neural_augmentation': 0.5737, 'total_loss': 33.3066}, LR: [0.00022, 0.00022], Avg. batch load time: 0.004, Elapsed time: 5239.52
2024-07-26 08:07:07 - [34m[1mLOGS   [0m - Epoch:  18 [  145179/  200000], loss: {'classification': 32.7307, 'neural_augmentation': 0.5738, 'total_loss': 33.3045}, LR: [0.00022, 0.00022], Avg. batch load time: 0.004, Elapsed time: 5364.98
2024-07-26 08:09:13 - [34m[1mLOGS   [0m - Epoch:  18 [  145241/  200000], loss: {'classification': 32.7304, 'neural_augmentation': 0.574, 'total_loss': 33.3043}, LR: [0.000219, 0.000219], Avg. batch load time: 0.004, Elapsed time: 5490.47
2024-07-26 08:11:18 - [34m[1mLOGS   [0m - Epoch:  18 [  145304/  200000], loss: {'classification': 32.7287, 'neural_augmentation': 0.5741, 'total_loss': 33.3028}, LR: [0.000219, 0.000219], Avg. batch load time: 0.004, Elapsed time: 5615.82
2024-07-26 08:13:23 - [34m[1mLOGS   [0m - Epoch:  18 [  145366/  200000], loss: {'classification': 32.7252, 'neural_augmentation': 0.5742, 'total_loss': 33.2994}, LR: [0.000218, 0.000218], Avg. batch load time: 0.004, Elapsed time: 5740.98
2024-07-26 08:15:28 - [34m[1mLOGS   [0m - Epoch:  18 [  145429/  200000], loss: {'classification': 32.7244, 'neural_augmentation': 0.5744, 'total_loss': 33.2988}, LR: [0.000218, 0.000218], Avg. batch load time: 0.004, Elapsed time: 5866.29
2024-07-26 08:17:34 - [34m[1mLOGS   [0m - Epoch:  18 [  145491/  200000], loss: {'classification': 32.7228, 'neural_augmentation': 0.5745, 'total_loss': 33.2973}, LR: [0.000218, 0.000218], Avg. batch load time: 0.004, Elapsed time: 5991.63
2024-07-26 08:19:39 - [34m[1mLOGS   [0m - Epoch:  18 [  145554/  200000], loss: {'classification': 32.722, 'neural_augmentation': 0.5746, 'total_loss': 33.2967}, LR: [0.000217, 0.000217], Avg. batch load time: 0.004, Elapsed time: 6117.05
2024-07-26 08:21:46 - [34m[1mLOGS   [0m - Epoch:  18 [  145616/  200000], loss: {'classification': 32.7203, 'neural_augmentation': 0.5747, 'total_loss': 33.2951}, LR: [0.000217, 0.000217], Avg. batch load time: 0.004, Elapsed time: 6243.71
2024-07-26 08:23:59 - [34m[1mLOGS   [0m - Epoch:  18 [  145679/  200000], loss: {'classification': 32.7188, 'neural_augmentation': 0.5748, 'total_loss': 33.2937}, LR: [0.000216, 0.000216], Avg. batch load time: 0.004, Elapsed time: 6376.76
2024-07-26 08:26:04 - [34m[1mLOGS   [0m - Epoch:  18 [  145741/  200000], loss: {'classification': 32.7171, 'neural_augmentation': 0.575, 'total_loss': 33.292}, LR: [0.000216, 0.000216], Avg. batch load time: 0.003, Elapsed time: 6502.17
2024-07-26 08:28:10 - [34m[1mLOGS   [0m - Epoch:  18 [  145804/  200000], loss: {'classification': 32.7189, 'neural_augmentation': 0.5751, 'total_loss': 33.294}, LR: [0.000215, 0.000215], Avg. batch load time: 0.003, Elapsed time: 6627.70
2024-07-26 08:30:16 - [34m[1mLOGS   [0m - Epoch:  18 [  145866/  200000], loss: {'classification': 32.7164, 'neural_augmentation': 0.5752, 'total_loss': 33.2916}, LR: [0.000215, 0.000215], Avg. batch load time: 0.003, Elapsed time: 6753.34
2024-07-26 08:32:21 - [34m[1mLOGS   [0m - Epoch:  18 [  145929/  200000], loss: {'classification': 32.7162, 'neural_augmentation': 0.5753, 'total_loss': 33.2915}, LR: [0.000215, 0.000215], Avg. batch load time: 0.003, Elapsed time: 6879.26
2024-07-26 08:34:27 - [34m[1mLOGS   [0m - Epoch:  18 [  145991/  200000], loss: {'classification': 32.7153, 'neural_augmentation': 0.5755, 'total_loss': 33.2907}, LR: [0.000214, 0.000214], Avg. batch load time: 0.003, Elapsed time: 7004.58
2024-07-26 08:36:32 - [34m[1mLOGS   [0m - Epoch:  18 [  146054/  200000], loss: {'classification': 32.7135, 'neural_augmentation': 0.5756, 'total_loss': 33.2891}, LR: [0.000214, 0.000214], Avg. batch load time: 0.003, Elapsed time: 7130.28
2024-07-26 08:38:38 - [34m[1mLOGS   [0m - Epoch:  18 [  146116/  200000], loss: {'classification': 32.7122, 'neural_augmentation': 0.5757, 'total_loss': 33.2879}, LR: [0.000213, 0.000213], Avg. batch load time: 0.003, Elapsed time: 7255.76
2024-07-26 08:40:43 - [34m[1mLOGS   [0m - Epoch:  18 [  146179/  200000], loss: {'classification': 32.7115, 'neural_augmentation': 0.5758, 'total_loss': 33.2873}, LR: [0.000213, 0.000213], Avg. batch load time: 0.003, Elapsed time: 7381.17
2024-07-26 08:42:49 - [34m[1mLOGS   [0m - Epoch:  18 [  146241/  200000], loss: {'classification': 32.7104, 'neural_augmentation': 0.576, 'total_loss': 33.2863}, LR: [0.000212, 0.000212], Avg. batch load time: 0.003, Elapsed time: 7506.38
2024-07-26 08:44:54 - [34m[1mLOGS   [0m - Epoch:  18 [  146304/  200000], loss: {'classification': 32.7102, 'neural_augmentation': 0.5761, 'total_loss': 33.2862}, LR: [0.000212, 0.000212], Avg. batch load time: 0.003, Elapsed time: 7631.87
2024-07-26 08:47:01 - [34m[1mLOGS   [0m - Epoch:  18 [  146366/  200000], loss: {'classification': 32.7082, 'neural_augmentation': 0.5762, 'total_loss': 33.2844}, LR: [0.000211, 0.000211], Avg. batch load time: 0.003, Elapsed time: 7758.89
2024-07-26 08:49:14 - [34m[1mLOGS   [0m - Epoch:  18 [  146429/  200000], loss: {'classification': 32.7063, 'neural_augmentation': 0.5763, 'total_loss': 33.2826}, LR: [0.000211, 0.000211], Avg. batch load time: 0.003, Elapsed time: 7892.02
2024-07-26 08:51:20 - [34m[1mLOGS   [0m - Epoch:  18 [  146491/  200000], loss: {'classification': 32.705, 'neural_augmentation': 0.5765, 'total_loss': 33.2815}, LR: [0.000211, 0.000211], Avg. batch load time: 0.003, Elapsed time: 8017.37
2024-07-26 08:53:25 - [34m[1mLOGS   [0m - Epoch:  18 [  146554/  200000], loss: {'classification': 32.7025, 'neural_augmentation': 0.5766, 'total_loss': 33.2791}, LR: [0.00021, 0.00021], Avg. batch load time: 0.003, Elapsed time: 8142.84
2024-07-26 08:55:31 - [34m[1mLOGS   [0m - Epoch:  18 [  146616/  200000], loss: {'classification': 32.7017, 'neural_augmentation': 0.5767, 'total_loss': 33.2784}, LR: [0.00021, 0.00021], Avg. batch load time: 0.003, Elapsed time: 8268.35
2024-07-26 08:57:36 - [34m[1mLOGS   [0m - Epoch:  18 [  146679/  200000], loss: {'classification': 32.7003, 'neural_augmentation': 0.5768, 'total_loss': 33.2772}, LR: [0.000209, 0.000209], Avg. batch load time: 0.003, Elapsed time: 8394.07
2024-07-26 08:59:42 - [34m[1mLOGS   [0m - Epoch:  18 [  146741/  200000], loss: {'classification': 32.6975, 'neural_augmentation': 0.577, 'total_loss': 33.2745}, LR: [0.000209, 0.000209], Avg. batch load time: 0.003, Elapsed time: 8519.82
2024-07-26 09:01:48 - [34m[1mLOGS   [0m - Epoch:  18 [  146804/  200000], loss: {'classification': 32.6955, 'neural_augmentation': 0.5771, 'total_loss': 33.2725}, LR: [0.000208, 0.000208], Avg. batch load time: 0.003, Elapsed time: 8645.53
2024-07-26 09:03:53 - [34m[1mLOGS   [0m - Epoch:  18 [  146866/  200000], loss: {'classification': 32.6956, 'neural_augmentation': 0.5772, 'total_loss': 33.2728}, LR: [0.000208, 0.000208], Avg. batch load time: 0.003, Elapsed time: 8771.12
2024-07-26 09:05:59 - [34m[1mLOGS   [0m - Epoch:  18 [  146929/  200000], loss: {'classification': 32.6935, 'neural_augmentation': 0.5773, 'total_loss': 33.2708}, LR: [0.000208, 0.000208], Avg. batch load time: 0.003, Elapsed time: 8896.54
2024-07-26 09:08:04 - [34m[1mLOGS   [0m - Epoch:  18 [  146991/  200000], loss: {'classification': 32.6931, 'neural_augmentation': 0.5775, 'total_loss': 33.2705}, LR: [0.000207, 0.000207], Avg. batch load time: 0.003, Elapsed time: 9022.18
2024-07-26 09:10:10 - [34m[1mLOGS   [0m - Epoch:  18 [  147054/  200000], loss: {'classification': 32.6926, 'neural_augmentation': 0.5776, 'total_loss': 33.2702}, LR: [0.000207, 0.000207], Avg. batch load time: 0.003, Elapsed time: 9147.70
2024-07-26 09:12:16 - [34m[1mLOGS   [0m - Epoch:  18 [  147116/  200000], loss: {'classification': 32.6919, 'neural_augmentation': 0.5777, 'total_loss': 33.2696}, LR: [0.000206, 0.000206], Avg. batch load time: 0.003, Elapsed time: 9273.36
2024-07-26 09:14:30 - [34m[1mLOGS   [0m - Epoch:  18 [  147179/  200000], loss: {'classification': 32.6906, 'neural_augmentation': 0.5778, 'total_loss': 33.2685}, LR: [0.000206, 0.000206], Avg. batch load time: 0.003, Elapsed time: 9407.73
2024-07-26 09:16:36 - [34m[1mLOGS   [0m - Epoch:  18 [  147241/  200000], loss: {'classification': 32.6905, 'neural_augmentation': 0.578, 'total_loss': 33.2685}, LR: [0.000205, 0.000205], Avg. batch load time: 0.003, Elapsed time: 9533.43
2024-07-26 09:18:41 - [34m[1mLOGS   [0m - Epoch:  18 [  147304/  200000], loss: {'classification': 32.6903, 'neural_augmentation': 0.5781, 'total_loss': 33.2684}, LR: [0.000205, 0.000205], Avg. batch load time: 0.003, Elapsed time: 9658.82
2024-07-26 09:20:46 - [34m[1mLOGS   [0m - Epoch:  18 [  147366/  200000], loss: {'classification': 32.6895, 'neural_augmentation': 0.5782, 'total_loss': 33.2677}, LR: [0.000205, 0.000205], Avg. batch load time: 0.003, Elapsed time: 9784.25
2024-07-26 09:22:52 - [34m[1mLOGS   [0m - Epoch:  18 [  147429/  200000], loss: {'classification': 32.6888, 'neural_augmentation': 0.5783, 'total_loss': 33.2671}, LR: [0.000204, 0.000204], Avg. batch load time: 0.003, Elapsed time: 9909.57
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
2024-07-26 09:24:30 - [34m[1mLOGS   [0m - Exception occurred that interrupted the training:
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 541, in run
    train_loss, train_ckpt_metric = self.train_epoch(epoch)  # 训练入口
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 315, in train_epoch
    loss_dict_or_tensor: Union[Dict, Tensor] = self.criteria(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/composite_loss.py", line 179, in forward
    loss_val = loss_layer(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 291, in forward
    loss_na = self._compute_loss(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 238, in _compute_loss
    loss_na = forward_loss_fn(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 211, in _forward_psnr
    target=target_mse.expand_as(pred_mse).to(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1469101) is killed by signal: Hangup. 

2024-07-26 09:24:30 - [34m[1mLOGS   [0m - Exception occurred that interrupted the training:
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 541, in run
    train_loss, train_ckpt_metric = self.train_epoch(epoch)  # 训练入口
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 315, in train_epoch
    loss_dict_or_tensor: Union[Dict, Tensor] = self.criteria(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/composite_loss.py", line 179, in forward
    loss_val = loss_layer(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 291, in forward
    loss_na = self._compute_loss(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 238, in _compute_loss
    loss_na = forward_loss_fn(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 211, in _forward_psnr
    target=target_mse.expand_as(pred_mse).to(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1495084) is killed by signal: Hangup. 

2024-07-26 09:24:30 - [34m[1mLOGS   [0m - Exception occurred that interrupted the training:
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 541, in run
    train_loss, train_ckpt_metric = self.train_epoch(epoch)  # 训练入口
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 315, in train_epoch
    loss_dict_or_tensor: Union[Dict, Tensor] = self.criteria(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/composite_loss.py", line 179, in forward
    loss_val = loss_layer(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 291, in forward
    loss_na = self._compute_loss(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 238, in _compute_loss
    loss_na = forward_loss_fn(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 211, in _forward_psnr
    target=target_mse.expand_as(pred_mse).to(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1489891) is killed by signal: Hangup. 

2024-07-26 09:24:31 - [34m[1mLOGS   [0m - Training took 16:15:38.75
2024-07-26 09:24:30 - [34m[1mLOGS   [0m - Exception occurred that interrupted the training:
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 541, in run
    train_loss, train_ckpt_metric = self.train_epoch(epoch)  # 训练入口
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 315, in train_epoch
    loss_dict_or_tensor: Union[Dict, Tensor] = self.criteria(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/composite_loss.py", line 179, in forward
    loss_val = loss_layer(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 291, in forward
    loss_na = self._compute_loss(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 238, in _compute_loss
    loss_na = forward_loss_fn(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 211, in _forward_psnr
    target=target_mse.expand_as(pred_mse).to(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1521552) is killed by signal: Hangup. 

2024-07-26 09:24:30 - [34m[1mLOGS   [0m - Exception occurred that interrupted the training:
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 541, in run
    train_loss, train_ckpt_metric = self.train_epoch(epoch)  # 训练入口
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 315, in train_epoch
    loss_dict_or_tensor: Union[Dict, Tensor] = self.criteria(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/composite_loss.py", line 179, in forward
    loss_val = loss_layer(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 291, in forward
    loss_na = self._compute_loss(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 238, in _compute_loss
    loss_na = forward_loss_fn(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 211, in _forward_psnr
    target=target_mse.expand_as(pred_mse).to(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1489388) is killed by signal: Hangup. 

2024-07-26 09:24:30 - [34m[1mLOGS   [0m - Exception occurred that interrupted the training:
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 541, in run
    train_loss, train_ckpt_metric = self.train_epoch(epoch)  # 训练入口
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 315, in train_epoch
    loss_dict_or_tensor: Union[Dict, Tensor] = self.criteria(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/composite_loss.py", line 179, in forward
    loss_val = loss_layer(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 291, in forward
    loss_na = self._compute_loss(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 238, in _compute_loss
    loss_na = forward_loss_fn(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 211, in _forward_psnr
    target=target_mse.expand_as(pred_mse).to(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1536564) is killed by signal: Hangup. 

2024-07-26 09:24:30 - [34m[1mLOGS   [0m - Exception occurred that interrupted the training:
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 541, in run
    train_loss, train_ckpt_metric = self.train_epoch(epoch)  # 训练入口
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 315, in train_epoch
    loss_dict_or_tensor: Union[Dict, Tensor] = self.criteria(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/composite_loss.py", line 179, in forward
    loss_val = loss_layer(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 291, in forward
    loss_na = self._compute_loss(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 238, in _compute_loss
    loss_na = forward_loss_fn(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 211, in _forward_psnr
    target=target_mse.expand_as(pred_mse).to(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1501157) is killed by signal: Hangup. 

Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/pretrain/../corenet/cli/main_train.py", line 42, in <module>
    main_worker()
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/pretrain/../corenet/cli/main_train.py", line 37, in main_worker
    launcher(callback)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/train_eval_pipelines/default_train_eval.py", line 312, in <lambda>
    return lambda callback: torch.multiprocessing.spawn(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 241, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 158, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 7 terminated with the following error:
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/spawn.py", line 68, in _wrap
    fn(i, *args)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/train_eval_pipelines/default_train_eval.py", line 433, in _launcher_distributed_spawn_fn
    callback(train_eval_pipeline)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/cli/main_train.py", line 28, in callback
    train_eval_pipeline.training_engine.run(train_sampler=train_sampler)  # 分两步，先init了training_engine,然后调用default_trainer.py里面的run
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 630, in run
    raise e
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 541, in run
    train_loss, train_ckpt_metric = self.train_epoch(epoch)  # 训练入口
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/engine/default_trainer.py", line 315, in train_epoch
    loss_dict_or_tensor: Union[Dict, Tensor] = self.criteria(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/composite_loss.py", line 179, in forward
    loss_val = loss_layer(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 291, in forward
    loss_na = self._compute_loss(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 238, in _compute_loss
    loss_na = forward_loss_fn(
  File "/ML-A800/home/guoshuyue/madehua/code/corenet/corenet/loss_fn/neural_augmentation.py", line 211, in _forward_psnr
    target=target_mse.expand_as(pred_mse).to(
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1469101) is killed by signal: Hangup. 

/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 200 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
