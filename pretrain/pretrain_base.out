nohup: ignoring input
2024-07-20 07:29:34 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
base
dci
2024-07-20 07:29:35 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-20 07:29:35 - [34m[1mLOGS   [0m - [36mModel[0m
ViTamin(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (128,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=1024, out_features=6743, bias=True, channel_first=False)
)
[31m=================================================================[0m
                            ViTamin Summary
[31m=================================================================[0m
Total parameters     =  109.299 M
Total trainable parameters =  109.299 M

2024-07-20 07:29:35 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-20 07:29:35 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 256, 256]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.109G                 | 17.507G    |
|  pos_embed                           |  (1, 1, 512)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  3.653M                |  7.21G     |
|   patch_embed.backbone.stem          |   0.151M               |   2.483G   |
|    patch_embed.backbone.stem.conv1   |    3.584K              |    56.623M |
|    patch_embed.backbone.stem.norm1   |    0.256K              |    10.486M |
|    patch_embed.backbone.stem.conv2   |    0.148M              |    2.416G  |
|   patch_embed.backbone.stages        |   2.321M               |   4.424G   |
|    patch_embed.backbone.stages.0     |    0.274M              |    1.93G   |
|    patch_embed.backbone.stages.1     |    2.047M              |    2.494G  |
|   patch_embed.backbone.pool          |   1.181M               |   0.303G   |
|    patch_embed.backbone.pool.proj    |    1.18M               |    0.302G  |
|    patch_embed.backbone.pool.norm    |    0.512K              |    1.311M  |
|  blocks                              |  18.404M               |  4.711G    |
|   blocks.0                           |   2.629M               |   0.673G   |
|    blocks.0.norm1                    |    1.024K              |    0.655M  |
|    blocks.0.attn                     |    1.051M              |    0.268G  |
|    blocks.0.norm2                    |    1.024K              |    0.655M  |
|    blocks.0.mlp                      |    1.576M              |    0.403G  |
|   blocks.1                           |   2.629M               |   0.673G   |
|    blocks.1.norm1                    |    1.024K              |    0.655M  |
|    blocks.1.attn                     |    1.051M              |    0.268G  |
|    blocks.1.norm2                    |    1.024K              |    0.655M  |
|    blocks.1.mlp                      |    1.576M              |    0.403G  |
|   blocks.2                           |   2.629M               |   0.673G   |
|    blocks.2.norm1                    |    1.024K              |    0.655M  |
|    blocks.2.attn                     |    1.051M              |    0.268G  |
|    blocks.2.norm2                    |    1.024K              |    0.655M  |
|    blocks.2.mlp                      |    1.576M              |    0.403G  |
|   blocks.3                           |   2.629M               |   0.673G   |
|    blocks.3.norm1                    |    1.024K              |    0.655M  |
|    blocks.3.attn                     |    1.051M              |    0.268G  |
|    blocks.3.norm2                    |    1.024K              |    0.655M  |
|    blocks.3.mlp                      |    1.576M              |    0.403G  |
|   blocks.4                           |   2.629M               |   0.673G   |
|    blocks.4.norm1                    |    1.024K              |    0.655M  |
|    blocks.4.attn                     |    1.051M              |    0.268G  |
|    blocks.4.norm2                    |    1.024K              |    0.655M  |
|    blocks.4.mlp                      |    1.576M              |    0.403G  |
|   blocks.5                           |   2.629M               |   0.673G   |
|    blocks.5.norm1                    |    1.024K              |    0.655M  |
|    blocks.5.attn                     |    1.051M              |    0.268G  |
|    blocks.5.norm2                    |    1.024K              |    0.655M  |
|    blocks.5.mlp                      |    1.576M              |    0.403G  |
|   blocks.6                           |   2.629M               |   0.673G   |
|    blocks.6.norm1                    |    1.024K              |    0.655M  |
|    blocks.6.attn                     |    1.051M              |    0.268G  |
|    blocks.6.norm2                    |    1.024K              |    0.655M  |
|    blocks.6.mlp                      |    1.576M              |    0.403G  |
|  pool                                |  4.721M                |  0.605G    |
|   pool.proj                          |   4.72M                |   0.604G   |
|    pool.proj.weight                  |    (1024, 512, 3, 3)   |            |
|    pool.proj.bias                    |    (1024,)             |            |
|   pool.norm                          |   1.024K               |   1.311M   |
|    pool.norm.weight                  |    (512,)              |            |
|    pool.norm.bias                    |    (512,)              |            |
|  blocks1                             |  73.508M               |  4.705G    |
|   blocks1.0                          |   10.501M              |   0.672G   |
|    blocks1.0.norm1                   |    2.048K              |    0.328M  |
|    blocks1.0.attn                    |    4.198M              |    0.268G  |
|    blocks1.0.norm2                   |    2.048K              |    0.328M  |
|    blocks1.0.mlp                     |    6.299M              |    0.403G  |
|   blocks1.1                          |   10.501M              |   0.672G   |
|    blocks1.1.norm1                   |    2.048K              |    0.328M  |
|    blocks1.1.attn                    |    4.198M              |    0.268G  |
|    blocks1.1.norm2                   |    2.048K              |    0.328M  |
|    blocks1.1.mlp                     |    6.299M              |    0.403G  |
|   blocks1.2                          |   10.501M              |   0.672G   |
|    blocks1.2.norm1                   |    2.048K              |    0.328M  |
|    blocks1.2.attn                    |    4.198M              |    0.268G  |
|    blocks1.2.norm2                   |    2.048K              |    0.328M  |
|    blocks1.2.mlp                     |    6.299M              |    0.403G  |
|   blocks1.3                          |   10.501M              |   0.672G   |
|    blocks1.3.norm1                   |    2.048K              |    0.328M  |
|    blocks1.3.attn                    |    4.198M              |    0.268G  |
|    blocks1.3.norm2                   |    2.048K              |    0.328M  |
|    blocks1.3.mlp                     |    6.299M              |    0.403G  |
|   blocks1.4                          |   10.501M              |   0.672G   |
|    blocks1.4.norm1                   |    2.048K              |    0.328M  |
|    blocks1.4.attn                    |    4.198M              |    0.268G  |
|    blocks1.4.norm2                   |    2.048K              |    0.328M  |
|    blocks1.4.mlp                     |    6.299M              |    0.403G  |
|   blocks1.5                          |   10.501M              |   0.672G   |
|    blocks1.5.norm1                   |    2.048K              |    0.328M  |
|    blocks1.5.attn                    |    4.198M              |    0.268G  |
|    blocks1.5.norm2                   |    2.048K              |    0.328M  |
|    blocks1.5.mlp                     |    6.299M              |    0.403G  |
|   blocks1.6                          |   10.501M              |   0.672G   |
|    blocks1.6.norm1                   |    2.048K              |    0.328M  |
|    blocks1.6.attn                    |    4.198M              |    0.268G  |
|    blocks1.6.norm2                   |    2.048K              |    0.328M  |
|    blocks1.6.mlp                     |    6.299M              |    0.403G  |
|  mlp                                 |  2.099M                |  0.268G    |
|   mlp.0                              |   1.05M                |   0.134G   |
|    mlp.0.weight                      |    (1024, 1024)        |            |
|    mlp.0.bias                        |    (1024,)             |            |
|   mlp.2                              |   1.05M                |   0.134G   |
|    mlp.2.weight                      |    (1024, 1024)        |            |
|    mlp.2.bias                        |    (1024,)             |            |
|  fc_norm                             |  2.048K                |  5.12K     |
|   fc_norm.weight                     |   (1024,)              |            |
|   fc_norm.bias                       |   (1024,)              |            |
|  classifier                          |  6.912M                |  6.905M    |
|   classifier.weight                  |   (6743, 1024)         |            |
|   classifier.bias                    |   (6743,)              |            |
2024-07-20 07:29:36 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-20 07:29:36 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks.6.attn.attn_drop', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks.3.attn.k_norm', 'patch_drop', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks1.6.drop_path2', 'blocks1.6.ls2', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'norm', 'neural_augmentor.noise.max_fn', 'blocks.4.ls2', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks1.0.attn.k_norm', 'blocks.6.drop_path1', 'blocks1.0.attn.attn_drop', 'blocks1.5.attn.k_norm', 'blocks.5.attn.attn_drop', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks1.1.drop_path1', 'blocks.1.ls1', 'blocks1.3.ls1', 'blocks.5.ls2', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks1.3.ls2', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks.0.drop_path1', 'blocks1.1.ls1', 'blocks1.4.drop_path1', 'blocks1.3.drop_path1', 'blocks.6.attn.q_norm', 'blocks.2.attn.k_norm', 'blocks.2.drop_path2', 'norm_pre', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks1.2.attn.k_norm', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks.0.attn.k_norm', 'blocks.0.attn.q_norm', 'blocks.3.attn.attn_drop', 'blocks1.3.attn.q_norm', 'blocks1.1.drop_path2', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks1.5.attn.q_norm', 'blocks1.2.drop_path1', 'blocks.3.ls1', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks.5.drop_path1', 'neural_augmentor.brightness', 'neural_augmentor.contrast', 'blocks.4.drop_path2', 'blocks.1.ls2', 'patch_embed.backbone.stages.0.1.down', 'patch_embed.backbone.stages.1.2.down', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks.1.drop_path2', 'blocks1.0.drop_path2', 'blocks.2.attn.attn_drop', 'blocks1.2.attn.attn_drop', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks.5.ls1', 'blocks.0.ls2', 'blocks1.1.attn.attn_drop', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks1.6.drop_path1', 'patch_embed.backbone.stages.1.3.down', 'blocks1.5.ls1', 'blocks1.4.ls2', 'blocks.4.ls1', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks1.0.ls2', 'blocks1.1.ls2', 'blocks1.6.attn.k_norm', 'blocks1.5.ls2', 'blocks1.4.attn.attn_drop', 'blocks1.3.attn.k_norm', 'patch_embed.backbone.stages.1.1.down', 'blocks.2.ls2', 'blocks.2.drop_path1', 'blocks.4.attn.attn_drop', 'blocks1.2.ls1', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks.4.drop_path1', 'blocks1.0.drop_path1', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'neural_augmentor.noise.min_fn', 'blocks.0.attn.attn_drop', 'blocks1.1.attn.q_norm', 'blocks1.2.drop_path2', 'blocks.5.drop_path2', 'blocks.2.ls1', 'blocks1.6.attn.q_norm', 'neural_augmentor.brightness.max_fn', 'blocks.6.drop_path2', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'neural_augmentor.contrast.min_fn', 'patch_embed.backbone.stages.1.0.down', 'blocks1.0.ls1', 'blocks1.6.ls1', 'blocks.0.drop_path2', 'blocks1.3.drop_path2', 'blocks.0.ls1', 'neural_augmentor.contrast.max_fn', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks1.4.ls1', 'neural_augmentor', 'blocks1.0.attn.q_norm', 'blocks.3.attn.q_norm', 'blocks1.4.attn.k_norm', 'blocks.6.attn.k_norm', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks.2.attn.q_norm', 'blocks1.4.drop_path2', 'patch_embed.backbone.stages.0.0.down', 'blocks1.5.drop_path2', 'blocks1.6.attn.attn_drop', 'blocks.1.attn.attn_drop', 'blocks1.2.attn.q_norm', 'neural_augmentor.brightness.min_fn', 'blocks1.3.attn.attn_drop', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks.3.drop_path2', 'patch_embed.proj', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks.3.ls2', 'blocks.4.attn.k_norm', 'blocks.5.attn.k_norm', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks.5.attn.q_norm', 'blocks1.5.drop_path1', 'blocks1.1.attn.k_norm', 'blocks1.5.attn.attn_drop', 'blocks.6.ls2', 'blocks1.4.attn.q_norm', 'patch_embed.backbone.stem.norm1.drop', 'blocks.4.attn.q_norm', 'blocks.1.attn.k_norm', 'blocks.3.drop_path1', 'blocks.6.ls1', 'neural_augmentor.noise', 'blocks1.2.ls2', 'blocks.1.attn.q_norm', 'blocks.1.drop_path1'}
2024-07-20 07:29:36 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::avg_pool2d': 2, 'aten::sum': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-20 07:29:36 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-20 07:29:36 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-20 07:29:36 - [34m[1mLOGS   [0m - Available GPUs: 4
2024-07-20 07:29:36 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-20 07:29:36 - [34m[1mLOGS   [0m - Directory exists at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train
2024-07-20 07:29:39 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:40000
base
dci
2024-07-20 07:29:39 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:40000
base
dci
2024-07-20 07:29:39 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:40000
2024-07-20 07:29:43 - [34m[1mLOGS   [0m - Training dataset details are given below
WordnetTaggedClassificationDataset(
	root= 
	is_training=True 
	num_samples=64290000
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	total_tar_files=6429
	max_files_per_tar=10000
	num_synsets=6743
)
2024-07-20 07:29:45 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=True
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=400
	 scales=[(128, 128, 1225), (144, 144, 967), (160, 160, 784), (176, 176, 647), (192, 192, 544), (208, 208, 463), (224, 224, 400), (240, 240, 348), (256, 256, 306), (272, 272, 271), (288, 288, 241), (304, 304, 217), (320, 320, 196)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-20 07:29:45 - [34m[1mLOGS   [0m - Number of data workers: 64
base
dci
2024-07-20 07:29:47 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-20 07:29:47 - [34m[1mLOGS   [0m - [36mModel[0m
ViTamin(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (128,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=1024, out_features=6743, bias=True, channel_first=False)
)
[31m=================================================================[0m
                            ViTamin Summary
[31m=================================================================[0m
Total parameters     =  109.299 M
Total trainable parameters =  109.299 M

2024-07-20 07:29:47 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-20 07:29:47 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 256, 256]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.109G                 | 17.507G    |
|  pos_embed                           |  (1, 1, 512)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  3.653M                |  7.21G     |
|   patch_embed.backbone.stem          |   0.151M               |   2.483G   |
|    patch_embed.backbone.stem.conv1   |    3.584K              |    56.623M |
|    patch_embed.backbone.stem.norm1   |    0.256K              |    10.486M |
|    patch_embed.backbone.stem.conv2   |    0.148M              |    2.416G  |
|   patch_embed.backbone.stages        |   2.321M               |   4.424G   |
|    patch_embed.backbone.stages.0     |    0.274M              |    1.93G   |
|    patch_embed.backbone.stages.1     |    2.047M              |    2.494G  |
|   patch_embed.backbone.pool          |   1.181M               |   0.303G   |
|    patch_embed.backbone.pool.proj    |    1.18M               |    0.302G  |
|    patch_embed.backbone.pool.norm    |    0.512K              |    1.311M  |
|  blocks                              |  18.404M               |  4.711G    |
|   blocks.0                           |   2.629M               |   0.673G   |
|    blocks.0.norm1                    |    1.024K              |    0.655M  |
|    blocks.0.attn                     |    1.051M              |    0.268G  |
|    blocks.0.norm2                    |    1.024K              |    0.655M  |
|    blocks.0.mlp                      |    1.576M              |    0.403G  |
|   blocks.1                           |   2.629M               |   0.673G   |
|    blocks.1.norm1                    |    1.024K              |    0.655M  |
|    blocks.1.attn                     |    1.051M              |    0.268G  |
|    blocks.1.norm2                    |    1.024K              |    0.655M  |
|    blocks.1.mlp                      |    1.576M              |    0.403G  |
|   blocks.2                           |   2.629M               |   0.673G   |
|    blocks.2.norm1                    |    1.024K              |    0.655M  |
|    blocks.2.attn                     |    1.051M              |    0.268G  |
|    blocks.2.norm2                    |    1.024K              |    0.655M  |
|    blocks.2.mlp                      |    1.576M              |    0.403G  |
|   blocks.3                           |   2.629M               |   0.673G   |
|    blocks.3.norm1                    |    1.024K              |    0.655M  |
|    blocks.3.attn                     |    1.051M              |    0.268G  |
|    blocks.3.norm2                    |    1.024K              |    0.655M  |
|    blocks.3.mlp                      |    1.576M              |    0.403G  |
|   blocks.4                           |   2.629M               |   0.673G   |
|    blocks.4.norm1                    |    1.024K              |    0.655M  |
|    blocks.4.attn                     |    1.051M              |    0.268G  |
|    blocks.4.norm2                    |    1.024K              |    0.655M  |
|    blocks.4.mlp                      |    1.576M              |    0.403G  |
|   blocks.5                           |   2.629M               |   0.673G   |
|    blocks.5.norm1                    |    1.024K              |    0.655M  |
|    blocks.5.attn                     |    1.051M              |    0.268G  |
|    blocks.5.norm2                    |    1.024K              |    0.655M  |
|    blocks.5.mlp                      |    1.576M              |    0.403G  |
|   blocks.6                           |   2.629M               |   0.673G   |
|    blocks.6.norm1                    |    1.024K              |    0.655M  |
|    blocks.6.attn                     |    1.051M              |    0.268G  |
|    blocks.6.norm2                    |    1.024K              |    0.655M  |
|    blocks.6.mlp                      |    1.576M              |    0.403G  |
|  pool                                |  4.721M                |  0.605G    |
|   pool.proj                          |   4.72M                |   0.604G   |
|    pool.proj.weight                  |    (1024, 512, 3, 3)   |            |
|    pool.proj.bias                    |    (1024,)             |            |
|   pool.norm                          |   1.024K               |   1.311M   |
|    pool.norm.weight                  |    (512,)              |            |
|    pool.norm.bias                    |    (512,)              |            |
|  blocks1                             |  73.508M               |  4.705G    |
|   blocks1.0                          |   10.501M              |   0.672G   |
|    blocks1.0.norm1                   |    2.048K              |    0.328M  |
|    blocks1.0.attn                    |    4.198M              |    0.268G  |
|    blocks1.0.norm2                   |    2.048K              |    0.328M  |
|    blocks1.0.mlp                     |    6.299M              |    0.403G  |
|   blocks1.1                          |   10.501M              |   0.672G   |
|    blocks1.1.norm1                   |    2.048K              |    0.328M  |
|    blocks1.1.attn                    |    4.198M              |    0.268G  |
|    blocks1.1.norm2                   |    2.048K              |    0.328M  |
|    blocks1.1.mlp                     |    6.299M              |    0.403G  |
|   blocks1.2                          |   10.501M              |   0.672G   |
|    blocks1.2.norm1                   |    2.048K              |    0.328M  |
|    blocks1.2.attn                    |    4.198M              |    0.268G  |
|    blocks1.2.norm2                   |    2.048K              |    0.328M  |
|    blocks1.2.mlp                     |    6.299M              |    0.403G  |
|   blocks1.3                          |   10.501M              |   0.672G   |
|    blocks1.3.norm1                   |    2.048K              |    0.328M  |
|    blocks1.3.attn                    |    4.198M              |    0.268G  |
|    blocks1.3.norm2                   |    2.048K              |    0.328M  |
|    blocks1.3.mlp                     |    6.299M              |    0.403G  |
|   blocks1.4                          |   10.501M              |   0.672G   |
|    blocks1.4.norm1                   |    2.048K              |    0.328M  |
|    blocks1.4.attn                    |    4.198M              |    0.268G  |
|    blocks1.4.norm2                   |    2.048K              |    0.328M  |
|    blocks1.4.mlp                     |    6.299M              |    0.403G  |
|   blocks1.5                          |   10.501M              |   0.672G   |
|    blocks1.5.norm1                   |    2.048K              |    0.328M  |
|    blocks1.5.attn                    |    4.198M              |    0.268G  |
|    blocks1.5.norm2                   |    2.048K              |    0.328M  |
|    blocks1.5.mlp                     |    6.299M              |    0.403G  |
|   blocks1.6                          |   10.501M              |   0.672G   |
|    blocks1.6.norm1                   |    2.048K              |    0.328M  |
|    blocks1.6.attn                    |    4.198M              |    0.268G  |
|    blocks1.6.norm2                   |    2.048K              |    0.328M  |
|    blocks1.6.mlp                     |    6.299M              |    0.403G  |
|  mlp                                 |  2.099M                |  0.268G    |
|   mlp.0                              |   1.05M                |   0.134G   |
|    mlp.0.weight                      |    (1024, 1024)        |            |
|    mlp.0.bias                        |    (1024,)             |            |
|   mlp.2                              |   1.05M                |   0.134G   |
|    mlp.2.weight                      |    (1024, 1024)        |            |
|    mlp.2.bias                        |    (1024,)             |            |
|  fc_norm                             |  2.048K                |  5.12K     |
|   fc_norm.weight                     |   (1024,)              |            |
|   fc_norm.bias                       |   (1024,)              |            |
|  classifier                          |  6.912M                |  6.905M    |
|   classifier.weight                  |   (6743, 1024)         |            |
|   classifier.bias                    |   (6743,)              |            |
2024-07-20 07:29:48 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-20 07:29:48 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks.4.drop_path2', 'blocks1.3.ls2', 'patch_embed.backbone.stages.1.1.shortcut', 'patch_embed.backbone.stages.1.3.drop_path', 'neural_augmentor.contrast.min_fn', 'blocks.0.attn.q_norm', 'blocks.1.drop_path2', 'blocks.4.drop_path1', 'blocks.2.drop_path1', 'blocks1.0.drop_path1', 'blocks.5.drop_path1', 'blocks.2.attn.k_norm', 'blocks1.3.ls1', 'blocks1.2.attn.attn_drop', 'blocks1.3.drop_path2', 'blocks.0.attn.attn_drop', 'blocks.3.attn.q_norm', 'blocks1.6.attn.attn_drop', 'patch_embed.backbone.stages.1.0.down', 'blocks.3.attn.k_norm', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.0.attn.k_norm', 'blocks.4.attn.q_norm', 'patch_drop', 'blocks1.1.attn.k_norm', 'blocks.6.drop_path1', 'neural_augmentor.contrast', 'norm_pre', 'blocks.2.attn.attn_drop', 'blocks.1.attn.q_norm', 'blocks.2.attn.q_norm', 'blocks1.5.ls1', 'blocks.1.attn.k_norm', 'blocks.5.attn.k_norm', 'blocks.0.drop_path1', 'blocks1.2.drop_path2', 'blocks1.1.ls1', 'blocks.4.ls2', 'blocks1.6.drop_path1', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks1.3.drop_path1', 'blocks1.4.attn.attn_drop', 'blocks.3.ls1', 'blocks.6.attn.attn_drop', 'blocks.0.ls1', 'blocks.3.drop_path2', 'blocks.0.attn.k_norm', 'patch_embed.backbone.stages.0.1.down', 'patch_embed.backbone.stages.1.3.down', 'blocks1.2.drop_path1', 'blocks1.6.drop_path2', 'blocks.6.drop_path2', 'blocks1.6.ls2', 'blocks.4.ls1', 'neural_augmentor.noise.max_fn', 'patch_embed.backbone.stages.0.1.drop_path', 'norm', 'blocks1.3.attn.k_norm', 'blocks1.3.attn.attn_drop', 'neural_augmentor.brightness.max_fn', 'blocks.6.attn.k_norm', 'blocks.5.drop_path2', 'blocks.2.drop_path2', 'blocks1.6.attn.q_norm', 'patch_embed.backbone.stages.1.1.down', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'neural_augmentor.brightness.min_fn', 'blocks.1.ls2', 'neural_augmentor.contrast.max_fn', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks1.0.ls2', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks1.4.drop_path2', 'blocks.1.attn.attn_drop', 'blocks1.0.attn.q_norm', 'blocks.2.ls1', 'patch_embed.backbone.stages.1.2.down', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks1.5.drop_path1', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks.5.ls1', 'blocks1.6.ls1', 'blocks1.5.ls2', 'blocks1.1.drop_path2', 'blocks.3.attn.attn_drop', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks.4.attn.attn_drop', 'patch_embed.backbone.stages.1.0.drop_path', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks.0.drop_path2', 'blocks1.4.attn.k_norm', 'blocks.5.attn.attn_drop', 'patch_embed.backbone.stages.0.0.down', 'blocks1.1.attn.q_norm', 'blocks1.4.ls2', 'neural_augmentor', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks.6.ls2', 'blocks1.5.attn.attn_drop', 'neural_augmentor.noise.min_fn', 'blocks.3.drop_path1', 'blocks.6.ls1', 'patch_embed.backbone.stem.norm1.drop', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks1.5.attn.q_norm', 'blocks1.5.drop_path2', 'blocks1.1.attn.attn_drop', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks1.4.drop_path1', 'blocks.2.ls2', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks1.3.attn.q_norm', 'blocks1.2.ls1', 'blocks1.4.attn.q_norm', 'blocks1.4.ls1', 'blocks1.6.attn.k_norm', 'patch_embed.proj', 'blocks.1.drop_path1', 'patch_embed.backbone.stages.1.2.drop_path', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks.1.ls1', 'blocks1.0.ls1', 'blocks1.2.attn.q_norm', 'blocks.0.ls2', 'blocks1.2.attn.k_norm', 'blocks1.0.drop_path2', 'blocks.5.attn.q_norm', 'neural_augmentor.brightness', 'blocks.4.attn.k_norm', 'blocks1.1.ls2', 'neural_augmentor.noise', 'blocks.3.ls2', 'blocks1.5.attn.k_norm', 'blocks1.1.drop_path1', 'blocks.5.ls2', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks.6.attn.q_norm', 'blocks1.0.attn.attn_drop', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks1.2.ls2'}
2024-07-20 07:29:48 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::avg_pool2d': 2, 'aten::sum': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-20 07:29:48 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-20 07:29:48 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-20 07:29:48 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-07-20 07:29:48 - [34m[1mLOGS   [0m - Max. iteration for training: 100000
2024-07-20 07:29:48 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=1e-05
 	 max_lr=0.001
 	 period=90001
 	 warmup_init_lr=1e-06
 	 warmup_iters=10000
 )
2024-07-20 07:29:48 - [34m[1mLOGS   [0m - No checkpoint found at '/ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt'
2024-07-20 07:29:48 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results_base_dci/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-20 07:29:50 - [32m[1mINFO   [0m - Training epoch 0
2024-07-20 07:29:39 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:40000
base
dci
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-20 07:33:43 - [34m[1mLOGS   [0m - Epoch:   0 [       0/  100000], loss: {'classification': 4905.1431, 'neural_augmentation': 8.9835, 'total_loss': 4914.1266}, LR: [1e-06, 1e-06], Avg. batch load time: 228.786, Elapsed time: 233.31
2024-07-20 07:43:19 - [34m[1mLOGS   [0m - Epoch:   0 [     125/  100000], loss: {'classification': 3459.4448, 'neural_augmentation': 9.2707, 'total_loss': 3468.7155}, LR: [1.3e-05, 1.3e-05], Avg. batch load time: 0.477, Elapsed time: 809.18
2024-07-20 07:52:37 - [34m[1mLOGS   [0m - Epoch:   0 [     250/  100000], loss: {'classification': 2131.2582, 'neural_augmentation': 9.3196, 'total_loss': 2140.5778}, LR: [2.6e-05, 2.6e-05], Avg. batch load time: 0.239, Elapsed time: 1367.35
2024-07-20 08:01:56 - [34m[1mLOGS   [0m - Epoch:   0 [     375/  100000], loss: {'classification': 1449.4813, 'neural_augmentation': 9.3406, 'total_loss': 1458.8219}, LR: [3.8e-05, 3.8e-05], Avg. batch load time: 0.160, Elapsed time: 1925.86
2024-07-20 08:11:14 - [34m[1mLOGS   [0m - Epoch:   0 [     500/  100000], loss: {'classification': 1093.6486, 'neural_augmentation': 9.341, 'total_loss': 1102.9896}, LR: [5.1e-05, 5.1e-05], Avg. batch load time: 0.120, Elapsed time: 2484.35
2024-07-20 08:20:33 - [34m[1mLOGS   [0m - Epoch:   0 [     625/  100000], loss: {'classification': 884.7817, 'neural_augmentation': 9.3357, 'total_loss': 894.1173}, LR: [6.3e-05, 6.3e-05], Avg. batch load time: 0.096, Elapsed time: 3042.66
2024-07-20 08:29:51 - [34m[1mLOGS   [0m - Epoch:   0 [     750/  100000], loss: {'classification': 742.7571, 'neural_augmentation': 9.3269, 'total_loss': 752.084}, LR: [7.6e-05, 7.6e-05], Avg. batch load time: 0.080, Elapsed time: 3600.88
2024-07-20 08:39:09 - [34m[1mLOGS   [0m - Epoch:   0 [     875/  100000], loss: {'classification': 640.5896, 'neural_augmentation': 9.3197, 'total_loss': 649.9093}, LR: [8.8e-05, 8.8e-05], Avg. batch load time: 0.069, Elapsed time: 4158.80
2024-07-20 08:48:29 - [34m[1mLOGS   [0m - Epoch:   0 [    1000/  100000], loss: {'classification': 564.949, 'neural_augmentation': 9.3072, 'total_loss': 574.2562}, LR: [0.000101, 0.000101], Avg. batch load time: 0.060, Elapsed time: 4719.32
2024-07-20 08:57:47 - [34m[1mLOGS   [0m - Epoch:   0 [    1125/  100000], loss: {'classification': 503.6869, 'neural_augmentation': 9.2942, 'total_loss': 512.9811}, LR: [0.000113, 0.000113], Avg. batch load time: 0.054, Elapsed time: 5276.93
2024-07-20 09:07:05 - [34m[1mLOGS   [0m - Epoch:   0 [    1250/  100000], loss: {'classification': 455.7244, 'neural_augmentation': 9.2874, 'total_loss': 465.0118}, LR: [0.000126, 0.000126], Avg. batch load time: 0.048, Elapsed time: 5835.02
2024-07-20 09:16:23 - [34m[1mLOGS   [0m - Epoch:   0 [    1375/  100000], loss: {'classification': 415.261, 'neural_augmentation': 9.2789, 'total_loss': 424.5399}, LR: [0.000138, 0.000138], Avg. batch load time: 0.044, Elapsed time: 6393.38
2024-07-20 09:25:42 - [34m[1mLOGS   [0m - Epoch:   0 [    1500/  100000], loss: {'classification': 382.3332, 'neural_augmentation': 9.2709, 'total_loss': 391.6041}, LR: [0.000151, 0.000151], Avg. batch load time: 0.041, Elapsed time: 6951.77
2024-07-20 09:35:00 - [34m[1mLOGS   [0m - Epoch:   0 [    1625/  100000], loss: {'classification': 354.0741, 'neural_augmentation': 9.2667, 'total_loss': 363.3409}, LR: [0.000163, 0.000163], Avg. batch load time: 0.037, Elapsed time: 7510.24
2024-07-20 09:44:19 - [34m[1mLOGS   [0m - Epoch:   0 [    1750/  100000], loss: {'classification': 330.275, 'neural_augmentation': 9.2542, 'total_loss': 339.5292}, LR: [0.000176, 0.000176], Avg. batch load time: 0.035, Elapsed time: 8068.72
2024-07-20 09:53:39 - [34m[1mLOGS   [0m - Epoch:   0 [    1875/  100000], loss: {'classification': 309.0347, 'neural_augmentation': 9.2401, 'total_loss': 318.2748}, LR: [0.000188, 0.000188], Avg. batch load time: 0.033, Elapsed time: 8629.13
2024-07-20 10:02:58 - [34m[1mLOGS   [0m - Epoch:   0 [    2000/  100000], loss: {'classification': 291.2323, 'neural_augmentation': 9.2292, 'total_loss': 300.4615}, LR: [0.000201, 0.000201], Avg. batch load time: 0.031, Elapsed time: 9188.24
2024-07-20 10:12:17 - [34m[1mLOGS   [0m - Epoch:   0 [    2125/  100000], loss: {'classification': 275.8051, 'neural_augmentation': 9.2144, 'total_loss': 285.0195}, LR: [0.000213, 0.000213], Avg. batch load time: 0.029, Elapsed time: 9746.75
2024-07-20 10:21:35 - [34m[1mLOGS   [0m - Epoch:   0 [    2250/  100000], loss: {'classification': 261.864, 'neural_augmentation': 9.2015, 'total_loss': 271.0655}, LR: [0.000226, 0.000226], Avg. batch load time: 0.027, Elapsed time: 10305.48
2024-07-20 10:30:54 - [34m[1mLOGS   [0m - Epoch:   0 [    2375/  100000], loss: {'classification': 248.9065, 'neural_augmentation': 9.1825, 'total_loss': 258.089}, LR: [0.000238, 0.000238], Avg. batch load time: 0.026, Elapsed time: 10864.20
2024-07-20 10:40:13 - [34m[1mLOGS   [0m - Epoch:   0 [    2500/  100000], loss: {'classification': 237.5817, 'neural_augmentation': 9.1619, 'total_loss': 246.7435}, LR: [0.000251, 0.000251], Avg. batch load time: 0.025, Elapsed time: 11422.82
2024-07-20 10:49:32 - [34m[1mLOGS   [0m - Epoch:   0 [    2625/  100000], loss: {'classification': 227.7254, 'neural_augmentation': 9.1511, 'total_loss': 236.8766}, LR: [0.000263, 0.000263], Avg. batch load time: 0.024, Elapsed time: 11981.82
2024-07-20 10:58:51 - [34m[1mLOGS   [0m - Epoch:   0 [    2750/  100000], loss: {'classification': 218.7203, 'neural_augmentation': 9.1372, 'total_loss': 227.8576}, LR: [0.000276, 0.000276], Avg. batch load time: 0.022, Elapsed time: 12541.07
2024-07-20 11:08:11 - [34m[1mLOGS   [0m - Epoch:   0 [    2875/  100000], loss: {'classification': 210.2331, 'neural_augmentation': 9.1225, 'total_loss': 219.3557}, LR: [0.000288, 0.000288], Avg. batch load time: 0.022, Elapsed time: 13101.22
2024-07-20 11:17:29 - [34m[1mLOGS   [0m - Epoch:   0 [    3000/  100000], loss: {'classification': 202.5594, 'neural_augmentation': 9.1079, 'total_loss': 211.6672}, LR: [0.000301, 0.000301], Avg. batch load time: 0.021, Elapsed time: 13659.26
2024-07-20 11:26:47 - [34m[1mLOGS   [0m - Epoch:   0 [    3125/  100000], loss: {'classification': 195.8576, 'neural_augmentation': 9.0938, 'total_loss': 204.9514}, LR: [0.000313, 0.000313], Avg. batch load time: 0.020, Elapsed time: 14216.84
2024-07-20 11:36:05 - [34m[1mLOGS   [0m - Epoch:   0 [    3250/  100000], loss: {'classification': 189.2355, 'neural_augmentation': 9.0755, 'total_loss': 198.311}, LR: [0.000326, 0.000326], Avg. batch load time: 0.019, Elapsed time: 14775.16
2024-07-20 11:45:23 - [34m[1mLOGS   [0m - Epoch:   0 [    3375/  100000], loss: {'classification': 183.1344, 'neural_augmentation': 9.055, 'total_loss': 192.1894}, LR: [0.000338, 0.000338], Avg. batch load time: 0.018, Elapsed time: 15333.38
2024-07-20 11:54:42 - [34m[1mLOGS   [0m - Epoch:   0 [    3500/  100000], loss: {'classification': 177.4936, 'neural_augmentation': 9.0348, 'total_loss': 186.5284}, LR: [0.000351, 0.000351], Avg. batch load time: 0.018, Elapsed time: 15891.73
2024-07-20 12:04:00 - [34m[1mLOGS   [0m - Epoch:   0 [    3625/  100000], loss: {'classification': 172.5008, 'neural_augmentation': 9.0091, 'total_loss': 181.5099}, LR: [0.000363, 0.000363], Avg. batch load time: 0.017, Elapsed time: 16450.15
2024-07-20 12:13:22 - [34m[1mLOGS   [0m - Epoch:   0 [    3750/  100000], loss: {'classification': 167.3991, 'neural_augmentation': 8.979, 'total_loss': 176.3781}, LR: [0.000376, 0.000376], Avg. batch load time: 0.017, Elapsed time: 17012.50
2024-07-20 12:22:41 - [34m[1mLOGS   [0m - Epoch:   0 [    3875/  100000], loss: {'classification': 162.8627, 'neural_augmentation': 8.9479, 'total_loss': 171.8106}, LR: [0.000388, 0.000388], Avg. batch load time: 0.016, Elapsed time: 17571.61
2024-07-20 12:32:00 - [34m[1mLOGS   [0m - Epoch:   0 [    4000/  100000], loss: {'classification': 158.3662, 'neural_augmentation': 8.9144, 'total_loss': 167.2806}, LR: [0.000401, 0.000401], Avg. batch load time: 0.016, Elapsed time: 18130.14
2024-07-20 12:41:19 - [34m[1mLOGS   [0m - Epoch:   0 [    4125/  100000], loss: {'classification': 154.078, 'neural_augmentation': 8.8793, 'total_loss': 162.9573}, LR: [0.000413, 0.000413], Avg. batch load time: 0.015, Elapsed time: 18688.70
2024-07-20 12:50:37 - [34m[1mLOGS   [0m - Epoch:   0 [    4250/  100000], loss: {'classification': 150.3013, 'neural_augmentation': 8.8452, 'total_loss': 159.1465}, LR: [0.000426, 0.000426], Avg. batch load time: 0.015, Elapsed time: 19247.25
2024-07-20 12:59:56 - [34m[1mLOGS   [0m - Epoch:   0 [    4375/  100000], loss: {'classification': 146.732, 'neural_augmentation': 8.8114, 'total_loss': 155.5434}, LR: [0.000438, 0.000438], Avg. batch load time: 0.014, Elapsed time: 19805.70
2024-07-20 13:09:14 - [34m[1mLOGS   [0m - Epoch:   0 [    4500/  100000], loss: {'classification': 143.4089, 'neural_augmentation': 8.7761, 'total_loss': 152.185}, LR: [0.000451, 0.000451], Avg. batch load time: 0.014, Elapsed time: 20364.50
2024-07-20 13:18:36 - [34m[1mLOGS   [0m - Epoch:   0 [    4625/  100000], loss: {'classification': 140.2724, 'neural_augmentation': 8.738, 'total_loss': 149.0104}, LR: [0.000463, 0.000463], Avg. batch load time: 0.014, Elapsed time: 20926.16
2024-07-20 13:27:55 - [34m[1mLOGS   [0m - Epoch:   0 [    4750/  100000], loss: {'classification': 137.2213, 'neural_augmentation': 8.6978, 'total_loss': 145.919}, LR: [0.000476, 0.000476], Avg. batch load time: 0.013, Elapsed time: 21485.25
2024-07-20 13:37:14 - [34m[1mLOGS   [0m - Epoch:   0 [    4875/  100000], loss: {'classification': 134.2823, 'neural_augmentation': 8.6551, 'total_loss': 142.9375}, LR: [0.000488, 0.000488], Avg. batch load time: 0.013, Elapsed time: 22044.54
2024-07-20 13:46:33 - [34m[1mLOGS   [0m - Epoch:   0 [    5000/  100000], loss: {'classification': 131.72, 'neural_augmentation': 8.613, 'total_loss': 140.333}, LR: [0.0005, 0.0005], Avg. batch load time: 0.013, Elapsed time: 22603.37
2024-07-20 13:55:52 - [34m[1mLOGS   [0m - Epoch:   0 [    5125/  100000], loss: {'classification': 128.997, 'neural_augmentation': 8.5659, 'total_loss': 137.563}, LR: [0.000513, 0.000513], Avg. batch load time: 0.012, Elapsed time: 23162.36
2024-07-20 14:05:12 - [34m[1mLOGS   [0m - Epoch:   0 [    5250/  100000], loss: {'classification': 126.4509, 'neural_augmentation': 8.5173, 'total_loss': 134.9681}, LR: [0.000525, 0.000525], Avg. batch load time: 0.012, Elapsed time: 23721.76
2024-07-20 14:14:30 - [34m[1mLOGS   [0m - Epoch:   0 [    5375/  100000], loss: {'classification': 124.1034, 'neural_augmentation': 8.4687, 'total_loss': 132.5721}, LR: [0.000538, 0.000538], Avg. batch load time: 0.012, Elapsed time: 24279.97
2024-07-20 14:23:52 - [34m[1mLOGS   [0m - Epoch:   0 [    5500/  100000], loss: {'classification': 121.8357, 'neural_augmentation': 8.4183, 'total_loss': 130.254}, LR: [0.00055, 0.00055], Avg. batch load time: 0.012, Elapsed time: 24841.71
2024-07-20 14:33:11 - [34m[1mLOGS   [0m - Epoch:   0 [    5625/  100000], loss: {'classification': 119.6523, 'neural_augmentation': 8.3655, 'total_loss': 128.0178}, LR: [0.000563, 0.000563], Avg. batch load time: 0.011, Elapsed time: 25401.62
2024-07-20 14:42:31 - [34m[1mLOGS   [0m - Epoch:   0 [    5750/  100000], loss: {'classification': 117.5811, 'neural_augmentation': 8.3104, 'total_loss': 125.8915}, LR: [0.000575, 0.000575], Avg. batch load time: 0.011, Elapsed time: 25960.72
2024-07-20 14:51:49 - [34m[1mLOGS   [0m - Epoch:   0 [    5875/  100000], loss: {'classification': 115.6222, 'neural_augmentation': 8.2536, 'total_loss': 123.8757}, LR: [0.000588, 0.000588], Avg. batch load time: 0.011, Elapsed time: 26519.56
2024-07-20 15:01:08 - [34m[1mLOGS   [0m - Epoch:   0 [    6000/  100000], loss: {'classification': 113.7408, 'neural_augmentation': 8.1951, 'total_loss': 121.9359}, LR: [0.0006, 0.0006], Avg. batch load time: 0.011, Elapsed time: 27078.23
2024-07-20 15:10:28 - [34m[1mLOGS   [0m - Epoch:   0 [    6125/  100000], loss: {'classification': 111.918, 'neural_augmentation': 8.1343, 'total_loss': 120.0523}, LR: [0.000613, 0.000613], Avg. batch load time: 0.011, Elapsed time: 27637.64
2024-07-20 15:19:46 - [34m[1mLOGS   [0m - Epoch:   0 [    6250/  100000], loss: {'classification': 110.8478, 'neural_augmentation': 8.0762, 'total_loss': 118.9241}, LR: [0.000625, 0.000625], Avg. batch load time: 0.010, Elapsed time: 28196.48
2024-07-20 15:29:07 - [34m[1mLOGS   [0m - Epoch:   0 [    6375/  100000], loss: {'classification': 110.6244, 'neural_augmentation': 8.0182, 'total_loss': 118.6426}, LR: [0.000638, 0.000638], Avg. batch load time: 0.010, Elapsed time: 28757.55
2024-07-20 15:38:29 - [34m[1mLOGS   [0m - Epoch:   0 [    6500/  100000], loss: {'classification': 110.398, 'neural_augmentation': 7.9586, 'total_loss': 118.3566}, LR: [0.00065, 0.00065], Avg. batch load time: 0.010, Elapsed time: 29319.31
2024-07-20 15:47:49 - [34m[1mLOGS   [0m - Epoch:   0 [    6625/  100000], loss: {'classification': 110.1694, 'neural_augmentation': 7.897, 'total_loss': 118.0664}, LR: [0.000663, 0.000663], Avg. batch load time: 0.010, Elapsed time: 29879.25
2024-07-20 15:57:08 - [34m[1mLOGS   [0m - Epoch:   0 [    6750/  100000], loss: {'classification': 109.9505, 'neural_augmentation': 7.8365, 'total_loss': 117.787}, LR: [0.000675, 0.000675], Avg. batch load time: 0.010, Elapsed time: 30438.51
2024-07-20 16:06:28 - [34m[1mLOGS   [0m - Epoch:   0 [    6875/  100000], loss: {'classification': 109.746, 'neural_augmentation': 7.7761, 'total_loss': 117.5221}, LR: [0.000688, 0.000688], Avg. batch load time: 0.010, Elapsed time: 30997.96
2024-07-20 16:15:47 - [34m[1mLOGS   [0m - Epoch:   0 [    7000/  100000], loss: {'classification': 109.5292, 'neural_augmentation': 7.7111, 'total_loss': 117.2402}, LR: [0.0007, 0.0007], Avg. batch load time: 0.009, Elapsed time: 31557.42
2024-07-20 16:25:06 - [34m[1mLOGS   [0m - Epoch:   0 [    7125/  100000], loss: {'classification': 109.3297, 'neural_augmentation': 7.6487, 'total_loss': 116.9784}, LR: [0.000713, 0.000713], Avg. batch load time: 0.009, Elapsed time: 32116.18
2024-07-20 16:34:26 - [34m[1mLOGS   [0m - Epoch:   0 [    7250/  100000], loss: {'classification': 109.1331, 'neural_augmentation': 7.5836, 'total_loss': 116.7167}, LR: [0.000725, 0.000725], Avg. batch load time: 0.009, Elapsed time: 32675.83
2024-07-20 16:43:49 - [34m[1mLOGS   [0m - Epoch:   0 [    7375/  100000], loss: {'classification': 108.9452, 'neural_augmentation': 7.5189, 'total_loss': 116.464}, LR: [0.000738, 0.000738], Avg. batch load time: 0.009, Elapsed time: 33239.04
2024-07-20 16:53:09 - [34m[1mLOGS   [0m - Epoch:   0 [    7500/  100000], loss: {'classification': 108.7499, 'neural_augmentation': 7.4496, 'total_loss': 116.1995}, LR: [0.00075, 0.00075], Avg. batch load time: 0.009, Elapsed time: 33799.43
2024-07-20 17:02:29 - [34m[1mLOGS   [0m - Epoch:   0 [    7625/  100000], loss: {'classification': 108.5713, 'neural_augmentation': 7.3839, 'total_loss': 115.9551}, LR: [0.000763, 0.000763], Avg. batch load time: 0.009, Elapsed time: 34358.78
2024-07-20 17:11:49 - [34m[1mLOGS   [0m - Epoch:   0 [    7750/  100000], loss: {'classification': 108.391, 'neural_augmentation': 7.3147, 'total_loss': 115.7057}, LR: [0.000775, 0.000775], Avg. batch load time: 0.009, Elapsed time: 34919.33
2024-07-20 17:19:33 - [34m[1mLOGS   [0m - *** Training summary for epoch 0
	 loss={'classification': 108.2487, 'neural_augmentation': 7.2592, 'total_loss': 115.5078}
2024-07-20 17:19:35 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_best.pt
2024-07-20 17:19:36 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_last.pt
2024-07-20 17:19:36 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_last.pt
2024-07-20 17:19:37 - [34m[1mLOGS   [0m - Training checkpoint for epoch 0/iteration 7853 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/training_checkpoint_epoch_0_iter_7853.pt
2024-07-20 17:19:38 - [34m[1mLOGS   [0m - Model state for epoch 0/iteration 7853 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_0_iter_7853.pt
[31m===========================================================================[0m
2024-07-20 17:19:40 - [32m[1mINFO   [0m - Training epoch 1
2024-07-20 17:21:31 - [34m[1mLOGS   [0m - Epoch:   1 [    7853/  100000], loss: {'classification': 60.1225, 'neural_augmentation': 2.8918, 'total_loss': 63.0144}, LR: [0.000786, 0.000786], Avg. batch load time: 110.736, Elapsed time: 111.85
