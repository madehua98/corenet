nohup: ignoring input
2024-07-14 09:44:24 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
2024-07-14 09:44:26 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.fc1.weight', 'blocks1.0.mlp.fc1.bias', 'blocks1.0.mlp.fc2.weight', 'blocks1.0.mlp.fc2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.fc1.weight', 'blocks1.1.mlp.fc1.bias', 'blocks1.1.mlp.fc2.weight', 'blocks1.1.mlp.fc2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.fc1.weight', 'blocks1.2.mlp.fc1.bias', 'blocks1.2.mlp.fc2.weight', 'blocks1.2.mlp.fc2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.fc1.weight', 'blocks1.3.mlp.fc1.bias', 'blocks1.3.mlp.fc2.weight', 'blocks1.3.mlp.fc2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.fc1.weight', 'blocks1.4.mlp.fc1.bias', 'blocks1.4.mlp.fc2.weight', 'blocks1.4.mlp.fc2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.fc1.weight', 'blocks1.5.mlp.fc1.bias', 'blocks1.5.mlp.fc2.weight', 'blocks1.5.mlp.fc2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.fc1.weight', 'blocks1.6.mlp.fc1.bias', 'blocks1.6.mlp.fc2.weight', 'blocks1.6.mlp.fc2.bias', 'blocks1.7.norm1.weight', 'blocks1.7.norm1.bias', 'blocks1.7.attn.qkv.weight', 'blocks1.7.attn.qkv.bias', 'blocks1.7.attn.proj.weight', 'blocks1.7.attn.proj.bias', 'blocks1.7.norm2.weight', 'blocks1.7.norm2.bias', 'blocks1.7.mlp.fc1.weight', 'blocks1.7.mlp.fc1.bias', 'blocks1.7.mlp.fc2.weight', 'blocks1.7.mlp.fc2.bias', 'block_to_block1.weight', 'block_to_block1.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-14 09:44:26 - [34m[1mLOGS   [0m - [36mModel[0m
ViTamin(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(384, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (block_to_block1): LinearLayer(in_features=384, out_features=512, bias=True, channel_first=False)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=6743, bias=True, channel_first=False)
)
[31m=================================================================[0m
                            ViTamin Summary
[31m=================================================================[0m
Total parameters     =   46.446 M
Total trainable parameters =   46.446 M

2024-07-14 09:44:27 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-14 09:44:27 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 256, 256]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 46.446M                | 7.461G     |
|  pos_embed                           |  (1, 1, 384)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  1.077M                |  1.881G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.638G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    28.312M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    5.243M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.604G  |
|   patch_embed.backbone.stages        |   0.595M               |   1.13G    |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.495G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.635G  |
|   patch_embed.backbone.pool          |   0.443M               |   0.114G   |
|    patch_embed.backbone.pool.proj    |    0.443M              |    0.113G  |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.655M  |
|  blocks                              |  14.196M               |  3.632G    |
|   blocks.0                           |   1.774M               |   0.454G   |
|    blocks.0.norm1                    |    0.768K              |    0.492M  |
|    blocks.0.attn                     |    0.591M              |    0.151G  |
|    blocks.0.norm2                    |    0.768K              |    0.492M  |
|    blocks.0.mlp                      |    1.182M              |    0.302G  |
|   blocks.1                           |   1.774M               |   0.454G   |
|    blocks.1.norm1                    |    0.768K              |    0.492M  |
|    blocks.1.attn                     |    0.591M              |    0.151G  |
|    blocks.1.norm2                    |    0.768K              |    0.492M  |
|    blocks.1.mlp                      |    1.182M              |    0.302G  |
|   blocks.2                           |   1.774M               |   0.454G   |
|    blocks.2.norm1                    |    0.768K              |    0.492M  |
|    blocks.2.attn                     |    0.591M              |    0.151G  |
|    blocks.2.norm2                    |    0.768K              |    0.492M  |
|    blocks.2.mlp                      |    1.182M              |    0.302G  |
|   blocks.3                           |   1.774M               |   0.454G   |
|    blocks.3.norm1                    |    0.768K              |    0.492M  |
|    blocks.3.attn                     |    0.591M              |    0.151G  |
|    blocks.3.norm2                    |    0.768K              |    0.492M  |
|    blocks.3.mlp                      |    1.182M              |    0.302G  |
|   blocks.4                           |   1.774M               |   0.454G   |
|    blocks.4.norm1                    |    0.768K              |    0.492M  |
|    blocks.4.attn                     |    0.591M              |    0.151G  |
|    blocks.4.norm2                    |    0.768K              |    0.492M  |
|    blocks.4.mlp                      |    1.182M              |    0.302G  |
|   blocks.5                           |   1.774M               |   0.454G   |
|    blocks.5.norm1                    |    0.768K              |    0.492M  |
|    blocks.5.attn                     |    0.591M              |    0.151G  |
|    blocks.5.norm2                    |    0.768K              |    0.492M  |
|    blocks.5.mlp                      |    1.182M              |    0.302G  |
|   blocks.6                           |   1.774M               |   0.454G   |
|    blocks.6.norm1                    |    0.768K              |    0.492M  |
|    blocks.6.attn                     |    0.591M              |    0.151G  |
|    blocks.6.norm2                    |    0.768K              |    0.492M  |
|    blocks.6.mlp                      |    1.182M              |    0.302G  |
|   blocks.7                           |   1.774M               |   0.454G   |
|    blocks.7.norm1                    |    0.768K              |    0.492M  |
|    blocks.7.attn                     |    0.591M              |    0.151G  |
|    blocks.7.norm2                    |    0.768K              |    0.492M  |
|    blocks.7.mlp                      |    1.182M              |    0.302G  |
|  pool                                |  1.771M                |  0.114G    |
|   pool.proj                          |   1.77M                |   0.113G   |
|    pool.proj.weight                  |    (512, 384, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.768K               |   0.492M   |
|    pool.norm.weight                  |    (384,)              |            |
|    pool.norm.bias                    |    (384,)              |            |
|  blocks1                             |  25.219M               |  1.613G    |
|   blocks1.0                          |   3.152M               |   0.202G   |
|    blocks1.0.norm1                   |    1.024K              |    0.164M  |
|    blocks1.0.attn                    |    1.051M              |    67.109M |
|    blocks1.0.norm2                   |    1.024K              |    0.164M  |
|    blocks1.0.mlp                     |    2.1M                |    0.134G  |
|   blocks1.1                          |   3.152M               |   0.202G   |
|    blocks1.1.norm1                   |    1.024K              |    0.164M  |
|    blocks1.1.attn                    |    1.051M              |    67.109M |
|    blocks1.1.norm2                   |    1.024K              |    0.164M  |
|    blocks1.1.mlp                     |    2.1M                |    0.134G  |
|   blocks1.2                          |   3.152M               |   0.202G   |
|    blocks1.2.norm1                   |    1.024K              |    0.164M  |
|    blocks1.2.attn                    |    1.051M              |    67.109M |
|    blocks1.2.norm2                   |    1.024K              |    0.164M  |
|    blocks1.2.mlp                     |    2.1M                |    0.134G  |
|   blocks1.3                          |   3.152M               |   0.202G   |
|    blocks1.3.norm1                   |    1.024K              |    0.164M  |
|    blocks1.3.attn                    |    1.051M              |    67.109M |
|    blocks1.3.norm2                   |    1.024K              |    0.164M  |
|    blocks1.3.mlp                     |    2.1M                |    0.134G  |
|   blocks1.4                          |   3.152M               |   0.202G   |
|    blocks1.4.norm1                   |    1.024K              |    0.164M  |
|    blocks1.4.attn                    |    1.051M              |    67.109M |
|    blocks1.4.norm2                   |    1.024K              |    0.164M  |
|    blocks1.4.mlp                     |    2.1M                |    0.134G  |
|   blocks1.5                          |   3.152M               |   0.202G   |
|    blocks1.5.norm1                   |    1.024K              |    0.164M  |
|    blocks1.5.attn                    |    1.051M              |    67.109M |
|    blocks1.5.norm2                   |    1.024K              |    0.164M  |
|    blocks1.5.mlp                     |    2.1M                |    0.134G  |
|   blocks1.6                          |   3.152M               |   0.202G   |
|    blocks1.6.norm1                   |    1.024K              |    0.164M  |
|    blocks1.6.attn                    |    1.051M              |    67.109M |
|    blocks1.6.norm2                   |    1.024K              |    0.164M  |
|    blocks1.6.mlp                     |    2.1M                |    0.134G  |
|   blocks1.7                          |   3.152M               |   0.202G   |
|    blocks1.7.norm1                   |    1.024K              |    0.164M  |
|    blocks1.7.attn                    |    1.051M              |    67.109M |
|    blocks1.7.norm2                   |    1.024K              |    0.164M  |
|    blocks1.7.mlp                     |    2.1M                |    0.134G  |
|  block_to_block1                     |  0.197M                |  50.332M   |
|   block_to_block1.weight             |   (512, 384)           |            |
|   block_to_block1.bias               |   (512,)               |            |
|  mlp                                 |  0.525M                |  0.168G    |
|   mlp.0                              |   0.263M               |   83.886M  |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   83.886M  |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  3.459M                |  3.452M    |
|   classifier.weight                  |   (6743, 512)          |            |
|   classifier.bias                    |   (6743,)              |            |
2024-07-14 09:44:27 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-14 09:44:27 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks.7.attn.attn_drop', 'blocks.3.ls2', 'blocks.1.ls1', 'blocks1.6.ls1', 'blocks1.4.attn.q_norm', 'blocks1.2.attn.attn_drop', 'blocks1.6.attn.k_norm', 'blocks.2.ls1', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'patch_embed.backbone.stages.1.1.drop_path', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks1.3.attn.attn_drop', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks1.0.drop_path1', 'blocks.0.ls1', 'blocks1.3.attn.k_norm', 'blocks.3.attn.q_norm', 'blocks1.6.mlp.norm', 'blocks1.0.attn.k_norm', 'blocks.0.attn.attn_drop', 'blocks.1.drop_path2', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.5.attn.attn_drop', 'blocks.4.attn.q_norm', 'neural_augmentor.contrast', 'blocks1.0.mlp.norm', 'blocks1.3.ls2', 'blocks.3.mlp.norm', 'blocks.0.ls2', 'blocks.0.attn.k_norm', 'patch_embed.backbone.stages.1.0.down', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks1.2.ls1', 'neural_augmentor.noise.min_fn', 'blocks1.6.drop_path1', 'blocks.4.drop_path2', 'blocks.2.mlp.norm', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks.4.ls2', 'blocks1.2.drop_path2', 'blocks.1.drop_path1', 'blocks.3.drop_path2', 'blocks.6.ls1', 'blocks.6.drop_path2', 'blocks.2.drop_path1', 'blocks1.1.attn.attn_drop', 'blocks.5.ls1', 'blocks.0.drop_path2', 'blocks1.1.drop_path2', 'blocks.4.ls1', 'blocks.5.ls2', 'blocks1.0.ls2', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks1.1.attn.k_norm', 'patch_embed.backbone.stages.1.3.down', 'neural_augmentor.contrast.max_fn', 'patch_embed.backbone.stages.0.0.down', 'blocks.2.drop_path2', 'blocks.4.mlp.norm', 'blocks1.5.attn.q_norm', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks1.1.attn.q_norm', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks1.0.ls1', 'blocks1.4.ls2', 'blocks1.3.drop_path1', 'blocks.6.ls2', 'blocks1.1.ls2', 'blocks1.4.attn.k_norm', 'blocks1.7.attn.attn_drop', 'blocks.7.ls2', 'blocks.1.ls2', 'blocks1.0.attn.attn_drop', 'patch_embed.backbone.stages.0.0.drop_path', 'neural_augmentor.contrast.min_fn', 'blocks.7.ls1', 'neural_augmentor.noise.max_fn', 'blocks.6.attn.q_norm', 'blocks1.7.ls2', 'blocks1.3.attn.q_norm', 'blocks1.4.attn.attn_drop', 'blocks.7.attn.k_norm', 'blocks1.2.drop_path1', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks.2.ls2', 'blocks.6.attn.attn_drop', 'blocks.7.drop_path2', 'blocks.6.mlp.norm', 'blocks1.6.attn.attn_drop', 'patch_embed.backbone.stages.1.2.down', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks.5.drop_path1', 'blocks.1.mlp.norm', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks.5.attn.attn_drop', 'blocks1.2.mlp.norm', 'patch_embed.backbone.stem.norm1.drop', 'blocks.4.drop_path1', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks1.1.ls1', 'blocks1.3.ls1', 'patch_embed.proj', 'blocks.5.mlp.norm', 'blocks1.2.attn.q_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks.1.attn.q_norm', 'blocks1.5.attn.k_norm', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks.0.attn.q_norm', 'blocks.7.mlp.norm', 'blocks.2.attn.k_norm', 'blocks.4.attn.k_norm', 'blocks1.2.ls2', 'blocks.0.mlp.norm', 'blocks1.5.mlp.norm', 'blocks.5.attn.q_norm', 'blocks1.1.mlp.norm', 'blocks1.0.attn.q_norm', 'blocks1.4.drop_path1', 'blocks1.3.drop_path2', 'neural_augmentor', 'blocks1.6.drop_path2', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks.7.attn.q_norm', 'blocks1.4.mlp.norm', 'blocks1.7.drop_path1', 'blocks1.7.attn.k_norm', 'patch_drop', 'blocks1.5.drop_path1', 'blocks.7.drop_path1', 'blocks.3.attn.attn_drop', 'blocks1.2.attn.k_norm', 'neural_augmentor.brightness', 'neural_augmentor.brightness.max_fn', 'blocks.1.attn.k_norm', 'blocks1.4.drop_path2', 'blocks1.1.drop_path1', 'blocks.3.attn.k_norm', 'blocks.5.drop_path2', 'blocks1.3.mlp.norm', 'norm', 'blocks1.5.drop_path2', 'blocks.2.attn.attn_drop', 'blocks1.6.attn.q_norm', 'blocks1.5.ls2', 'blocks1.7.attn.q_norm', 'blocks.2.attn.q_norm', 'blocks.1.attn.attn_drop', 'blocks1.7.mlp.norm', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks1.6.ls2', 'patch_embed.backbone.stages.1.2.drop_path', 'patch_embed.backbone.stages.0.1.down', 'blocks.3.drop_path1', 'blocks.6.attn.k_norm', 'blocks.5.attn.k_norm', 'blocks1.7.ls1', 'blocks1.5.ls1', 'blocks.4.attn.attn_drop', 'blocks1.7.drop_path2', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks1.4.ls1', 'patch_embed.backbone.stages.1.1.down', 'blocks1.0.drop_path2', 'blocks.0.drop_path1', 'neural_augmentor.brightness.min_fn', 'neural_augmentor.noise', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks.6.drop_path1', 'blocks.3.ls1', 'norm_pre'}
2024-07-14 09:44:27 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 39, 'aten::gelu': 30, 'aten::scaled_dot_product_attention': 16, 'aten::avg_pool2d': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-14 09:44:27 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-14 09:44:27 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-14 09:44:27 - [34m[1mLOGS   [0m - Available GPUs: 6
2024-07-14 09:44:27 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-14 09:44:27 - [34m[1mLOGS   [0m - Directory exists at: /ML-A100/team/mm/models/catlip_data/results500_1/train
2024-07-14 09:44:31 - [32m[1mINFO   [0m - distributed init (rank 4): tcp://localhost:40002
2024-07-14 09:44:31 - [32m[1mINFO   [0m - distributed init (rank 5): tcp://localhost:40002
2024-07-14 09:44:31 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:40002
2024-07-14 09:44:31 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:40002
2024-07-14 09:44:31 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:40002
2024-07-14 09:44:34 - [34m[1mLOGS   [0m - Training dataset details are given below
WordnetTaggedClassificationDataset(
	root= 
	is_training=True 
	num_samples=10000000
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	total_tar_files=1000
	max_files_per_tar=10000
	num_synsets=6743
)
2024-07-14 09:44:35 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=True
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=512
	 scales=[(128, 128, 1568), (144, 144, 1238), (160, 160, 1003), (176, 176, 829), (192, 192, 696), (208, 208, 593), (224, 224, 512), (240, 240, 446), (256, 256, 392), (272, 272, 347), (288, 288, 309), (304, 304, 277), (320, 320, 250)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-14 09:44:35 - [34m[1mLOGS   [0m - Number of data workers: 64
2024-07-14 09:44:35 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.fc1.weight', 'blocks1.0.mlp.fc1.bias', 'blocks1.0.mlp.fc2.weight', 'blocks1.0.mlp.fc2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.fc1.weight', 'blocks1.1.mlp.fc1.bias', 'blocks1.1.mlp.fc2.weight', 'blocks1.1.mlp.fc2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.fc1.weight', 'blocks1.2.mlp.fc1.bias', 'blocks1.2.mlp.fc2.weight', 'blocks1.2.mlp.fc2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.fc1.weight', 'blocks1.3.mlp.fc1.bias', 'blocks1.3.mlp.fc2.weight', 'blocks1.3.mlp.fc2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.fc1.weight', 'blocks1.4.mlp.fc1.bias', 'blocks1.4.mlp.fc2.weight', 'blocks1.4.mlp.fc2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.fc1.weight', 'blocks1.5.mlp.fc1.bias', 'blocks1.5.mlp.fc2.weight', 'blocks1.5.mlp.fc2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.fc1.weight', 'blocks1.6.mlp.fc1.bias', 'blocks1.6.mlp.fc2.weight', 'blocks1.6.mlp.fc2.bias', 'blocks1.7.norm1.weight', 'blocks1.7.norm1.bias', 'blocks1.7.attn.qkv.weight', 'blocks1.7.attn.qkv.bias', 'blocks1.7.attn.proj.weight', 'blocks1.7.attn.proj.bias', 'blocks1.7.norm2.weight', 'blocks1.7.norm2.bias', 'blocks1.7.mlp.fc1.weight', 'blocks1.7.mlp.fc1.bias', 'blocks1.7.mlp.fc2.weight', 'blocks1.7.mlp.fc2.bias', 'block_to_block1.weight', 'block_to_block1.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-14 09:44:35 - [34m[1mLOGS   [0m - [36mModel[0m
ViTamin(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(384, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (7): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU(approximate='none')
        (drop1): Dropout(p=0.0, inplace=False)
        (norm): Identity()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop2): Dropout(p=0.0, inplace=False)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (block_to_block1): LinearLayer(in_features=384, out_features=512, bias=True, channel_first=False)
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=6743, bias=True, channel_first=False)
)
[31m=================================================================[0m
                            ViTamin Summary
[31m=================================================================[0m
Total parameters     =   46.446 M
Total trainable parameters =   46.446 M

2024-07-14 09:44:35 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-14 09:44:35 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 256, 256]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 46.446M                | 7.461G     |
|  pos_embed                           |  (1, 1, 384)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  1.077M                |  1.881G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.638G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    28.312M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    5.243M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.604G  |
|   patch_embed.backbone.stages        |   0.595M               |   1.13G    |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.495G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.635G  |
|   patch_embed.backbone.pool          |   0.443M               |   0.114G   |
|    patch_embed.backbone.pool.proj    |    0.443M              |    0.113G  |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.655M  |
|  blocks                              |  14.196M               |  3.632G    |
|   blocks.0                           |   1.774M               |   0.454G   |
|    blocks.0.norm1                    |    0.768K              |    0.492M  |
|    blocks.0.attn                     |    0.591M              |    0.151G  |
|    blocks.0.norm2                    |    0.768K              |    0.492M  |
|    blocks.0.mlp                      |    1.182M              |    0.302G  |
|   blocks.1                           |   1.774M               |   0.454G   |
|    blocks.1.norm1                    |    0.768K              |    0.492M  |
|    blocks.1.attn                     |    0.591M              |    0.151G  |
|    blocks.1.norm2                    |    0.768K              |    0.492M  |
|    blocks.1.mlp                      |    1.182M              |    0.302G  |
|   blocks.2                           |   1.774M               |   0.454G   |
|    blocks.2.norm1                    |    0.768K              |    0.492M  |
|    blocks.2.attn                     |    0.591M              |    0.151G  |
|    blocks.2.norm2                    |    0.768K              |    0.492M  |
|    blocks.2.mlp                      |    1.182M              |    0.302G  |
|   blocks.3                           |   1.774M               |   0.454G   |
|    blocks.3.norm1                    |    0.768K              |    0.492M  |
|    blocks.3.attn                     |    0.591M              |    0.151G  |
|    blocks.3.norm2                    |    0.768K              |    0.492M  |
|    blocks.3.mlp                      |    1.182M              |    0.302G  |
|   blocks.4                           |   1.774M               |   0.454G   |
|    blocks.4.norm1                    |    0.768K              |    0.492M  |
|    blocks.4.attn                     |    0.591M              |    0.151G  |
|    blocks.4.norm2                    |    0.768K              |    0.492M  |
|    blocks.4.mlp                      |    1.182M              |    0.302G  |
|   blocks.5                           |   1.774M               |   0.454G   |
|    blocks.5.norm1                    |    0.768K              |    0.492M  |
|    blocks.5.attn                     |    0.591M              |    0.151G  |
|    blocks.5.norm2                    |    0.768K              |    0.492M  |
|    blocks.5.mlp                      |    1.182M              |    0.302G  |
|   blocks.6                           |   1.774M               |   0.454G   |
|    blocks.6.norm1                    |    0.768K              |    0.492M  |
|    blocks.6.attn                     |    0.591M              |    0.151G  |
|    blocks.6.norm2                    |    0.768K              |    0.492M  |
|    blocks.6.mlp                      |    1.182M              |    0.302G  |
|   blocks.7                           |   1.774M               |   0.454G   |
|    blocks.7.norm1                    |    0.768K              |    0.492M  |
|    blocks.7.attn                     |    0.591M              |    0.151G  |
|    blocks.7.norm2                    |    0.768K              |    0.492M  |
|    blocks.7.mlp                      |    1.182M              |    0.302G  |
|  pool                                |  1.771M                |  0.114G    |
|   pool.proj                          |   1.77M                |   0.113G   |
|    pool.proj.weight                  |    (512, 384, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.768K               |   0.492M   |
|    pool.norm.weight                  |    (384,)              |            |
|    pool.norm.bias                    |    (384,)              |            |
|  blocks1                             |  25.219M               |  1.613G    |
|   blocks1.0                          |   3.152M               |   0.202G   |
|    blocks1.0.norm1                   |    1.024K              |    0.164M  |
|    blocks1.0.attn                    |    1.051M              |    67.109M |
|    blocks1.0.norm2                   |    1.024K              |    0.164M  |
|    blocks1.0.mlp                     |    2.1M                |    0.134G  |
|   blocks1.1                          |   3.152M               |   0.202G   |
|    blocks1.1.norm1                   |    1.024K              |    0.164M  |
|    blocks1.1.attn                    |    1.051M              |    67.109M |
|    blocks1.1.norm2                   |    1.024K              |    0.164M  |
|    blocks1.1.mlp                     |    2.1M                |    0.134G  |
|   blocks1.2                          |   3.152M               |   0.202G   |
|    blocks1.2.norm1                   |    1.024K              |    0.164M  |
|    blocks1.2.attn                    |    1.051M              |    67.109M |
|    blocks1.2.norm2                   |    1.024K              |    0.164M  |
|    blocks1.2.mlp                     |    2.1M                |    0.134G  |
|   blocks1.3                          |   3.152M               |   0.202G   |
|    blocks1.3.norm1                   |    1.024K              |    0.164M  |
|    blocks1.3.attn                    |    1.051M              |    67.109M |
|    blocks1.3.norm2                   |    1.024K              |    0.164M  |
|    blocks1.3.mlp                     |    2.1M                |    0.134G  |
|   blocks1.4                          |   3.152M               |   0.202G   |
|    blocks1.4.norm1                   |    1.024K              |    0.164M  |
|    blocks1.4.attn                    |    1.051M              |    67.109M |
|    blocks1.4.norm2                   |    1.024K              |    0.164M  |
|    blocks1.4.mlp                     |    2.1M                |    0.134G  |
|   blocks1.5                          |   3.152M               |   0.202G   |
|    blocks1.5.norm1                   |    1.024K              |    0.164M  |
|    blocks1.5.attn                    |    1.051M              |    67.109M |
|    blocks1.5.norm2                   |    1.024K              |    0.164M  |
|    blocks1.5.mlp                     |    2.1M                |    0.134G  |
|   blocks1.6                          |   3.152M               |   0.202G   |
|    blocks1.6.norm1                   |    1.024K              |    0.164M  |
|    blocks1.6.attn                    |    1.051M              |    67.109M |
|    blocks1.6.norm2                   |    1.024K              |    0.164M  |
|    blocks1.6.mlp                     |    2.1M                |    0.134G  |
|   blocks1.7                          |   3.152M               |   0.202G   |
|    blocks1.7.norm1                   |    1.024K              |    0.164M  |
|    blocks1.7.attn                    |    1.051M              |    67.109M |
|    blocks1.7.norm2                   |    1.024K              |    0.164M  |
|    blocks1.7.mlp                     |    2.1M                |    0.134G  |
|  block_to_block1                     |  0.197M                |  50.332M   |
|   block_to_block1.weight             |   (512, 384)           |            |
|   block_to_block1.bias               |   (512,)               |            |
|  mlp                                 |  0.525M                |  0.168G    |
|   mlp.0                              |   0.263M               |   83.886M  |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   83.886M  |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  3.459M                |  3.452M    |
|   classifier.weight                  |   (6743, 512)          |            |
|   classifier.bias                    |   (6743,)              |            |
2024-07-14 09:44:36 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-14 09:44:36 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks.3.ls2', 'blocks1.6.attn.k_norm', 'neural_augmentor.brightness.min_fn', 'blocks.5.attn.q_norm', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks1.1.drop_path1', 'blocks.0.attn.attn_drop', 'blocks1.1.ls1', 'blocks.0.ls1', 'blocks1.3.mlp.norm', 'blocks1.0.ls2', 'blocks1.0.attn.q_norm', 'blocks1.0.attn.attn_drop', 'neural_augmentor', 'blocks.2.drop_path2', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks.6.attn.attn_drop', 'norm', 'blocks.2.mlp.norm', 'neural_augmentor.brightness.max_fn', 'blocks.5.drop_path1', 'neural_augmentor.noise', 'patch_embed.backbone.stages.0.0.down', 'blocks1.5.attn.k_norm', 'blocks.5.mlp.norm', 'blocks1.5.ls1', 'blocks.7.ls1', 'blocks.3.attn.k_norm', 'blocks1.7.attn.k_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks1.5.mlp.norm', 'blocks1.6.drop_path1', 'blocks.6.attn.q_norm', 'blocks1.7.drop_path2', 'blocks.2.ls2', 'neural_augmentor.contrast.min_fn', 'blocks.4.ls1', 'blocks1.0.mlp.norm', 'blocks.7.attn.q_norm', 'blocks.1.ls2', 'blocks.4.drop_path2', 'neural_augmentor.noise.max_fn', 'blocks1.6.drop_path2', 'blocks1.3.ls2', 'blocks.4.attn.k_norm', 'blocks1.4.attn.k_norm', 'blocks1.4.drop_path1', 'patch_embed.backbone.stages.1.2.down', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks.7.ls2', 'patch_embed.backbone.stages.1.1.drop_path', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks1.2.drop_path2', 'patch_embed.proj', 'blocks.5.ls2', 'blocks.2.attn.q_norm', 'patch_embed.backbone.stages.1.3.down', 'blocks.0.drop_path1', 'blocks.7.attn.k_norm', 'blocks.7.drop_path1', 'blocks1.1.drop_path2', 'blocks.1.attn.k_norm', 'blocks1.7.drop_path1', 'blocks1.3.drop_path2', 'blocks1.7.ls1', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks1.3.drop_path1', 'blocks.1.ls1', 'blocks1.4.ls1', 'blocks.1.drop_path2', 'blocks.4.attn.attn_drop', 'blocks1.6.mlp.norm', 'blocks.3.attn.attn_drop', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks1.0.ls1', 'blocks.4.drop_path1', 'blocks.5.attn.attn_drop', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks1.1.attn.attn_drop', 'blocks.1.attn.attn_drop', 'blocks.6.ls2', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks.0.attn.q_norm', 'blocks1.4.mlp.norm', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.0.drop_path1', 'patch_drop', 'blocks1.5.drop_path1', 'blocks1.4.attn.attn_drop', 'blocks1.4.ls2', 'blocks1.7.ls2', 'blocks.7.attn.attn_drop', 'norm_pre', 'blocks1.2.ls1', 'blocks1.6.ls2', 'blocks.4.mlp.norm', 'blocks1.5.drop_path2', 'patch_embed.backbone.stem.norm1.drop', 'blocks1.4.attn.q_norm', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks1.5.attn.attn_drop', 'blocks1.0.drop_path2', 'blocks1.7.attn.attn_drop', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks1.6.attn.attn_drop', 'blocks.4.ls2', 'blocks.3.drop_path2', 'blocks.2.attn.attn_drop', 'blocks.4.attn.q_norm', 'blocks.3.mlp.norm', 'blocks.0.attn.k_norm', 'blocks.5.drop_path2', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks.2.attn.k_norm', 'blocks.0.ls2', 'neural_augmentor.contrast.max_fn', 'blocks1.1.attn.k_norm', 'blocks.1.drop_path1', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks1.0.attn.k_norm', 'blocks1.6.attn.q_norm', 'blocks.5.ls1', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks1.3.ls1', 'neural_augmentor.noise.min_fn', 'blocks.0.mlp.norm', 'blocks.2.ls1', 'blocks1.5.attn.q_norm', 'blocks.0.drop_path2', 'blocks.2.drop_path1', 'blocks1.5.ls2', 'blocks.3.drop_path1', 'patch_embed.backbone.stages.1.0.down', 'blocks.6.drop_path1', 'neural_augmentor.brightness', 'blocks.6.mlp.norm', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks1.2.attn.q_norm', 'blocks.6.ls1', 'blocks1.1.attn.q_norm', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks1.2.mlp.norm', 'blocks1.2.attn.attn_drop', 'blocks.1.attn.q_norm', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks.1.mlp.norm', 'blocks1.3.attn.q_norm', 'neural_augmentor.contrast', 'blocks.6.attn.k_norm', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks.3.attn.q_norm', 'blocks1.7.mlp.norm', 'blocks1.2.ls2', 'blocks1.7.attn.q_norm', 'blocks1.6.ls1', 'blocks.7.drop_path2', 'blocks.7.mlp.norm', 'blocks1.3.attn.k_norm', 'patch_embed.backbone.stages.1.0.drop_path', 'patch_embed.backbone.stages.1.1.down', 'blocks1.4.drop_path2', 'patch_embed.backbone.stages.0.1.down', 'blocks.5.attn.k_norm', 'blocks1.1.ls2', 'blocks1.2.drop_path1', 'blocks.3.ls1', 'blocks.6.drop_path2', 'blocks1.1.mlp.norm', 'blocks1.2.attn.k_norm', 'blocks1.3.attn.attn_drop'}
2024-07-14 09:44:36 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 39, 'aten::gelu': 30, 'aten::scaled_dot_product_attention': 16, 'aten::avg_pool2d': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-14 09:44:36 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-14 09:44:36 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-14 09:44:36 - [34m[1mLOGS   [0m - [36mOptimizer[0m
AdamWOptimizer (
	 amsgrad: [False, False]
	 betas: [(0.9, 0.999), (0.9, 0.999)]
	 capturable: [False, False]
	 differentiable: [False, False]
	 eps: [1e-08, 1e-08]
	 foreach: [None, None]
	 fused: [None, None]
	 lr: [0.1, 0.1]
	 maximize: [False, False]
	 weight_decay: [0.2, 0.0]
)
2024-07-14 09:44:36 - [34m[1mLOGS   [0m - Max. iteration for training: 1000000
2024-07-14 09:44:36 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=1e-05
 	 max_lr=0.001
 	 period=990001
 	 warmup_init_lr=1e-06
 	 warmup_iters=10000
 )
2024-07-14 09:44:36 - [34m[1mLOGS   [0m - No checkpoint found at '/ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_last.pt'
2024-07-14 09:44:36 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results500_1/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-14 09:44:38 - [32m[1mINFO   [0m - Training epoch 0
2024-07-14 09:44:31 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:40002
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-14 09:48:47 - [34m[1mLOGS   [0m - Epoch:   0 [       1/ 1000000], loss: {'classification': 4803.554, 'neural_augmentation': 8.6746, 'total_loss': 4812.2285}, LR: [1e-06, 1e-06], Avg. batch load time: 238.830, Elapsed time: 248.76
2024-07-14 09:55:00 - [34m[1mLOGS   [0m - Epoch:   0 [     501/ 1000000], loss: {'classification': 1600.816, 'neural_augmentation': 9.2662, 'total_loss': 1610.0822}, LR: [5.1e-05, 5.1e-05], Avg. batch load time: 0.515, Elapsed time: 621.91
2024-07-14 10:00:34 - [34m[1mLOGS   [0m - Epoch:   0 [    1001/ 1000000], loss: {'classification': 814.9384, 'neural_augmentation': 9.2716, 'total_loss': 824.21}, LR: [0.000101, 0.000101], Avg. batch load time: 0.258, Elapsed time: 955.87
2024-07-14 10:06:10 - [34m[1mLOGS   [0m - Epoch:   0 [    1501/ 1000000], loss: {'classification': 553.0587, 'neural_augmentation': 9.2493, 'total_loss': 562.308}, LR: [0.000151, 0.000151], Avg. batch load time: 0.172, Elapsed time: 1291.63
2024-07-14 10:11:44 - [34m[1mLOGS   [0m - Epoch:   0 [    2001/ 1000000], loss: {'classification': 421.1237, 'neural_augmentation': 9.2074, 'total_loss': 430.3311}, LR: [0.000201, 0.000201], Avg. batch load time: 0.129, Elapsed time: 1626.02
2024-07-14 10:17:19 - [34m[1mLOGS   [0m - Epoch:   0 [    2501/ 1000000], loss: {'classification': 343.6781, 'neural_augmentation': 9.1529, 'total_loss': 352.8309}, LR: [0.000251, 0.000251], Avg. batch load time: 0.104, Elapsed time: 1961.01
2024-07-14 10:18:11 - [34m[1mLOGS   [0m - *** Training summary for epoch 0
	 loss={'classification': 335.3059, 'neural_augmentation': 9.1459, 'total_loss': 344.4518}
2024-07-14 10:18:12 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_best.pt
2024-07-14 10:18:12 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_last.pt
2024-07-14 10:18:13 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_last.pt
2024-07-14 10:18:13 - [34m[1mLOGS   [0m - Training checkpoint for epoch 0/iteration 2579 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_epoch_0_iter_2579.pt
2024-07-14 10:18:13 - [34m[1mLOGS   [0m - Model state for epoch 0/iteration 2579 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_epoch_0_iter_2579.pt
[31m===========================================================================[0m
2024-07-14 10:18:15 - [32m[1mINFO   [0m - Training epoch 1
2024-07-14 10:18:39 - [34m[1mLOGS   [0m - Epoch:   1 [    2580/ 1000000], loss: {'classification': 24.114, 'neural_augmentation': 9.2882, 'total_loss': 33.4022}, LR: [0.000259, 0.000259], Avg. batch load time: 23.503, Elapsed time: 24.18
2024-07-14 10:24:36 - [34m[1mLOGS   [0m - Epoch:   1 [    3080/ 1000000], loss: {'classification': 24.8526, 'neural_augmentation': 8.6262, 'total_loss': 33.4788}, LR: [0.000309, 0.000309], Avg. batch load time: 0.091, Elapsed time: 381.16
2024-07-14 10:30:11 - [34m[1mLOGS   [0m - Epoch:   1 [    3580/ 1000000], loss: {'classification': 24.739, 'neural_augmentation': 8.5557, 'total_loss': 33.2947}, LR: [0.000359, 0.000359], Avg. batch load time: 0.046, Elapsed time: 715.39
2024-07-14 10:35:45 - [34m[1mLOGS   [0m - Epoch:   1 [    4080/ 1000000], loss: {'classification': 24.6412, 'neural_augmentation': 8.4235, 'total_loss': 33.0647}, LR: [0.000408, 0.000408], Avg. batch load time: 0.031, Elapsed time: 1049.80
2024-07-14 10:41:19 - [34m[1mLOGS   [0m - Epoch:   1 [    4580/ 1000000], loss: {'classification': 24.5243, 'neural_augmentation': 8.2459, 'total_loss': 32.7703}, LR: [0.000458, 0.000458], Avg. batch load time: 0.024, Elapsed time: 1383.84
2024-07-14 10:46:49 - [34m[1mLOGS   [0m - *** Training summary for epoch 1
	 loss={'classification': 24.4107, 'neural_augmentation': 8.0437, 'total_loss': 32.4545}
2024-07-14 10:46:50 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_best.pt
2024-07-14 10:46:50 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_last.pt
2024-07-14 10:46:50 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_last.pt
2024-07-14 10:46:51 - [34m[1mLOGS   [0m - Training checkpoint for epoch 1/iteration 5073 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_epoch_1_iter_5073.pt
2024-07-14 10:46:51 - [34m[1mLOGS   [0m - Model state for epoch 1/iteration 5073 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_epoch_1_iter_5073.pt
[31m===========================================================================[0m
2024-07-14 10:46:53 - [32m[1mINFO   [0m - Training epoch 2
2024-07-14 10:47:25 - [34m[1mLOGS   [0m - Epoch:   2 [    5074/ 1000000], loss: {'classification': 23.6134, 'neural_augmentation': 7.0344, 'total_loss': 30.6478}, LR: [0.000508, 0.000508], Avg. batch load time: 31.546, Elapsed time: 32.20
2024-07-14 10:53:14 - [34m[1mLOGS   [0m - Epoch:   2 [    5574/ 1000000], loss: {'classification': 23.7343, 'neural_augmentation': 6.6368, 'total_loss': 30.3711}, LR: [0.000558, 0.000558], Avg. batch load time: 0.093, Elapsed time: 381.16
2024-07-14 10:58:49 - [34m[1mLOGS   [0m - Epoch:   2 [    6074/ 1000000], loss: {'classification': 23.5883, 'neural_augmentation': 6.2903, 'total_loss': 29.8787}, LR: [0.000608, 0.000608], Avg. batch load time: 0.047, Elapsed time: 715.98
2024-07-14 11:04:24 - [34m[1mLOGS   [0m - Epoch:   2 [    6574/ 1000000], loss: {'classification': 23.4407, 'neural_augmentation': 5.9135, 'total_loss': 29.3543}, LR: [0.000658, 0.000658], Avg. batch load time: 0.032, Elapsed time: 1051.32
2024-07-14 11:09:59 - [34m[1mLOGS   [0m - Epoch:   2 [    7074/ 1000000], loss: {'classification': 23.3044, 'neural_augmentation': 5.5201, 'total_loss': 28.8246}, LR: [0.000708, 0.000708], Avg. batch load time: 0.024, Elapsed time: 1385.75
2024-07-14 11:15:35 - [34m[1mLOGS   [0m - Epoch:   2 [    7574/ 1000000], loss: {'classification': 23.168, 'neural_augmentation': 5.1484, 'total_loss': 28.3164}, LR: [0.000758, 0.000758], Avg. batch load time: 0.019, Elapsed time: 1722.07
2024-07-14 11:16:10 - [34m[1mLOGS   [0m - *** Training summary for epoch 2
	 loss={'classification': 23.1545, 'neural_augmentation': 5.1129, 'total_loss': 28.2674}
2024-07-14 11:16:10 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_best.pt
2024-07-14 11:16:11 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_last.pt
2024-07-14 11:16:11 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_last.pt
2024-07-14 11:16:12 - [34m[1mLOGS   [0m - Training checkpoint for epoch 2/iteration 7626 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_epoch_2_iter_7626.pt
2024-07-14 11:16:12 - [34m[1mLOGS   [0m - Model state for epoch 2/iteration 7626 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_epoch_2_iter_7626.pt
[31m===========================================================================[0m
2024-07-14 11:16:14 - [32m[1mINFO   [0m - Training epoch 3
2024-07-14 11:16:45 - [34m[1mLOGS   [0m - Epoch:   3 [    7627/ 1000000], loss: {'classification': 22.3273, 'neural_augmentation': 3.2828, 'total_loss': 25.6101}, LR: [0.000763, 0.000763], Avg. batch load time: 30.562, Elapsed time: 31.25
2024-07-14 11:22:38 - [34m[1mLOGS   [0m - Epoch:   3 [    8127/ 1000000], loss: {'classification': 22.3324, 'neural_augmentation': 2.92, 'total_loss': 25.2525}, LR: [0.000813, 0.000813], Avg. batch load time: 0.096, Elapsed time: 384.58
2024-07-14 11:28:13 - [34m[1mLOGS   [0m - Epoch:   3 [    8627/ 1000000], loss: {'classification': 22.2371, 'neural_augmentation': 2.6605, 'total_loss': 24.8976}, LR: [0.000863, 0.000863], Avg. batch load time: 0.049, Elapsed time: 718.93
2024-07-14 11:33:47 - [34m[1mLOGS   [0m - Epoch:   3 [    9127/ 1000000], loss: {'classification': 22.1306, 'neural_augmentation': 2.4256, 'total_loss': 24.5562}, LR: [0.000913, 0.000913], Avg. batch load time: 0.033, Elapsed time: 1053.53
2024-07-14 11:39:22 - [34m[1mLOGS   [0m - Epoch:   3 [    9627/ 1000000], loss: {'classification': 22.0371, 'neural_augmentation': 2.225, 'total_loss': 24.2621}, LR: [0.000963, 0.000963], Avg. batch load time: 0.025, Elapsed time: 1388.15
2024-07-14 11:44:57 - [34m[1mLOGS   [0m - Epoch:   3 [   10127/ 1000000], loss: {'classification': 21.9539, 'neural_augmentation': 2.0584, 'total_loss': 24.0123}, LR: [0.001, 0.001], Avg. batch load time: 0.020, Elapsed time: 1722.75
2024-07-14 11:45:47 - [34m[1mLOGS   [0m - *** Training summary for epoch 3
	 loss={'classification': 21.9417, 'neural_augmentation': 2.0375, 'total_loss': 23.9792}
2024-07-14 11:45:48 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_best.pt
2024-07-14 11:45:48 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_last.pt
2024-07-14 11:45:49 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_last.pt
2024-07-14 11:45:49 - [34m[1mLOGS   [0m - Training checkpoint for epoch 3/iteration 10203 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_epoch_3_iter_10203.pt
2024-07-14 11:45:49 - [34m[1mLOGS   [0m - Model state for epoch 3/iteration 10203 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_epoch_3_iter_10203.pt
[31m===========================================================================[0m
2024-07-14 11:45:51 - [32m[1mINFO   [0m - Training epoch 4
2024-07-14 11:46:29 - [34m[1mLOGS   [0m - Epoch:   4 [   10204/ 1000000], loss: {'classification': 21.6661, 'neural_augmentation': 1.2362, 'total_loss': 22.9023}, LR: [0.001, 0.001], Avg. batch load time: 36.758, Elapsed time: 37.45
2024-07-14 11:52:11 - [34m[1mLOGS   [0m - Epoch:   4 [   10704/ 1000000], loss: {'classification': 21.39, 'neural_augmentation': 1.1628, 'total_loss': 22.5528}, LR: [0.001, 0.001], Avg. batch load time: 0.089, Elapsed time: 379.48
2024-07-14 11:57:45 - [34m[1mLOGS   [0m - Epoch:   4 [   11204/ 1000000], loss: {'classification': 21.2824, 'neural_augmentation': 1.0875, 'total_loss': 22.3699}, LR: [0.001, 0.001], Avg. batch load time: 0.045, Elapsed time: 713.84
2024-07-14 12:03:20 - [34m[1mLOGS   [0m - Epoch:   4 [   11704/ 1000000], loss: {'classification': 21.1971, 'neural_augmentation': 1.0182, 'total_loss': 22.2152}, LR: [0.001, 0.001], Avg. batch load time: 0.030, Elapsed time: 1048.67
2024-07-14 12:08:55 - [34m[1mLOGS   [0m - Epoch:   4 [   12204/ 1000000], loss: {'classification': 21.1175, 'neural_augmentation': 0.9594, 'total_loss': 22.0769}, LR: [0.001, 0.001], Avg. batch load time: 0.023, Elapsed time: 1383.27
2024-07-14 12:14:29 - [34m[1mLOGS   [0m - Epoch:   4 [   12704/ 1000000], loss: {'classification': 21.0429, 'neural_augmentation': 0.9104, 'total_loss': 21.9534}, LR: [0.001, 0.001], Avg. batch load time: 0.019, Elapsed time: 1717.91
2024-07-14 12:15:20 - [34m[1mLOGS   [0m - *** Training summary for epoch 4
	 loss={'classification': 21.031, 'neural_augmentation': 0.9035, 'total_loss': 21.9345}
2024-07-14 12:15:20 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_best.pt
2024-07-14 12:15:21 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_last.pt
2024-07-14 12:15:21 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_last.pt
2024-07-14 12:15:22 - [34m[1mLOGS   [0m - Training checkpoint for epoch 4/iteration 12780 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/training_checkpoint_epoch_4_iter_12780.pt
2024-07-14 12:15:22 - [34m[1mLOGS   [0m - Model state for epoch 4/iteration 12780 is saved at: /ML-A100/team/mm/models/catlip_data/results500_1/train/checkpoint_epoch_4_iter_12780.pt
[31m===========================================================================[0m
2024-07-14 12:15:24 - [32m[1mINFO   [0m - Training epoch 5
2024-07-14 12:16:10 - [34m[1mLOGS   [0m - Epoch:   5 [   12781/ 1000000], loss: {'classification': 20.9421, 'neural_augmentation': 0.6688, 'total_loss': 21.6109}, LR: [0.001, 0.001], Avg. batch load time: 43.664, Elapsed time: 45.53
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 502, in Client
    c = SocketClient(address)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 630, in SocketClient
    s.connect(address)
ConnectionRefusedError: [Errno 111] Connection refused
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
Exception in thread Thread-1 (_pin_memory_loop):
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 53, in _pin_memory_loop
    do_one_step()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py", line 30, in do_one_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 495, in rebuild_storage_fd
    fd = df.detach()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
2024-07-14 12:19:55 - [34m[1mLOGS   [0m - Keyboard interruption. Exiting from early training
2024-07-14 12:19:55 - [34m[1mLOGS   [0m - Training took 02:35:19.35
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 12 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
