nohup: ignoring input
2024-07-25 08:57:56 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
2024-07-25 08:57:57 - [32m[1mINFO   [0m - Trainable parameters: ['cls_token', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_emb.0.block.conv.weight', 'patch_emb.0.block.norm.weight', 'patch_emb.0.block.norm.bias', 'patch_emb.1.block.conv.weight', 'patch_emb.1.block.norm.weight', 'patch_emb.1.block.norm.bias', 'patch_emb.2.block.conv.weight', 'patch_emb.2.block.conv.bias', 'post_transformer_norm.weight', 'post_transformer_norm.bias', 'transformer.0.pre_norm_mha.0.weight', 'transformer.0.pre_norm_mha.0.bias', 'transformer.0.pre_norm_mha.1.qkv_proj.weight', 'transformer.0.pre_norm_mha.1.qkv_proj.bias', 'transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'transformer.0.pre_norm_ffn.0.weight', 'transformer.0.pre_norm_ffn.0.bias', 'transformer.0.pre_norm_ffn.1.weight', 'transformer.0.pre_norm_ffn.1.bias', 'transformer.0.pre_norm_ffn.4.weight', 'transformer.0.pre_norm_ffn.4.bias', 'transformer.1.pre_norm_mha.0.weight', 'transformer.1.pre_norm_mha.0.bias', 'transformer.1.pre_norm_mha.1.qkv_proj.weight', 'transformer.1.pre_norm_mha.1.qkv_proj.bias', 'transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'transformer.1.pre_norm_ffn.0.weight', 'transformer.1.pre_norm_ffn.0.bias', 'transformer.1.pre_norm_ffn.1.weight', 'transformer.1.pre_norm_ffn.1.bias', 'transformer.1.pre_norm_ffn.4.weight', 'transformer.1.pre_norm_ffn.4.bias', 'transformer.2.pre_norm_mha.0.weight', 'transformer.2.pre_norm_mha.0.bias', 'transformer.2.pre_norm_mha.1.qkv_proj.weight', 'transformer.2.pre_norm_mha.1.qkv_proj.bias', 'transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'transformer.2.pre_norm_ffn.0.weight', 'transformer.2.pre_norm_ffn.0.bias', 'transformer.2.pre_norm_ffn.1.weight', 'transformer.2.pre_norm_ffn.1.bias', 'transformer.2.pre_norm_ffn.4.weight', 'transformer.2.pre_norm_ffn.4.bias', 'transformer.3.pre_norm_mha.0.weight', 'transformer.3.pre_norm_mha.0.bias', 'transformer.3.pre_norm_mha.1.qkv_proj.weight', 'transformer.3.pre_norm_mha.1.qkv_proj.bias', 'transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'transformer.3.pre_norm_ffn.0.weight', 'transformer.3.pre_norm_ffn.0.bias', 'transformer.3.pre_norm_ffn.1.weight', 'transformer.3.pre_norm_ffn.1.bias', 'transformer.3.pre_norm_ffn.4.weight', 'transformer.3.pre_norm_ffn.4.bias', 'transformer.4.pre_norm_mha.0.weight', 'transformer.4.pre_norm_mha.0.bias', 'transformer.4.pre_norm_mha.1.qkv_proj.weight', 'transformer.4.pre_norm_mha.1.qkv_proj.bias', 'transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'transformer.4.pre_norm_ffn.0.weight', 'transformer.4.pre_norm_ffn.0.bias', 'transformer.4.pre_norm_ffn.1.weight', 'transformer.4.pre_norm_ffn.1.bias', 'transformer.4.pre_norm_ffn.4.weight', 'transformer.4.pre_norm_ffn.4.bias', 'transformer.5.pre_norm_mha.0.weight', 'transformer.5.pre_norm_mha.0.bias', 'transformer.5.pre_norm_mha.1.qkv_proj.weight', 'transformer.5.pre_norm_mha.1.qkv_proj.bias', 'transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'transformer.5.pre_norm_ffn.0.weight', 'transformer.5.pre_norm_ffn.0.bias', 'transformer.5.pre_norm_ffn.1.weight', 'transformer.5.pre_norm_ffn.1.bias', 'transformer.5.pre_norm_ffn.4.weight', 'transformer.5.pre_norm_ffn.4.bias', 'transformer.6.pre_norm_mha.0.weight', 'transformer.6.pre_norm_mha.0.bias', 'transformer.6.pre_norm_mha.1.qkv_proj.weight', 'transformer.6.pre_norm_mha.1.qkv_proj.bias', 'transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'transformer.6.pre_norm_ffn.0.weight', 'transformer.6.pre_norm_ffn.0.bias', 'transformer.6.pre_norm_ffn.1.weight', 'transformer.6.pre_norm_ffn.1.bias', 'transformer.6.pre_norm_ffn.4.weight', 'transformer.6.pre_norm_ffn.4.bias', 'transformer.7.pre_norm_mha.0.weight', 'transformer.7.pre_norm_mha.0.bias', 'transformer.7.pre_norm_mha.1.qkv_proj.weight', 'transformer.7.pre_norm_mha.1.qkv_proj.bias', 'transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'transformer.7.pre_norm_ffn.0.weight', 'transformer.7.pre_norm_ffn.0.bias', 'transformer.7.pre_norm_ffn.1.weight', 'transformer.7.pre_norm_ffn.1.bias', 'transformer.7.pre_norm_ffn.4.weight', 'transformer.7.pre_norm_ffn.4.bias', 'transformer.8.pre_norm_mha.0.weight', 'transformer.8.pre_norm_mha.0.bias', 'transformer.8.pre_norm_mha.1.qkv_proj.weight', 'transformer.8.pre_norm_mha.1.qkv_proj.bias', 'transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'transformer.8.pre_norm_ffn.0.weight', 'transformer.8.pre_norm_ffn.0.bias', 'transformer.8.pre_norm_ffn.1.weight', 'transformer.8.pre_norm_ffn.1.bias', 'transformer.8.pre_norm_ffn.4.weight', 'transformer.8.pre_norm_ffn.4.bias', 'transformer.9.pre_norm_mha.0.weight', 'transformer.9.pre_norm_mha.0.bias', 'transformer.9.pre_norm_mha.1.qkv_proj.weight', 'transformer.9.pre_norm_mha.1.qkv_proj.bias', 'transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'transformer.9.pre_norm_ffn.0.weight', 'transformer.9.pre_norm_ffn.0.bias', 'transformer.9.pre_norm_ffn.1.weight', 'transformer.9.pre_norm_ffn.1.bias', 'transformer.9.pre_norm_ffn.4.weight', 'transformer.9.pre_norm_ffn.4.bias', 'transformer.10.pre_norm_mha.0.weight', 'transformer.10.pre_norm_mha.0.bias', 'transformer.10.pre_norm_mha.1.qkv_proj.weight', 'transformer.10.pre_norm_mha.1.qkv_proj.bias', 'transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'transformer.10.pre_norm_ffn.0.weight', 'transformer.10.pre_norm_ffn.0.bias', 'transformer.10.pre_norm_ffn.1.weight', 'transformer.10.pre_norm_ffn.1.bias', 'transformer.10.pre_norm_ffn.4.weight', 'transformer.10.pre_norm_ffn.4.bias', 'transformer.11.pre_norm_mha.0.weight', 'transformer.11.pre_norm_mha.0.bias', 'transformer.11.pre_norm_mha.1.qkv_proj.weight', 'transformer.11.pre_norm_mha.1.qkv_proj.bias', 'transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'transformer.11.pre_norm_ffn.0.weight', 'transformer.11.pre_norm_ffn.0.bias', 'transformer.11.pre_norm_ffn.1.weight', 'transformer.11.pre_norm_ffn.1.bias', 'transformer.11.pre_norm_ffn.4.weight', 'transformer.11.pre_norm_ffn.4.bias', 'classifier.weight', 'classifier.bias', 'pos_embed.pos_embed.pos_embed']
2024-07-25 08:57:57 - [34m[1mLOGS   [0m - [36mModel[0m
VisionTransformer(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_emb): Sequential(
    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=GELU)
    (1): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2), bias=False, normalization=BatchNorm2d, activation=GELU)
    (2): Conv2d(96, 384, kernel_size=(2, 2), stride=(2, 2))
  )
  (post_transformer_norm): LayerNormFP32((384,), eps=1e-06, elementwise_affine=True)
  (transformer): Sequential(
    (0): FlashTransformerEncoder
    (1): FlashTransformerEncoder
    (2): FlashTransformerEncoder
    (3): FlashTransformerEncoder
    (4): FlashTransformerEncoder
    (5): FlashTransformerEncoder
    (6): FlashTransformerEncoder
    (7): FlashTransformerEncoder
    (8): FlashTransformerEncoder
    (9): FlashTransformerEncoder
    (10): FlashTransformerEncoder
    (11): FlashTransformerEncoder
  )
  (classifier): LinearLayer(in_features=384, out_features=7476, bias=True, channel_first=False)
  (pos_embed): LearnablePositionalEmbedding(num_embeddings=196, embedding_dim=384, padding_idx=None, sequence_first=False)
  (emb_dropout): Dropout(p=0.0, inplace=False)
)
[31m=================================================================[0m
                  VisionTransformer Summary
[31m=================================================================[0m
Total parameters     =   24.438 M
Total trainable parameters =   24.438 M

2024-07-25 08:57:57 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-25 08:57:57 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 24.438M                | 4.268G     |
|  cls_token                           |  (1, 1, 384)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_emb                           |  0.19M                 |  73.006M   |
|   patch_emb.0.block                  |   4.8K                 |   15.053M  |
|    patch_emb.0.block.conv            |    4.608K              |    14.451M |
|    patch_emb.0.block.norm            |    0.192K              |    0.602M  |
|   patch_emb.1.block                  |   37.056K              |   29.052M  |
|    patch_emb.1.block.conv            |    36.864K             |    28.901M |
|    patch_emb.1.block.norm            |    0.192K              |    0.151M  |
|   patch_emb.2.block.conv             |   0.148M               |   28.901M  |
|    patch_emb.2.block.conv.weight     |    (384, 96, 2, 2)     |            |
|    patch_emb.2.block.conv.bias       |    (384,)              |            |
|  post_transformer_norm               |  0.768K                |  0.378M    |
|   post_transformer_norm.weight       |   (384,)               |            |
|   post_transformer_norm.bias         |   (384,)               |            |
|  transformer                         |  21.294M               |  4.192G    |
|   transformer.0                      |   1.774M               |   0.349G   |
|    transformer.0.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.0.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.1                      |   1.774M               |   0.349G   |
|    transformer.1.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.1.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.2                      |   1.774M               |   0.349G   |
|    transformer.2.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.2.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.3                      |   1.774M               |   0.349G   |
|    transformer.3.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.3.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.4                      |   1.774M               |   0.349G   |
|    transformer.4.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.4.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.5                      |   1.774M               |   0.349G   |
|    transformer.5.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.5.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.6                      |   1.774M               |   0.349G   |
|    transformer.6.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.6.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.7                      |   1.774M               |   0.349G   |
|    transformer.7.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.7.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.8                      |   1.774M               |   0.349G   |
|    transformer.8.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.8.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.9                      |   1.774M               |   0.349G   |
|    transformer.9.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.9.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.10                     |   1.774M               |   0.349G   |
|    transformer.10.pre_norm_mha       |    0.592M              |    0.117G  |
|    transformer.10.pre_norm_ffn       |    1.182M              |    0.233G  |
|   transformer.11                     |   1.774M               |   0.349G   |
|    transformer.11.pre_norm_mha       |    0.592M              |    0.117G  |
|    transformer.11.pre_norm_ffn       |    1.182M              |    0.233G  |
|  classifier                          |  2.878M                |  2.871M    |
|   classifier.weight                  |   (7476, 384)          |            |
|   classifier.bias                    |   (7476,)              |            |
|  pos_embed.pos_embed                 |  75.264K               |  0         |
|   pos_embed.pos_embed.pos_embed      |   (1, 1, 196, 384)     |            |
2024-07-25 08:57:58 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-25 08:57:58 - [33m[1mWARNING[0m - Uncalled Modules:
{'neural_augmentor.noise', 'neural_augmentor.noise.min_fn', 'transformer.5.drop_path', 'neural_augmentor.contrast.min_fn', 'transformer.7.drop_path', 'neural_augmentor', 'transformer.11.drop_path', 'transformer.10.drop_path', 'neural_augmentor.contrast', 'transformer.8.drop_path', 'transformer.6.drop_path', 'neural_augmentor.brightness', 'transformer.3.drop_path', 'transformer.9.drop_path', 'transformer.0.drop_path', 'transformer.1.drop_path', 'neural_augmentor.brightness.min_fn', 'neural_augmentor.contrast.max_fn', 'transformer.2.drop_path', 'neural_augmentor.noise.max_fn', 'transformer.4.drop_path', 'neural_augmentor.brightness.max_fn'}
2024-07-25 08:57:58 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 25, 'aten::gelu': 14, 'aten::scaled_dot_product_attention': 12, 'aten::sub': 1})
[31m=================================================================[0m
2024-07-25 08:57:58 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-25 08:57:58 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-25 08:57:58 - [34m[1mLOGS   [0m - Available GPUs: 4
2024-07-25 08:57:58 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-25 08:57:58 - [34m[1mLOGS   [0m - Setting --ddp.world-size the same as the number of available gpus.
2024-07-25 08:57:58 - [34m[1mLOGS   [0m - Directory exists at: /ML-A100/team/mm/models/catlip_data/results_vit_small/train
2024-07-25 08:58:02 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:40011
2024-07-25 08:58:06 - [34m[1mLOGS   [0m - Training dataset details are given below
WordnetTaggedClassificationDataset(
	root= 
	is_training=True 
	num_samples=64290000
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	total_tar_files=6429
	max_files_per_tar=10000
	num_synsets=7476
)
2024-07-25 08:58:08 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=True
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=400
	 scales=[(128, 128, 1225), (144, 144, 967), (160, 160, 784), (176, 176, 647), (192, 192, 544), (208, 208, 463), (224, 224, 400), (240, 240, 348), (256, 256, 306), (272, 272, 271), (288, 288, 241), (304, 304, 217), (320, 320, 196)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-25 08:58:08 - [34m[1mLOGS   [0m - Number of data workers: 64
2024-07-25 08:58:09 - [32m[1mINFO   [0m - Trainable parameters: ['cls_token', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_emb.0.block.conv.weight', 'patch_emb.0.block.norm.weight', 'patch_emb.0.block.norm.bias', 'patch_emb.1.block.conv.weight', 'patch_emb.1.block.norm.weight', 'patch_emb.1.block.norm.bias', 'patch_emb.2.block.conv.weight', 'patch_emb.2.block.conv.bias', 'post_transformer_norm.weight', 'post_transformer_norm.bias', 'transformer.0.pre_norm_mha.0.weight', 'transformer.0.pre_norm_mha.0.bias', 'transformer.0.pre_norm_mha.1.qkv_proj.weight', 'transformer.0.pre_norm_mha.1.qkv_proj.bias', 'transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'transformer.0.pre_norm_ffn.0.weight', 'transformer.0.pre_norm_ffn.0.bias', 'transformer.0.pre_norm_ffn.1.weight', 'transformer.0.pre_norm_ffn.1.bias', 'transformer.0.pre_norm_ffn.4.weight', 'transformer.0.pre_norm_ffn.4.bias', 'transformer.1.pre_norm_mha.0.weight', 'transformer.1.pre_norm_mha.0.bias', 'transformer.1.pre_norm_mha.1.qkv_proj.weight', 'transformer.1.pre_norm_mha.1.qkv_proj.bias', 'transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'transformer.1.pre_norm_ffn.0.weight', 'transformer.1.pre_norm_ffn.0.bias', 'transformer.1.pre_norm_ffn.1.weight', 'transformer.1.pre_norm_ffn.1.bias', 'transformer.1.pre_norm_ffn.4.weight', 'transformer.1.pre_norm_ffn.4.bias', 'transformer.2.pre_norm_mha.0.weight', 'transformer.2.pre_norm_mha.0.bias', 'transformer.2.pre_norm_mha.1.qkv_proj.weight', 'transformer.2.pre_norm_mha.1.qkv_proj.bias', 'transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'transformer.2.pre_norm_ffn.0.weight', 'transformer.2.pre_norm_ffn.0.bias', 'transformer.2.pre_norm_ffn.1.weight', 'transformer.2.pre_norm_ffn.1.bias', 'transformer.2.pre_norm_ffn.4.weight', 'transformer.2.pre_norm_ffn.4.bias', 'transformer.3.pre_norm_mha.0.weight', 'transformer.3.pre_norm_mha.0.bias', 'transformer.3.pre_norm_mha.1.qkv_proj.weight', 'transformer.3.pre_norm_mha.1.qkv_proj.bias', 'transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'transformer.3.pre_norm_ffn.0.weight', 'transformer.3.pre_norm_ffn.0.bias', 'transformer.3.pre_norm_ffn.1.weight', 'transformer.3.pre_norm_ffn.1.bias', 'transformer.3.pre_norm_ffn.4.weight', 'transformer.3.pre_norm_ffn.4.bias', 'transformer.4.pre_norm_mha.0.weight', 'transformer.4.pre_norm_mha.0.bias', 'transformer.4.pre_norm_mha.1.qkv_proj.weight', 'transformer.4.pre_norm_mha.1.qkv_proj.bias', 'transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'transformer.4.pre_norm_ffn.0.weight', 'transformer.4.pre_norm_ffn.0.bias', 'transformer.4.pre_norm_ffn.1.weight', 'transformer.4.pre_norm_ffn.1.bias', 'transformer.4.pre_norm_ffn.4.weight', 'transformer.4.pre_norm_ffn.4.bias', 'transformer.5.pre_norm_mha.0.weight', 'transformer.5.pre_norm_mha.0.bias', 'transformer.5.pre_norm_mha.1.qkv_proj.weight', 'transformer.5.pre_norm_mha.1.qkv_proj.bias', 'transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'transformer.5.pre_norm_ffn.0.weight', 'transformer.5.pre_norm_ffn.0.bias', 'transformer.5.pre_norm_ffn.1.weight', 'transformer.5.pre_norm_ffn.1.bias', 'transformer.5.pre_norm_ffn.4.weight', 'transformer.5.pre_norm_ffn.4.bias', 'transformer.6.pre_norm_mha.0.weight', 'transformer.6.pre_norm_mha.0.bias', 'transformer.6.pre_norm_mha.1.qkv_proj.weight', 'transformer.6.pre_norm_mha.1.qkv_proj.bias', 'transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'transformer.6.pre_norm_ffn.0.weight', 'transformer.6.pre_norm_ffn.0.bias', 'transformer.6.pre_norm_ffn.1.weight', 'transformer.6.pre_norm_ffn.1.bias', 'transformer.6.pre_norm_ffn.4.weight', 'transformer.6.pre_norm_ffn.4.bias', 'transformer.7.pre_norm_mha.0.weight', 'transformer.7.pre_norm_mha.0.bias', 'transformer.7.pre_norm_mha.1.qkv_proj.weight', 'transformer.7.pre_norm_mha.1.qkv_proj.bias', 'transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'transformer.7.pre_norm_ffn.0.weight', 'transformer.7.pre_norm_ffn.0.bias', 'transformer.7.pre_norm_ffn.1.weight', 'transformer.7.pre_norm_ffn.1.bias', 'transformer.7.pre_norm_ffn.4.weight', 'transformer.7.pre_norm_ffn.4.bias', 'transformer.8.pre_norm_mha.0.weight', 'transformer.8.pre_norm_mha.0.bias', 'transformer.8.pre_norm_mha.1.qkv_proj.weight', 'transformer.8.pre_norm_mha.1.qkv_proj.bias', 'transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'transformer.8.pre_norm_ffn.0.weight', 'transformer.8.pre_norm_ffn.0.bias', 'transformer.8.pre_norm_ffn.1.weight', 'transformer.8.pre_norm_ffn.1.bias', 'transformer.8.pre_norm_ffn.4.weight', 'transformer.8.pre_norm_ffn.4.bias', 'transformer.9.pre_norm_mha.0.weight', 'transformer.9.pre_norm_mha.0.bias', 'transformer.9.pre_norm_mha.1.qkv_proj.weight', 'transformer.9.pre_norm_mha.1.qkv_proj.bias', 'transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'transformer.9.pre_norm_ffn.0.weight', 'transformer.9.pre_norm_ffn.0.bias', 'transformer.9.pre_norm_ffn.1.weight', 'transformer.9.pre_norm_ffn.1.bias', 'transformer.9.pre_norm_ffn.4.weight', 'transformer.9.pre_norm_ffn.4.bias', 'transformer.10.pre_norm_mha.0.weight', 'transformer.10.pre_norm_mha.0.bias', 'transformer.10.pre_norm_mha.1.qkv_proj.weight', 'transformer.10.pre_norm_mha.1.qkv_proj.bias', 'transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'transformer.10.pre_norm_ffn.0.weight', 'transformer.10.pre_norm_ffn.0.bias', 'transformer.10.pre_norm_ffn.1.weight', 'transformer.10.pre_norm_ffn.1.bias', 'transformer.10.pre_norm_ffn.4.weight', 'transformer.10.pre_norm_ffn.4.bias', 'transformer.11.pre_norm_mha.0.weight', 'transformer.11.pre_norm_mha.0.bias', 'transformer.11.pre_norm_mha.1.qkv_proj.weight', 'transformer.11.pre_norm_mha.1.qkv_proj.bias', 'transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'transformer.11.pre_norm_ffn.0.weight', 'transformer.11.pre_norm_ffn.0.bias', 'transformer.11.pre_norm_ffn.1.weight', 'transformer.11.pre_norm_ffn.1.bias', 'transformer.11.pre_norm_ffn.4.weight', 'transformer.11.pre_norm_ffn.4.bias', 'classifier.weight', 'classifier.bias', 'pos_embed.pos_embed.pos_embed']
2024-07-25 08:58:09 - [34m[1mLOGS   [0m - [36mModel[0m
VisionTransformer(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_emb): Sequential(
    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=GELU)
    (1): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2), bias=False, normalization=BatchNorm2d, activation=GELU)
    (2): Conv2d(96, 384, kernel_size=(2, 2), stride=(2, 2))
  )
  (post_transformer_norm): LayerNormFP32((384,), eps=1e-06, elementwise_affine=True)
  (transformer): Sequential(
    (0): FlashTransformerEncoder
    (1): FlashTransformerEncoder
    (2): FlashTransformerEncoder
    (3): FlashTransformerEncoder
    (4): FlashTransformerEncoder
    (5): FlashTransformerEncoder
    (6): FlashTransformerEncoder
    (7): FlashTransformerEncoder
    (8): FlashTransformerEncoder
    (9): FlashTransformerEncoder
    (10): FlashTransformerEncoder
    (11): FlashTransformerEncoder
  )
  (classifier): LinearLayer(in_features=384, out_features=7476, bias=True, channel_first=False)
  (pos_embed): LearnablePositionalEmbedding(num_embeddings=196, embedding_dim=384, padding_idx=None, sequence_first=False)
  (emb_dropout): Dropout(p=0.0, inplace=False)
)
[31m=================================================================[0m
                  VisionTransformer Summary
[31m=================================================================[0m
Total parameters     =   24.438 M
Total trainable parameters =   24.438 M

2024-07-25 08:58:09 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-25 08:58:09 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 24.438M                | 4.268G     |
|  cls_token                           |  (1, 1, 384)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_emb                           |  0.19M                 |  73.006M   |
|   patch_emb.0.block                  |   4.8K                 |   15.053M  |
|    patch_emb.0.block.conv            |    4.608K              |    14.451M |
|    patch_emb.0.block.norm            |    0.192K              |    0.602M  |
|   patch_emb.1.block                  |   37.056K              |   29.052M  |
|    patch_emb.1.block.conv            |    36.864K             |    28.901M |
|    patch_emb.1.block.norm            |    0.192K              |    0.151M  |
|   patch_emb.2.block.conv             |   0.148M               |   28.901M  |
|    patch_emb.2.block.conv.weight     |    (384, 96, 2, 2)     |            |
|    patch_emb.2.block.conv.bias       |    (384,)              |            |
|  post_transformer_norm               |  0.768K                |  0.378M    |
|   post_transformer_norm.weight       |   (384,)               |            |
|   post_transformer_norm.bias         |   (384,)               |            |
|  transformer                         |  21.294M               |  4.192G    |
|   transformer.0                      |   1.774M               |   0.349G   |
|    transformer.0.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.0.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.1                      |   1.774M               |   0.349G   |
|    transformer.1.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.1.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.2                      |   1.774M               |   0.349G   |
|    transformer.2.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.2.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.3                      |   1.774M               |   0.349G   |
|    transformer.3.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.3.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.4                      |   1.774M               |   0.349G   |
|    transformer.4.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.4.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.5                      |   1.774M               |   0.349G   |
|    transformer.5.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.5.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.6                      |   1.774M               |   0.349G   |
|    transformer.6.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.6.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.7                      |   1.774M               |   0.349G   |
|    transformer.7.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.7.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.8                      |   1.774M               |   0.349G   |
|    transformer.8.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.8.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.9                      |   1.774M               |   0.349G   |
|    transformer.9.pre_norm_mha        |    0.592M              |    0.117G  |
|    transformer.9.pre_norm_ffn        |    1.182M              |    0.233G  |
|   transformer.10                     |   1.774M               |   0.349G   |
|    transformer.10.pre_norm_mha       |    0.592M              |    0.117G  |
|    transformer.10.pre_norm_ffn       |    1.182M              |    0.233G  |
|   transformer.11                     |   1.774M               |   0.349G   |
|    transformer.11.pre_norm_mha       |    0.592M              |    0.117G  |
|    transformer.11.pre_norm_ffn       |    1.182M              |    0.233G  |
|  classifier                          |  2.878M                |  2.871M    |
|   classifier.weight                  |   (7476, 384)          |            |
|   classifier.bias                    |   (7476,)              |            |
|  pos_embed.pos_embed                 |  75.264K               |  0         |
|   pos_embed.pos_embed.pos_embed      |   (1, 1, 196, 384)     |            |
2024-07-25 08:58:10 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-25 08:58:10 - [33m[1mWARNING[0m - Uncalled Modules:
{'neural_augmentor.contrast.min_fn', 'neural_augmentor.noise.min_fn', 'neural_augmentor.contrast.max_fn', 'transformer.7.drop_path', 'neural_augmentor.contrast', 'transformer.2.drop_path', 'neural_augmentor.brightness', 'transformer.1.drop_path', 'neural_augmentor', 'transformer.11.drop_path', 'transformer.10.drop_path', 'transformer.4.drop_path', 'neural_augmentor.brightness.min_fn', 'transformer.9.drop_path', 'neural_augmentor.noise', 'transformer.6.drop_path', 'transformer.0.drop_path', 'transformer.5.drop_path', 'neural_augmentor.brightness.max_fn', 'transformer.3.drop_path', 'transformer.8.drop_path', 'neural_augmentor.noise.max_fn'}
2024-07-25 08:58:10 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 25, 'aten::gelu': 14, 'aten::scaled_dot_product_attention': 12, 'aten::sub': 1})
[31m=================================================================[0m
2024-07-25 08:58:10 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-25 08:58:10 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-25 08:58:11 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-07-25 08:58:11 - [34m[1mLOGS   [0m - Max. iteration for training: 200000
2024-07-25 08:58:11 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=1e-05
 	 max_lr=0.001
 	 period=180001
 	 warmup_init_lr=1e-06
 	 warmup_iters=20000
 )
2024-07-25 08:58:11 - [34m[1mLOGS   [0m - No checkpoint found at '/ML-A100/team/mm/models/catlip_data/results_vit_small/train/training_checkpoint_last.pt'
2024-07-25 08:58:11 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results_vit_small/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-25 08:58:13 - [32m[1mINFO   [0m - Training epoch 0
2024-07-25 08:58:03 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:40011
2024-07-25 08:58:03 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:40011
2024-07-25 08:58:03 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:40011
2024-07-25 09:02:27 - [34m[1mLOGS   [0m - Epoch:   0 [       0/  200000], loss: {'classification': 5279.17, 'neural_augmentation': 8.6152, 'total_loss': 5287.7844}, LR: [1e-06, 1e-06], Avg. batch load time: 246.878, Elapsed time: 254.39
2024-07-25 09:05:17 - [34m[1mLOGS   [0m - Epoch:   0 [     125/  200000], loss: {'classification': 4893.1411, 'neural_augmentation': 9.2818, 'total_loss': 4902.423}, LR: [7e-06, 7e-06], Avg. batch load time: 0.514, Elapsed time: 424.91
2024-07-25 09:07:46 - [34m[1mLOGS   [0m - Epoch:   0 [     250/  200000], loss: {'classification': 4258.2946, 'neural_augmentation': 9.3287, 'total_loss': 4267.6233}, LR: [1.3e-05, 1.3e-05], Avg. batch load time: 0.258, Elapsed time: 573.28
2024-07-25 09:10:16 - [34m[1mLOGS   [0m - Epoch:   0 [     375/  200000], loss: {'classification': 3524.621, 'neural_augmentation': 9.3458, 'total_loss': 3533.9668}, LR: [2e-05, 2e-05], Avg. batch load time: 0.172, Elapsed time: 723.90
2024-07-25 09:12:46 - [34m[1mLOGS   [0m - Epoch:   0 [     500/  200000], loss: {'classification': 2862.2488, 'neural_augmentation': 9.3531, 'total_loss': 2871.6019}, LR: [2.6e-05, 2.6e-05], Avg. batch load time: 0.129, Elapsed time: 873.34
2024-07-25 09:15:14 - [34m[1mLOGS   [0m - Epoch:   0 [     625/  200000], loss: {'classification': 2362.166, 'neural_augmentation': 9.3488, 'total_loss': 2371.5148}, LR: [3.2e-05, 3.2e-05], Avg. batch load time: 0.104, Elapsed time: 1021.39
2024-07-25 09:17:48 - [34m[1mLOGS   [0m - Epoch:   0 [     750/  200000], loss: {'classification': 1988.2864, 'neural_augmentation': 9.3452, 'total_loss': 1997.6316}, LR: [3.8e-05, 3.8e-05], Avg. batch load time: 0.087, Elapsed time: 1175.03
2024-07-25 09:20:17 - [34m[1mLOGS   [0m - Epoch:   0 [     875/  200000], loss: {'classification': 1710.9888, 'neural_augmentation': 9.3367, 'total_loss': 1720.3255}, LR: [4.5e-05, 4.5e-05], Avg. batch load time: 0.074, Elapsed time: 1324.34
2024-07-25 09:22:51 - [34m[1mLOGS   [0m - Epoch:   0 [    1000/  200000], loss: {'classification': 1504.1192, 'neural_augmentation': 9.3226, 'total_loss': 1513.4418}, LR: [5.1e-05, 5.1e-05], Avg. batch load time: 0.065, Elapsed time: 1478.09
2024-07-25 09:25:24 - [34m[1mLOGS   [0m - Epoch:   0 [    1125/  200000], loss: {'classification': 1336.3678, 'neural_augmentation': 9.3074, 'total_loss': 1345.6751}, LR: [5.7e-05, 5.7e-05], Avg. batch load time: 0.058, Elapsed time: 1631.69
2024-07-25 09:27:57 - [34m[1mLOGS   [0m - Epoch:   0 [    1250/  200000], loss: {'classification': 1205.0369, 'neural_augmentation': 9.2996, 'total_loss': 1214.3365}, LR: [6.3e-05, 6.3e-05], Avg. batch load time: 0.052, Elapsed time: 1783.99
2024-07-25 09:30:28 - [34m[1mLOGS   [0m - Epoch:   0 [    1375/  200000], loss: {'classification': 1094.2485, 'neural_augmentation': 9.2893, 'total_loss': 1103.5378}, LR: [7e-05, 7e-05], Avg. batch load time: 0.048, Elapsed time: 1935.96
2024-07-25 09:33:03 - [34m[1mLOGS   [0m - Epoch:   0 [    1500/  200000], loss: {'classification': 1004.1094, 'neural_augmentation': 9.2802, 'total_loss': 1013.3896}, LR: [7.6e-05, 7.6e-05], Avg. batch load time: 0.044, Elapsed time: 2090.13
2024-07-25 09:35:32 - [34m[1mLOGS   [0m - Epoch:   0 [    1625/  200000], loss: {'classification': 926.7611, 'neural_augmentation': 9.2754, 'total_loss': 936.0365}, LR: [8.2e-05, 8.2e-05], Avg. batch load time: 0.040, Elapsed time: 2239.77
2024-07-25 09:38:03 - [34m[1mLOGS   [0m - Epoch:   0 [    1750/  200000], loss: {'classification': 861.6377, 'neural_augmentation': 9.2626, 'total_loss': 870.9003}, LR: [8.8e-05, 8.8e-05], Avg. batch load time: 0.038, Elapsed time: 2390.74
2024-07-25 09:40:38 - [34m[1mLOGS   [0m - Epoch:   0 [    1875/  200000], loss: {'classification': 803.5319, 'neural_augmentation': 9.2487, 'total_loss': 812.7806}, LR: [9.5e-05, 9.5e-05], Avg. batch load time: 0.035, Elapsed time: 2545.98
2024-07-25 09:43:11 - [34m[1mLOGS   [0m - Epoch:   0 [    2000/  200000], loss: {'classification': 754.8497, 'neural_augmentation': 9.24, 'total_loss': 764.0898}, LR: [0.000101, 0.000101], Avg. batch load time: 0.033, Elapsed time: 2698.11
2024-07-25 09:45:44 - [34m[1mLOGS   [0m - Epoch:   0 [    2125/  200000], loss: {'classification': 712.674, 'neural_augmentation': 9.2278, 'total_loss': 721.9018}, LR: [0.000107, 0.000107], Avg. batch load time: 0.031, Elapsed time: 2851.94
2024-07-25 09:48:15 - [34m[1mLOGS   [0m - Epoch:   0 [    2250/  200000], loss: {'classification': 674.5549, 'neural_augmentation': 9.2176, 'total_loss': 683.7724}, LR: [0.000113, 0.000113], Avg. batch load time: 0.029, Elapsed time: 3002.90
2024-07-25 09:50:49 - [34m[1mLOGS   [0m - Epoch:   0 [    2375/  200000], loss: {'classification': 639.2415, 'neural_augmentation': 9.2018, 'total_loss': 648.4433}, LR: [0.00012, 0.00012], Avg. batch load time: 0.028, Elapsed time: 3156.92
2024-07-25 09:53:17 - [34m[1mLOGS   [0m - Epoch:   0 [    2500/  200000], loss: {'classification': 608.439, 'neural_augmentation': 9.1857, 'total_loss': 617.6247}, LR: [0.000126, 0.000126], Avg. batch load time: 0.026, Elapsed time: 3304.82
2024-07-25 09:55:52 - [34m[1mLOGS   [0m - Epoch:   0 [    2625/  200000], loss: {'classification': 581.4327, 'neural_augmentation': 9.1811, 'total_loss': 590.6138}, LR: [0.000132, 0.000132], Avg. batch load time: 0.025, Elapsed time: 3459.96
2024-07-25 09:58:23 - [34m[1mLOGS   [0m - Epoch:   0 [    2750/  200000], loss: {'classification': 556.7724, 'neural_augmentation': 9.1731, 'total_loss': 565.9455}, LR: [0.000138, 0.000138], Avg. batch load time: 0.024, Elapsed time: 3610.89
2024-07-25 10:00:57 - [34m[1mLOGS   [0m - Epoch:   0 [    2875/  200000], loss: {'classification': 533.5312, 'neural_augmentation': 9.1654, 'total_loss': 542.6966}, LR: [0.000145, 0.000145], Avg. batch load time: 0.023, Elapsed time: 3764.80
2024-07-25 10:03:30 - [34m[1mLOGS   [0m - Epoch:   0 [    3000/  200000], loss: {'classification': 512.5245, 'neural_augmentation': 9.1586, 'total_loss': 521.683}, LR: [0.000151, 0.000151], Avg. batch load time: 0.022, Elapsed time: 3917.43
2024-07-25 10:06:04 - [34m[1mLOGS   [0m - Epoch:   0 [    3125/  200000], loss: {'classification': 494.1883, 'neural_augmentation': 9.1529, 'total_loss': 503.3411}, LR: [0.000157, 0.000157], Avg. batch load time: 0.021, Elapsed time: 4071.29
2024-07-25 10:08:35 - [34m[1mLOGS   [0m - Epoch:   0 [    3250/  200000], loss: {'classification': 476.0726, 'neural_augmentation': 9.1449, 'total_loss': 485.2175}, LR: [0.000163, 0.000163], Avg. batch load time: 0.021, Elapsed time: 4222.20
2024-07-25 10:11:04 - [34m[1mLOGS   [0m - Epoch:   0 [    3375/  200000], loss: {'classification': 459.407, 'neural_augmentation': 9.1357, 'total_loss': 468.5427}, LR: [0.00017, 0.00017], Avg. batch load time: 0.020, Elapsed time: 4371.88
2024-07-25 10:13:35 - [34m[1mLOGS   [0m - Epoch:   0 [    3500/  200000], loss: {'classification': 444.0101, 'neural_augmentation': 9.1269, 'total_loss': 453.137}, LR: [0.000176, 0.000176], Avg. batch load time: 0.019, Elapsed time: 4522.69
2024-07-25 10:18:42 - [34m[1mLOGS   [0m - Epoch:   0 [    3625/  200000], loss: {'classification': 430.3888, 'neural_augmentation': 9.1138, 'total_loss': 439.5025}, LR: [0.000182, 0.000182], Avg. batch load time: 0.027, Elapsed time: 4829.21
2024-07-25 10:25:56 - [34m[1mLOGS   [0m - Epoch:   0 [    3750/  200000], loss: {'classification': 416.4787, 'neural_augmentation': 9.0978, 'total_loss': 425.5765}, LR: [0.000188, 0.000188], Avg. batch load time: 0.041, Elapsed time: 5263.67
2024-07-25 10:33:03 - [34m[1mLOGS   [0m - Epoch:   0 [    3875/  200000], loss: {'classification': 404.1332, 'neural_augmentation': 9.0811, 'total_loss': 413.2142}, LR: [0.000195, 0.000195], Avg. batch load time: 0.053, Elapsed time: 5690.20
2024-07-25 10:40:24 - [34m[1mLOGS   [0m - Epoch:   0 [    4000/  200000], loss: {'classification': 391.9138, 'neural_augmentation': 9.064, 'total_loss': 400.9778}, LR: [0.000201, 0.000201], Avg. batch load time: 0.065, Elapsed time: 6131.65
2024-07-25 10:48:19 - [34m[1mLOGS   [0m - Epoch:   0 [    4125/  200000], loss: {'classification': 380.2433, 'neural_augmentation': 9.0469, 'total_loss': 389.2902}, LR: [0.000207, 0.000207], Avg. batch load time: 0.077, Elapsed time: 6606.22
2024-07-25 10:55:03 - [34m[1mLOGS   [0m - Epoch:   0 [    4250/  200000], loss: {'classification': 369.9354, 'neural_augmentation': 9.03, 'total_loss': 378.9654}, LR: [0.000213, 0.000213], Avg. batch load time: 0.087, Elapsed time: 7010.32
2024-07-25 11:02:03 - [34m[1mLOGS   [0m - Epoch:   0 [    4375/  200000], loss: {'classification': 360.203, 'neural_augmentation': 9.0155, 'total_loss': 369.2185}, LR: [0.00022, 0.00022], Avg. batch load time: 0.096, Elapsed time: 7430.42
2024-07-25 11:09:09 - [34m[1mLOGS   [0m - Epoch:   0 [    4500/  200000], loss: {'classification': 351.144, 'neural_augmentation': 9.001, 'total_loss': 360.145}, LR: [0.000226, 0.000226], Avg. batch load time: 0.105, Elapsed time: 7856.63
2024-07-25 11:16:24 - [34m[1mLOGS   [0m - Epoch:   0 [    4625/  200000], loss: {'classification': 342.6015, 'neural_augmentation': 8.9837, 'total_loss': 351.5852}, LR: [0.000232, 0.000232], Avg. batch load time: 0.115, Elapsed time: 8291.35
2024-07-25 11:23:22 - [34m[1mLOGS   [0m - Epoch:   0 [    4750/  200000], loss: {'classification': 334.297, 'neural_augmentation': 8.9667, 'total_loss': 343.2637}, LR: [0.000238, 0.000238], Avg. batch load time: 0.124, Elapsed time: 8709.49
2024-07-25 11:30:32 - [34m[1mLOGS   [0m - Epoch:   0 [    4875/  200000], loss: {'classification': 326.3028, 'neural_augmentation': 8.9493, 'total_loss': 335.2521}, LR: [0.000245, 0.000245], Avg. batch load time: 0.130, Elapsed time: 9139.92
2024-07-25 11:37:27 - [34m[1mLOGS   [0m - Epoch:   0 [    5000/  200000], loss: {'classification': 319.335, 'neural_augmentation': 8.9309, 'total_loss': 328.2659}, LR: [0.000251, 0.000251], Avg. batch load time: 0.138, Elapsed time: 9554.03
2024-07-25 11:44:45 - [34m[1mLOGS   [0m - Epoch:   0 [    5125/  200000], loss: {'classification': 311.9214, 'neural_augmentation': 8.9122, 'total_loss': 320.8336}, LR: [0.000257, 0.000257], Avg. batch load time: 0.145, Elapsed time: 9992.04
2024-07-25 11:52:41 - [34m[1mLOGS   [0m - Epoch:   0 [    5250/  200000], loss: {'classification': 304.9946, 'neural_augmentation': 8.8927, 'total_loss': 313.8873}, LR: [0.000263, 0.000263], Avg. batch load time: 0.154, Elapsed time: 10468.95
2024-07-25 11:59:47 - [34m[1mLOGS   [0m - Epoch:   0 [    5375/  200000], loss: {'classification': 298.6178, 'neural_augmentation': 8.874, 'total_loss': 307.4918}, LR: [0.000269, 0.000269], Avg. batch load time: 0.157, Elapsed time: 10894.49
2024-07-25 12:07:08 - [34m[1mLOGS   [0m - Epoch:   0 [    5500/  200000], loss: {'classification': 292.4657, 'neural_augmentation': 8.8558, 'total_loss': 301.3215}, LR: [0.000276, 0.000276], Avg. batch load time: 0.160, Elapsed time: 11335.42
2024-07-25 12:14:32 - [34m[1mLOGS   [0m - Epoch:   0 [    5625/  200000], loss: {'classification': 286.5412, 'neural_augmentation': 8.8367, 'total_loss': 295.3778}, LR: [0.000282, 0.000282], Avg. batch load time: 0.163, Elapsed time: 11779.63
2024-07-25 12:21:49 - [34m[1mLOGS   [0m - Epoch:   0 [    5750/  200000], loss: {'classification': 280.9243, 'neural_augmentation': 8.8164, 'total_loss': 289.7407}, LR: [0.000288, 0.000288], Avg. batch load time: 0.169, Elapsed time: 12216.85
2024-07-25 12:29:03 - [34m[1mLOGS   [0m - Epoch:   0 [    5875/  200000], loss: {'classification': 275.62, 'neural_augmentation': 8.7941, 'total_loss': 284.4141}, LR: [0.000294, 0.000294], Avg. batch load time: 0.172, Elapsed time: 12650.89
2024-07-25 12:36:09 - [34m[1mLOGS   [0m - Epoch:   0 [    6000/  200000], loss: {'classification': 270.5192, 'neural_augmentation': 8.7722, 'total_loss': 279.2914}, LR: [0.000301, 0.000301], Avg. batch load time: 0.174, Elapsed time: 13076.18
2024-07-25 12:43:22 - [34m[1mLOGS   [0m - Epoch:   0 [    6125/  200000], loss: {'classification': 265.578, 'neural_augmentation': 8.7495, 'total_loss': 274.3275}, LR: [0.000307, 0.000307], Avg. batch load time: 0.177, Elapsed time: 13509.20
2024-07-25 12:50:01 - [34m[1mLOGS   [0m - Epoch:   0 [    6250/  200000], loss: {'classification': 261.6465, 'neural_augmentation': 8.7295, 'total_loss': 270.376}, LR: [0.000313, 0.000313], Avg. batch load time: 0.181, Elapsed time: 13908.82
2024-07-25 12:57:36 - [34m[1mLOGS   [0m - Epoch:   0 [    6375/  200000], loss: {'classification': 258.5442, 'neural_augmentation': 8.7128, 'total_loss': 267.257}, LR: [0.000319, 0.000319], Avg. batch load time: 0.188, Elapsed time: 14363.22
2024-07-25 13:04:34 - [34m[1mLOGS   [0m - Epoch:   0 [    6500/  200000], loss: {'classification': 255.4778, 'neural_augmentation': 8.6962, 'total_loss': 264.1741}, LR: [0.000326, 0.000326], Avg. batch load time: 0.192, Elapsed time: 14781.08
2024-07-25 13:12:04 - [34m[1mLOGS   [0m - Epoch:   0 [    6625/  200000], loss: {'classification': 252.4559, 'neural_augmentation': 8.6788, 'total_loss': 261.1348}, LR: [0.000332, 0.000332], Avg. batch load time: 0.198, Elapsed time: 15231.72
2024-07-25 13:19:17 - [34m[1mLOGS   [0m - Epoch:   0 [    6750/  200000], loss: {'classification': 249.6526, 'neural_augmentation': 8.66, 'total_loss': 258.3126}, LR: [0.000338, 0.000338], Avg. batch load time: 0.202, Elapsed time: 15664.73
2024-07-25 13:26:42 - [34m[1mLOGS   [0m - Epoch:   0 [    6875/  200000], loss: {'classification': 246.9607, 'neural_augmentation': 8.6425, 'total_loss': 255.6032}, LR: [0.000344, 0.000344], Avg. batch load time: 0.206, Elapsed time: 16109.77
2024-07-25 13:34:34 - [34m[1mLOGS   [0m - Epoch:   0 [    7000/  200000], loss: {'classification': 244.2054, 'neural_augmentation': 8.6227, 'total_loss': 252.8282}, LR: [0.000351, 0.000351], Avg. batch load time: 0.211, Elapsed time: 16581.67
2024-07-25 13:41:51 - [34m[1mLOGS   [0m - Epoch:   0 [    7125/  200000], loss: {'classification': 241.7055, 'neural_augmentation': 8.6025, 'total_loss': 250.3081}, LR: [0.000357, 0.000357], Avg. batch load time: 0.215, Elapsed time: 17018.71
2024-07-25 13:49:26 - [34m[1mLOGS   [0m - Epoch:   0 [    7250/  200000], loss: {'classification': 239.2087, 'neural_augmentation': 8.5825, 'total_loss': 247.7912}, LR: [0.000363, 0.000363], Avg. batch load time: 0.219, Elapsed time: 17473.22
2024-07-25 13:56:39 - [34m[1mLOGS   [0m - Epoch:   0 [    7375/  200000], loss: {'classification': 236.8484, 'neural_augmentation': 8.5615, 'total_loss': 245.4098}, LR: [0.000369, 0.000369], Avg. batch load time: 0.223, Elapsed time: 17906.56
2024-07-25 14:04:17 - [34m[1mLOGS   [0m - Epoch:   0 [    7500/  200000], loss: {'classification': 234.4412, 'neural_augmentation': 8.5377, 'total_loss': 242.979}, LR: [0.000376, 0.000376], Avg. batch load time: 0.227, Elapsed time: 18364.35
2024-07-25 14:12:05 - [34m[1mLOGS   [0m - Epoch:   0 [    7625/  200000], loss: {'classification': 232.2546, 'neural_augmentation': 8.5146, 'total_loss': 240.7691}, LR: [0.000382, 0.000382], Avg. batch load time: 0.232, Elapsed time: 18832.72
2024-07-25 14:19:40 - [34m[1mLOGS   [0m - Epoch:   0 [    7750/  200000], loss: {'classification': 230.0509, 'neural_augmentation': 8.4896, 'total_loss': 238.5405}, LR: [0.000388, 0.000388], Avg. batch load time: 0.236, Elapsed time: 19287.51
2024-07-25 14:24:57 - [34m[1mLOGS   [0m - *** Training summary for epoch 0
	 loss={'classification': 228.3399, 'neural_augmentation': 8.4689, 'total_loss': 236.8088}
2024-07-25 14:24:59 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at /ML-A100/team/mm/models/catlip_data/results_vit_small/train/checkpoint_best.pt
2024-07-25 14:24:59 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_vit_small/train/training_checkpoint_last.pt
2024-07-25 14:24:59 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_vit_small/train/checkpoint_last.pt
2024-07-25 14:24:59 - [34m[1mLOGS   [0m - Training checkpoint for epoch 0/iteration 7853 is saved at: /ML-A100/team/mm/models/catlip_data/results_vit_small/train/training_checkpoint_epoch_0_iter_7853.pt
2024-07-25 14:24:59 - [34m[1mLOGS   [0m - Model state for epoch 0/iteration 7853 is saved at: /ML-A100/team/mm/models/catlip_data/results_vit_small/train/checkpoint_epoch_0_iter_7853.pt
[31m===========================================================================[0m
2024-07-25 14:25:01 - [32m[1mINFO   [0m - Training epoch 1
2024-07-25 14:26:59 - [34m[1mLOGS   [0m - Epoch:   1 [    7853/  200000], loss: {'classification': 61.8656, 'neural_augmentation': 6.2273, 'total_loss': 68.0929}, LR: [0.000393, 0.000393], Avg. batch load time: 105.159, Elapsed time: 117.63
2024-07-25 14:31:21 - [34m[1mLOGS   [0m - Epoch:   1 [    7978/  200000], loss: {'classification': 48.1987, 'neural_augmentation': 6.5652, 'total_loss': 54.7638}, LR: [0.0004, 0.0004], Avg. batch load time: 0.244, Elapsed time: 379.75
2024-07-25 14:36:32 - [34m[1mLOGS   [0m - Epoch:   1 [    8103/  200000], loss: {'classification': 47.5927, 'neural_augmentation': 6.5036, 'total_loss': 54.0963}, LR: [0.000406, 0.000406], Avg. batch load time: 0.174, Elapsed time: 690.70
2024-07-25 14:42:02 - [34m[1mLOGS   [0m - Epoch:   1 [    8228/  200000], loss: {'classification': 47.2896, 'neural_augmentation': 6.468, 'total_loss': 53.7576}, LR: [0.000412, 0.000412], Avg. batch load time: 0.116, Elapsed time: 1020.86
Terminated
Process SpawnProcess-1:
Traceback (most recent call last):
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/process.py", line 317, in _bootstrap
    util._exit_function()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/util.py", line 353, in _exit_function
    p._popen.terminate()
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/popen_fork.py", line 57, in terminate
    self._send_signal(signal.SIGTERM)
  File "/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/popen_fork.py", line 49, in _send_signal
    os.kill(self.pid, sig)
KeyboardInterrupt
2024-07-25 14:45:23 - [34m[1mLOGS   [0m - Keyboard interruption. Exiting from early training
2024-07-25 14:45:23 - [34m[1mLOGS   [0m - Training took 05:47:12.94
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
