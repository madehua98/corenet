# pytest: disable

taskname: '+ CLIP-ViT-Tiny/16'
common:
  run_label: "train"
  log_freq: 500
  auto_resume: true
  mixed_precision: true
  accum_freq: 1
  save_all_checkpoints: true
  save_interval_freq: 5000
dataset:
  # root_train does not matter for img_text_tar dataset because dataset is information is expected
  # to be contained in metadata file.
  root_train: ""
  disable_val: true
  train_batch_size0: 2
  val_batch_size0: 2
  eval_batch_size0: 2
  padding_index: 0
  persistent_workers: false
  pin_memory: true
  workers: 0
  collate_fn_name_train: "multi_modal_img_text_collate_fn"
  collate_fn_name_val: "multi_modal_img_text_collate_fn"
  collate_fn_name_test: "multi_modal_img_text_collate_fn"

  name: "mock_img_text_tar"
  category: "multi_modal_image_text"
  multi_modal_img_text:
    context_length: 12
    img_text_tar:
      metadata_file: ".corenet_data_cache/metadata.pkl"

text_tokenizer:
  name: "clip"
  clip:
    merges_path: "http://download.pytorch.org/models/text/clip_merges.bpe"
    encoder_json_path: "http://download.pytorch.org/models/text/clip_encoder.json"
image_augmentation:
  random_resized_crop:
    enable: true
    interpolation: "bilinear"
    scale: [0.9, 1.0]
  resize:
    enable: true
    size: 64
    interpolation: "bilinear"
  center_crop:
    enable: true
    size: 64
sampler:
  name: "variable_batch_sampler"
  use_shards: true
  vbs:
    crop_size_width: 64
    crop_size_height: 64
    max_n_scales: 5
    min_crop_size_width: 32
    max_crop_size_width: 64
    min_crop_size_height: 32
    max_crop_size_height: 64
    check_scale: 32
loss:
  category: "multi_modal_image_text"
  multi_modal_image_text:
    name: "contrastive_loss_clip"
optim:
  name: "adamw"
  weight_decay: 0.2
  no_decay_bn_filter_bias: true
  adamw:
    beta1: 0.9
    beta2: 0.98
    eps: 1.e-6
scheduler:
  is_iteration_based: true
  max_iterations: 4
  name: cosine
  warmup_init_lr: 1.e-06
  warmup_iterations: 1000
  cosine:
    max_lr: 0.0005
    min_lr: 1.e-06
model:
  activation_checkpointing: true
  multi_modal_image_text: # multi-modal image-text model
    name: "clip"
    lr_multiplier_img_encoder: 1.0
    lr_multiplier_text_encoder: 1.0
    clip:
      projection_dim: 128
  classification:
    name: "vit"
    vit:
      mode: "tiny"
      norm_layer: "layer_norm_fp32"
    activation:
      name: "gelu"
  image_projection_head:
    name: "simple_projection_nc2nc"
  text: # text encoder
    name: "transformer"
    vocab_size: 49408
    context_length: 12
    transformer:
      causal_masking: true
      model_dim: 128
      n_transformer_layers: 2
      ffn_multiplier_per_layer: 4.0
      n_heads_per_layer: 8
      norm_layer: "layer_norm_fp32"
  normalization:
    name: "batch_norm"
    momentum: 0.1
  activation:
    name: "gelu"
    inplace: true
  layer:
    global_pool: "mean"
    conv_init: "kaiming_uniform"
    linear_init: "trunc_normal"
    linear_init_std_dev: 0.02
ema:
  enable: true
  momentum: 0.0005
stats:
  val: [ "top1" ]
  train: ["loss", "grad_norm" ]
  checkpoint_metric: "top1.zero_shot_image_logits"
  checkpoint_metric_max: true
