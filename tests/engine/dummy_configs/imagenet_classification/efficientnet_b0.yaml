# pytest: disable

# This is a dummy configuration for testing classification task end-to-end

taskname: '+ EfficientNet-B0'
common:
  run_label: "train"
  log_freq: 500
  auto_resume: true
  mixed_precision: true
  channels_last: true
dataset:
  root_train: "/mnt/imagenet/training"
  root_val: "/mnt/imagenet/validation"
  name: "mock_imagenet"
  category: "classification"
  train_batch_size0: 2
  val_batch_size0: 2
  eval_batch_size0: 2
  workers: 0
  persistent_workers: true
  pin_memory: true

image_augmentation:
  # training related parameters
  random_resized_crop:
    enable: true
    interpolation: "bilinear"
  random_horizontal_flip:
    enable: true
  # validation related parameters
  resize:
    enable: true
    size: 64
    interpolation: "bilinear"
  center_crop:
    enable: true
    size: 64
sampler:
  name: "variable_batch_sampler"
  vbs:
    crop_size_width: 64
    crop_size_height: 64
    max_n_scales: 5
    min_crop_size_width: 32
    max_crop_size_width: 96
    min_crop_size_height: 32
    max_crop_size_height: 96
    check_scale: 32
loss:
  category: "composite_loss"
  composite_loss:
    - loss_category: "classification"
      loss_weight: 1.0
      classification:
        name: "cross_entropy"
        cross_entropy:
          label_smoothing: 0.1
    - loss_category: "neural_augmentation"
      loss_weight: 1.0
      neural_augmentation:
        perceptual_metric: "psnr"
        target_value: [ 40, 10 ]
        curriculum_method: "cosine"
optim:
  name: "sgd"
  weight_decay: 4.e-5
  no_decay_bn_filter_bias: true
  sgd:
    momentum: 0.9
    nesterov: true
scheduler:
  name: "cosine"
  max_epochs: 2
  warmup_iterations: 3000
  warmup_init_lr: 0.1
  cosine:
    max_lr: 0.8
    min_lr: 4.e-4
model:
  classification:
    name: "efficientnet"
    activation:
      name: "swish"
    efficientnet:
      mode: "b0"
      stochastic_depth_prob: 0.0
    classifier_dropout: 0.0
  learn_augmentation:
    brightness: true
    contrast: true
    noise: true
    mode: "distribution"
  normalization:
    name: "batch_norm"
    momentum: 0.1
  activation:
    name: "swish"
    inplace: true
  layer:
    global_pool: "mean"
    conv_init: "kaiming_normal"
    linear_init: "normal"
ema:
  enable: true
  momentum: 0.0005
stats:
  val: [ "loss", "top1", "top5" ]
  train: ["loss"]
  checkpoint_metric: "top1.logits"
  checkpoint_metric_max: true
