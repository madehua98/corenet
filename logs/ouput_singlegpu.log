nohup: å¿½ç•¥è¾“å…¥
2024-06-07 09:05:31 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
2024-06-07 09:05:32 - [32m[1mINFO   [0m - Trainable parameters: ['cls_token', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_emb.0.block.conv.weight', 'patch_emb.0.block.norm.weight', 'patch_emb.0.block.norm.bias', 'patch_emb.1.block.conv.weight', 'patch_emb.1.block.norm.weight', 'patch_emb.1.block.norm.bias', 'patch_emb.2.block.conv.weight', 'patch_emb.2.block.conv.bias', 'post_transformer_norm.weight', 'post_transformer_norm.bias', 'transformer.0.pre_norm_mha.0.weight', 'transformer.0.pre_norm_mha.0.bias', 'transformer.0.pre_norm_mha.1.qkv_proj.weight', 'transformer.0.pre_norm_mha.1.qkv_proj.bias', 'transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'transformer.0.pre_norm_ffn.0.weight', 'transformer.0.pre_norm_ffn.0.bias', 'transformer.0.pre_norm_ffn.1.weight', 'transformer.0.pre_norm_ffn.1.bias', 'transformer.0.pre_norm_ffn.4.weight', 'transformer.0.pre_norm_ffn.4.bias', 'transformer.1.pre_norm_mha.0.weight', 'transformer.1.pre_norm_mha.0.bias', 'transformer.1.pre_norm_mha.1.qkv_proj.weight', 'transformer.1.pre_norm_mha.1.qkv_proj.bias', 'transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'transformer.1.pre_norm_ffn.0.weight', 'transformer.1.pre_norm_ffn.0.bias', 'transformer.1.pre_norm_ffn.1.weight', 'transformer.1.pre_norm_ffn.1.bias', 'transformer.1.pre_norm_ffn.4.weight', 'transformer.1.pre_norm_ffn.4.bias', 'transformer.2.pre_norm_mha.0.weight', 'transformer.2.pre_norm_mha.0.bias', 'transformer.2.pre_norm_mha.1.qkv_proj.weight', 'transformer.2.pre_norm_mha.1.qkv_proj.bias', 'transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'transformer.2.pre_norm_ffn.0.weight', 'transformer.2.pre_norm_ffn.0.bias', 'transformer.2.pre_norm_ffn.1.weight', 'transformer.2.pre_norm_ffn.1.bias', 'transformer.2.pre_norm_ffn.4.weight', 'transformer.2.pre_norm_ffn.4.bias', 'transformer.3.pre_norm_mha.0.weight', 'transformer.3.pre_norm_mha.0.bias', 'transformer.3.pre_norm_mha.1.qkv_proj.weight', 'transformer.3.pre_norm_mha.1.qkv_proj.bias', 'transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'transformer.3.pre_norm_ffn.0.weight', 'transformer.3.pre_norm_ffn.0.bias', 'transformer.3.pre_norm_ffn.1.weight', 'transformer.3.pre_norm_ffn.1.bias', 'transformer.3.pre_norm_ffn.4.weight', 'transformer.3.pre_norm_ffn.4.bias', 'transformer.4.pre_norm_mha.0.weight', 'transformer.4.pre_norm_mha.0.bias', 'transformer.4.pre_norm_mha.1.qkv_proj.weight', 'transformer.4.pre_norm_mha.1.qkv_proj.bias', 'transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'transformer.4.pre_norm_ffn.0.weight', 'transformer.4.pre_norm_ffn.0.bias', 'transformer.4.pre_norm_ffn.1.weight', 'transformer.4.pre_norm_ffn.1.bias', 'transformer.4.pre_norm_ffn.4.weight', 'transformer.4.pre_norm_ffn.4.bias', 'transformer.5.pre_norm_mha.0.weight', 'transformer.5.pre_norm_mha.0.bias', 'transformer.5.pre_norm_mha.1.qkv_proj.weight', 'transformer.5.pre_norm_mha.1.qkv_proj.bias', 'transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'transformer.5.pre_norm_ffn.0.weight', 'transformer.5.pre_norm_ffn.0.bias', 'transformer.5.pre_norm_ffn.1.weight', 'transformer.5.pre_norm_ffn.1.bias', 'transformer.5.pre_norm_ffn.4.weight', 'transformer.5.pre_norm_ffn.4.bias', 'transformer.6.pre_norm_mha.0.weight', 'transformer.6.pre_norm_mha.0.bias', 'transformer.6.pre_norm_mha.1.qkv_proj.weight', 'transformer.6.pre_norm_mha.1.qkv_proj.bias', 'transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'transformer.6.pre_norm_ffn.0.weight', 'transformer.6.pre_norm_ffn.0.bias', 'transformer.6.pre_norm_ffn.1.weight', 'transformer.6.pre_norm_ffn.1.bias', 'transformer.6.pre_norm_ffn.4.weight', 'transformer.6.pre_norm_ffn.4.bias', 'transformer.7.pre_norm_mha.0.weight', 'transformer.7.pre_norm_mha.0.bias', 'transformer.7.pre_norm_mha.1.qkv_proj.weight', 'transformer.7.pre_norm_mha.1.qkv_proj.bias', 'transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'transformer.7.pre_norm_ffn.0.weight', 'transformer.7.pre_norm_ffn.0.bias', 'transformer.7.pre_norm_ffn.1.weight', 'transformer.7.pre_norm_ffn.1.bias', 'transformer.7.pre_norm_ffn.4.weight', 'transformer.7.pre_norm_ffn.4.bias', 'transformer.8.pre_norm_mha.0.weight', 'transformer.8.pre_norm_mha.0.bias', 'transformer.8.pre_norm_mha.1.qkv_proj.weight', 'transformer.8.pre_norm_mha.1.qkv_proj.bias', 'transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'transformer.8.pre_norm_ffn.0.weight', 'transformer.8.pre_norm_ffn.0.bias', 'transformer.8.pre_norm_ffn.1.weight', 'transformer.8.pre_norm_ffn.1.bias', 'transformer.8.pre_norm_ffn.4.weight', 'transformer.8.pre_norm_ffn.4.bias', 'transformer.9.pre_norm_mha.0.weight', 'transformer.9.pre_norm_mha.0.bias', 'transformer.9.pre_norm_mha.1.qkv_proj.weight', 'transformer.9.pre_norm_mha.1.qkv_proj.bias', 'transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'transformer.9.pre_norm_ffn.0.weight', 'transformer.9.pre_norm_ffn.0.bias', 'transformer.9.pre_norm_ffn.1.weight', 'transformer.9.pre_norm_ffn.1.bias', 'transformer.9.pre_norm_ffn.4.weight', 'transformer.9.pre_norm_ffn.4.bias', 'transformer.10.pre_norm_mha.0.weight', 'transformer.10.pre_norm_mha.0.bias', 'transformer.10.pre_norm_mha.1.qkv_proj.weight', 'transformer.10.pre_norm_mha.1.qkv_proj.bias', 'transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'transformer.10.pre_norm_ffn.0.weight', 'transformer.10.pre_norm_ffn.0.bias', 'transformer.10.pre_norm_ffn.1.weight', 'transformer.10.pre_norm_ffn.1.bias', 'transformer.10.pre_norm_ffn.4.weight', 'transformer.10.pre_norm_ffn.4.bias', 'transformer.11.pre_norm_mha.0.weight', 'transformer.11.pre_norm_mha.0.bias', 'transformer.11.pre_norm_mha.1.qkv_proj.weight', 'transformer.11.pre_norm_mha.1.qkv_proj.bias', 'transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'transformer.11.pre_norm_ffn.0.weight', 'transformer.11.pre_norm_ffn.0.bias', 'transformer.11.pre_norm_ffn.1.weight', 'transformer.11.pre_norm_ffn.1.bias', 'transformer.11.pre_norm_ffn.4.weight', 'transformer.11.pre_norm_ffn.4.bias', 'classifier.weight', 'classifier.bias', 'pos_embed.pos_embed.pos_embed']
2024-06-07 09:05:32 - [34m[1mLOGS   [0m - [36mModel[0m
VisionTransformer(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_emb): Sequential(
    (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=GELU)
    (1): Conv2d(192, 192, kernel_size=(2, 2), stride=(2, 2), bias=False, normalization=BatchNorm2d, activation=GELU)
    (2): Conv2d(192, 768, kernel_size=(2, 2), stride=(2, 2))
  )
  (post_transformer_norm): LayerNormFP32((768,), eps=1e-06, elementwise_affine=True)
  (transformer): Sequential(
    (0): FlashTransformerEncoder
    (1): FlashTransformerEncoder
    (2): FlashTransformerEncoder
    (3): FlashTransformerEncoder
    (4): FlashTransformerEncoder
    (5): FlashTransformerEncoder
    (6): FlashTransformerEncoder
    (7): FlashTransformerEncoder
    (8): FlashTransformerEncoder
    (9): FlashTransformerEncoder
    (10): FlashTransformerEncoder
    (11): FlashTransformerEncoder
  )
  (classifier): LinearLayer(in_features=768, out_features=24320, bias=True, channel_first=False)
  (pos_embed): LearnablePositionalEmbedding(num_embeddings=196, embedding_dim=768, padding_idx=None, sequence_first=False)
  (emb_dropout): Dropout(p=0.0, inplace=False)
)
[31m=================================================================[0m
                  VisionTransformer Summary
[31m=================================================================[0m
Total parameters     =  104.657 M
Total trainable parameters =  104.657 M

2024-06-07 09:05:32 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-06-07 09:05:32 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.105G                 | 17.031G    |
|  cls_token                           |  (1, 1, 768)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_emb                           |  0.748M                |  0.262G    |
|   patch_emb.0.block                  |   9.6K                 |   30.106M  |
|    patch_emb.0.block.conv            |    9.216K              |    28.901M |
|    patch_emb.0.block.norm            |    0.384K              |    1.204M  |
|   patch_emb.1.block                  |   0.148M               |   0.116G   |
|    patch_emb.1.block.conv            |    0.147M              |    0.116G  |
|    patch_emb.1.block.norm            |    0.384K              |    0.301M  |
|   patch_emb.2.block.conv             |   0.591M               |   0.116G   |
|    patch_emb.2.block.conv.weight     |    (768, 192, 2, 2)    |            |
|    patch_emb.2.block.conv.bias       |    (768,)              |            |
|  post_transformer_norm               |  1.536K                |  0.756M    |
|   post_transformer_norm.weight       |   (768,)               |            |
|   post_transformer_norm.bias         |   (768,)               |            |
|  transformer                         |  85.054M               |  16.75G    |
|   transformer.0                      |   7.088M               |   1.396G   |
|    transformer.0.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.0.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.1                      |   7.088M               |   1.396G   |
|    transformer.1.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.1.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.2                      |   7.088M               |   1.396G   |
|    transformer.2.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.2.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.3                      |   7.088M               |   1.396G   |
|    transformer.3.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.3.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.4                      |   7.088M               |   1.396G   |
|    transformer.4.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.4.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.5                      |   7.088M               |   1.396G   |
|    transformer.5.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.5.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.6                      |   7.088M               |   1.396G   |
|    transformer.6.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.6.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.7                      |   7.088M               |   1.396G   |
|    transformer.7.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.7.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.8                      |   7.088M               |   1.396G   |
|    transformer.8.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.8.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.9                      |   7.088M               |   1.396G   |
|    transformer.9.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.9.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.10                     |   7.088M               |   1.396G   |
|    transformer.10.pre_norm_mha       |    2.364M              |    0.466G  |
|    transformer.10.pre_norm_ffn       |    4.724M              |    0.93G   |
|   transformer.11                     |   7.088M               |   1.396G   |
|    transformer.11.pre_norm_mha       |    2.364M              |    0.466G  |
|    transformer.11.pre_norm_ffn       |    4.724M              |    0.93G   |
|  classifier                          |  18.702M               |  18.678M   |
|   classifier.weight                  |   (24320, 768)         |            |
|   classifier.bias                    |   (24320,)             |            |
|  pos_embed.pos_embed                 |  0.151M                |  0         |
|   pos_embed.pos_embed.pos_embed      |   (1, 1, 196, 768)     |            |
2024-06-07 09:05:33 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-06-07 09:05:33 - [33m[1mWARNING[0m - Uncalled Modules:
{'neural_augmentor.noise.max_fn', 'neural_augmentor.brightness', 'transformer.7.drop_path', 'transformer.1.drop_path', 'transformer.9.drop_path', 'transformer.4.drop_path', 'transformer.10.drop_path', 'transformer.2.drop_path', 'transformer.5.drop_path', 'transformer.3.drop_path', 'transformer.0.drop_path', 'transformer.6.drop_path', 'transformer.8.drop_path', 'transformer.11.drop_path', 'neural_augmentor.noise', 'neural_augmentor.contrast', 'neural_augmentor.contrast.min_fn', 'neural_augmentor.brightness.min_fn', 'neural_augmentor.contrast.max_fn', 'neural_augmentor', 'neural_augmentor.noise.min_fn', 'neural_augmentor.brightness.max_fn'}
2024-06-07 09:05:33 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 25, 'aten::gelu': 14, 'aten::scaled_dot_product_attention': 12, 'aten::sub': 1})
[31m=================================================================[0m
2024-06-07 09:05:33 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-06-07 09:05:33 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-06-07 09:05:34 - [34m[1mLOGS   [0m - Available GPUs: 1
2024-06-07 09:05:34 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-06-07 09:05:34 - [34m[1mLOGS   [0m - Directory exists at: results_catlip/train
2024-06-07 09:05:34 - [34m[1mLOGS   [0m - Setting dataset.workers to 112.
2024-06-07 09:05:37 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://127.0.0.1:2345
2024-06-07 09:05:38 - [34m[1mLOGS   [0m - Training dataset details are given below
WordnetTaggedClassificationDataset(
	root= 
	is_training=True 
	num_samples=1040000
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	total_tar_files=104
	max_files_per_tar=10000
	num_synsets=24320
)
2024-06-07 09:05:38 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=True
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=512
	 scales=[(128, 128, 1568), (144, 144, 1238), (160, 160, 1003), (176, 176, 829), (192, 192, 696), (208, 208, 593), (224, 224, 512), (240, 240, 446), (256, 256, 392), (272, 272, 347), (288, 288, 309), (304, 304, 277), (320, 320, 250)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-06-07 09:05:38 - [34m[1mLOGS   [0m - Number of data workers: 112
2024-06-07 09:05:40 - [32m[1mINFO   [0m - Trainable parameters: ['cls_token', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_emb.0.block.conv.weight', 'patch_emb.0.block.norm.weight', 'patch_emb.0.block.norm.bias', 'patch_emb.1.block.conv.weight', 'patch_emb.1.block.norm.weight', 'patch_emb.1.block.norm.bias', 'patch_emb.2.block.conv.weight', 'patch_emb.2.block.conv.bias', 'post_transformer_norm.weight', 'post_transformer_norm.bias', 'transformer.0.pre_norm_mha.0.weight', 'transformer.0.pre_norm_mha.0.bias', 'transformer.0.pre_norm_mha.1.qkv_proj.weight', 'transformer.0.pre_norm_mha.1.qkv_proj.bias', 'transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'transformer.0.pre_norm_ffn.0.weight', 'transformer.0.pre_norm_ffn.0.bias', 'transformer.0.pre_norm_ffn.1.weight', 'transformer.0.pre_norm_ffn.1.bias', 'transformer.0.pre_norm_ffn.4.weight', 'transformer.0.pre_norm_ffn.4.bias', 'transformer.1.pre_norm_mha.0.weight', 'transformer.1.pre_norm_mha.0.bias', 'transformer.1.pre_norm_mha.1.qkv_proj.weight', 'transformer.1.pre_norm_mha.1.qkv_proj.bias', 'transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'transformer.1.pre_norm_ffn.0.weight', 'transformer.1.pre_norm_ffn.0.bias', 'transformer.1.pre_norm_ffn.1.weight', 'transformer.1.pre_norm_ffn.1.bias', 'transformer.1.pre_norm_ffn.4.weight', 'transformer.1.pre_norm_ffn.4.bias', 'transformer.2.pre_norm_mha.0.weight', 'transformer.2.pre_norm_mha.0.bias', 'transformer.2.pre_norm_mha.1.qkv_proj.weight', 'transformer.2.pre_norm_mha.1.qkv_proj.bias', 'transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'transformer.2.pre_norm_ffn.0.weight', 'transformer.2.pre_norm_ffn.0.bias', 'transformer.2.pre_norm_ffn.1.weight', 'transformer.2.pre_norm_ffn.1.bias', 'transformer.2.pre_norm_ffn.4.weight', 'transformer.2.pre_norm_ffn.4.bias', 'transformer.3.pre_norm_mha.0.weight', 'transformer.3.pre_norm_mha.0.bias', 'transformer.3.pre_norm_mha.1.qkv_proj.weight', 'transformer.3.pre_norm_mha.1.qkv_proj.bias', 'transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'transformer.3.pre_norm_ffn.0.weight', 'transformer.3.pre_norm_ffn.0.bias', 'transformer.3.pre_norm_ffn.1.weight', 'transformer.3.pre_norm_ffn.1.bias', 'transformer.3.pre_norm_ffn.4.weight', 'transformer.3.pre_norm_ffn.4.bias', 'transformer.4.pre_norm_mha.0.weight', 'transformer.4.pre_norm_mha.0.bias', 'transformer.4.pre_norm_mha.1.qkv_proj.weight', 'transformer.4.pre_norm_mha.1.qkv_proj.bias', 'transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'transformer.4.pre_norm_ffn.0.weight', 'transformer.4.pre_norm_ffn.0.bias', 'transformer.4.pre_norm_ffn.1.weight', 'transformer.4.pre_norm_ffn.1.bias', 'transformer.4.pre_norm_ffn.4.weight', 'transformer.4.pre_norm_ffn.4.bias', 'transformer.5.pre_norm_mha.0.weight', 'transformer.5.pre_norm_mha.0.bias', 'transformer.5.pre_norm_mha.1.qkv_proj.weight', 'transformer.5.pre_norm_mha.1.qkv_proj.bias', 'transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'transformer.5.pre_norm_ffn.0.weight', 'transformer.5.pre_norm_ffn.0.bias', 'transformer.5.pre_norm_ffn.1.weight', 'transformer.5.pre_norm_ffn.1.bias', 'transformer.5.pre_norm_ffn.4.weight', 'transformer.5.pre_norm_ffn.4.bias', 'transformer.6.pre_norm_mha.0.weight', 'transformer.6.pre_norm_mha.0.bias', 'transformer.6.pre_norm_mha.1.qkv_proj.weight', 'transformer.6.pre_norm_mha.1.qkv_proj.bias', 'transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'transformer.6.pre_norm_ffn.0.weight', 'transformer.6.pre_norm_ffn.0.bias', 'transformer.6.pre_norm_ffn.1.weight', 'transformer.6.pre_norm_ffn.1.bias', 'transformer.6.pre_norm_ffn.4.weight', 'transformer.6.pre_norm_ffn.4.bias', 'transformer.7.pre_norm_mha.0.weight', 'transformer.7.pre_norm_mha.0.bias', 'transformer.7.pre_norm_mha.1.qkv_proj.weight', 'transformer.7.pre_norm_mha.1.qkv_proj.bias', 'transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'transformer.7.pre_norm_ffn.0.weight', 'transformer.7.pre_norm_ffn.0.bias', 'transformer.7.pre_norm_ffn.1.weight', 'transformer.7.pre_norm_ffn.1.bias', 'transformer.7.pre_norm_ffn.4.weight', 'transformer.7.pre_norm_ffn.4.bias', 'transformer.8.pre_norm_mha.0.weight', 'transformer.8.pre_norm_mha.0.bias', 'transformer.8.pre_norm_mha.1.qkv_proj.weight', 'transformer.8.pre_norm_mha.1.qkv_proj.bias', 'transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'transformer.8.pre_norm_ffn.0.weight', 'transformer.8.pre_norm_ffn.0.bias', 'transformer.8.pre_norm_ffn.1.weight', 'transformer.8.pre_norm_ffn.1.bias', 'transformer.8.pre_norm_ffn.4.weight', 'transformer.8.pre_norm_ffn.4.bias', 'transformer.9.pre_norm_mha.0.weight', 'transformer.9.pre_norm_mha.0.bias', 'transformer.9.pre_norm_mha.1.qkv_proj.weight', 'transformer.9.pre_norm_mha.1.qkv_proj.bias', 'transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'transformer.9.pre_norm_ffn.0.weight', 'transformer.9.pre_norm_ffn.0.bias', 'transformer.9.pre_norm_ffn.1.weight', 'transformer.9.pre_norm_ffn.1.bias', 'transformer.9.pre_norm_ffn.4.weight', 'transformer.9.pre_norm_ffn.4.bias', 'transformer.10.pre_norm_mha.0.weight', 'transformer.10.pre_norm_mha.0.bias', 'transformer.10.pre_norm_mha.1.qkv_proj.weight', 'transformer.10.pre_norm_mha.1.qkv_proj.bias', 'transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'transformer.10.pre_norm_ffn.0.weight', 'transformer.10.pre_norm_ffn.0.bias', 'transformer.10.pre_norm_ffn.1.weight', 'transformer.10.pre_norm_ffn.1.bias', 'transformer.10.pre_norm_ffn.4.weight', 'transformer.10.pre_norm_ffn.4.bias', 'transformer.11.pre_norm_mha.0.weight', 'transformer.11.pre_norm_mha.0.bias', 'transformer.11.pre_norm_mha.1.qkv_proj.weight', 'transformer.11.pre_norm_mha.1.qkv_proj.bias', 'transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'transformer.11.pre_norm_ffn.0.weight', 'transformer.11.pre_norm_ffn.0.bias', 'transformer.11.pre_norm_ffn.1.weight', 'transformer.11.pre_norm_ffn.1.bias', 'transformer.11.pre_norm_ffn.4.weight', 'transformer.11.pre_norm_ffn.4.bias', 'classifier.weight', 'classifier.bias', 'pos_embed.pos_embed.pos_embed']
2024-06-07 09:05:40 - [34m[1mLOGS   [0m - [36mModel[0m
VisionTransformer(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_emb): Sequential(
    (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=GELU)
    (1): Conv2d(192, 192, kernel_size=(2, 2), stride=(2, 2), bias=False, normalization=BatchNorm2d, activation=GELU)
    (2): Conv2d(192, 768, kernel_size=(2, 2), stride=(2, 2))
  )
  (post_transformer_norm): LayerNormFP32((768,), eps=1e-06, elementwise_affine=True)
  (transformer): Sequential(
    (0): FlashTransformerEncoder
    (1): FlashTransformerEncoder
    (2): FlashTransformerEncoder
    (3): FlashTransformerEncoder
    (4): FlashTransformerEncoder
    (5): FlashTransformerEncoder
    (6): FlashTransformerEncoder
    (7): FlashTransformerEncoder
    (8): FlashTransformerEncoder
    (9): FlashTransformerEncoder
    (10): FlashTransformerEncoder
    (11): FlashTransformerEncoder
  )
  (classifier): LinearLayer(in_features=768, out_features=24320, bias=True, channel_first=False)
  (pos_embed): LearnablePositionalEmbedding(num_embeddings=196, embedding_dim=768, padding_idx=None, sequence_first=False)
  (emb_dropout): Dropout(p=0.0, inplace=False)
)
[31m=================================================================[0m
                  VisionTransformer Summary
[31m=================================================================[0m
Total parameters     =  104.657 M
Total trainable parameters =  104.657 M

2024-06-07 09:05:40 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-06-07 09:05:40 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.105G                 | 17.031G    |
|  cls_token                           |  (1, 1, 768)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_emb                           |  0.748M                |  0.262G    |
|   patch_emb.0.block                  |   9.6K                 |   30.106M  |
|    patch_emb.0.block.conv            |    9.216K              |    28.901M |
|    patch_emb.0.block.norm            |    0.384K              |    1.204M  |
|   patch_emb.1.block                  |   0.148M               |   0.116G   |
|    patch_emb.1.block.conv            |    0.147M              |    0.116G  |
|    patch_emb.1.block.norm            |    0.384K              |    0.301M  |
|   patch_emb.2.block.conv             |   0.591M               |   0.116G   |
|    patch_emb.2.block.conv.weight     |    (768, 192, 2, 2)    |            |
|    patch_emb.2.block.conv.bias       |    (768,)              |            |
|  post_transformer_norm               |  1.536K                |  0.756M    |
|   post_transformer_norm.weight       |   (768,)               |            |
|   post_transformer_norm.bias         |   (768,)               |            |
|  transformer                         |  85.054M               |  16.75G    |
|   transformer.0                      |   7.088M               |   1.396G   |
|    transformer.0.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.0.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.1                      |   7.088M               |   1.396G   |
|    transformer.1.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.1.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.2                      |   7.088M               |   1.396G   |
|    transformer.2.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.2.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.3                      |   7.088M               |   1.396G   |
|    transformer.3.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.3.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.4                      |   7.088M               |   1.396G   |
|    transformer.4.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.4.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.5                      |   7.088M               |   1.396G   |
|    transformer.5.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.5.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.6                      |   7.088M               |   1.396G   |
|    transformer.6.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.6.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.7                      |   7.088M               |   1.396G   |
|    transformer.7.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.7.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.8                      |   7.088M               |   1.396G   |
|    transformer.8.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.8.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.9                      |   7.088M               |   1.396G   |
|    transformer.9.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.9.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.10                     |   7.088M               |   1.396G   |
|    transformer.10.pre_norm_mha       |    2.364M              |    0.466G  |
|    transformer.10.pre_norm_ffn       |    4.724M              |    0.93G   |
|   transformer.11                     |   7.088M               |   1.396G   |
|    transformer.11.pre_norm_mha       |    2.364M              |    0.466G  |
|    transformer.11.pre_norm_ffn       |    4.724M              |    0.93G   |
|  classifier                          |  18.702M               |  18.678M   |
|   classifier.weight                  |   (24320, 768)         |            |
|   classifier.bias                    |   (24320,)             |            |
|  pos_embed.pos_embed                 |  0.151M                |  0         |
|   pos_embed.pos_embed.pos_embed      |   (1, 1, 196, 768)     |            |
2024-06-07 09:05:40 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-06-07 09:05:40 - [33m[1mWARNING[0m - Uncalled Modules:
{'transformer.8.drop_path', 'neural_augmentor.noise', 'transformer.9.drop_path', 'neural_augmentor.brightness.max_fn', 'neural_augmentor', 'neural_augmentor.brightness', 'transformer.6.drop_path', 'transformer.1.drop_path', 'transformer.0.drop_path', 'transformer.5.drop_path', 'transformer.7.drop_path', 'transformer.10.drop_path', 'transformer.3.drop_path', 'neural_augmentor.brightness.min_fn', 'neural_augmentor.noise.min_fn', 'neural_augmentor.contrast.max_fn', 'transformer.4.drop_path', 'neural_augmentor.contrast.min_fn', 'transformer.2.drop_path', 'neural_augmentor.contrast', 'transformer.11.drop_path', 'neural_augmentor.noise.max_fn'}
2024-06-07 09:05:40 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 25, 'aten::gelu': 14, 'aten::scaled_dot_product_attention': 12, 'aten::sub': 1})
[31m=================================================================[0m
2024-06-07 09:05:40 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-06-07 09:05:41 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-06-07 09:05:41 - [34m[1mLOGS   [0m - [36mOptimizer[0m
AdamWOptimizer (
	 amsgrad: [False, False]
	 betas: [(0.9, 0.999), (0.9, 0.999)]
	 capturable: [False, False]
	 differentiable: [False, False]
	 eps: [1e-08, 1e-08]
	 foreach: [None, None]
	 fused: [None, None]
	 lr: [0.1, 0.1]
	 maximize: [False, False]
	 weight_decay: [0.2, 0.0]
)
2024-06-07 09:05:41 - [34m[1mLOGS   [0m - Max. iteration for training: 200000
2024-06-07 09:05:41 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=1e-05
 	 max_lr=0.001
 	 period=190001
 	 warmup_init_lr=1e-06
 	 warmup_iters=10000
 )
2024-06-07 09:05:41 - [34m[1mLOGS   [0m - Using EMA
2024-06-07 09:05:43 - [34m[1mLOGS   [0m - Loaded checkpoint from results_catlip/train/training_checkpoint_last.pt
2024-06-07 09:05:43 - [34m[1mLOGS   [0m - Resuming training for epoch 13
2024-06-07 09:05:43 - [32m[1mINFO   [0m - Configuration file is stored here: [36mresults_catlip/train/config.yaml[0m
[31m===========================================================================[0m
2024-06-07 09:05:45 - [32m[1mINFO   [0m - Training epoch 13
2024-06-07 09:14:02 - [34m[1mLOGS   [0m - Epoch:  13 [   10387/  200000], loss: {'classification': 4.4078, 'neural_augmentation': 1.4384, 'total_loss': 5.8463}, LR: [0.001, 0.001], Avg. batch load time: 493.901, Elapsed time: 497.01
2024-06-07 09:39:29 - [34m[1mLOGS   [0m - Epoch:  13 [   10887/  200000], loss: {'classification': 5.5578, 'neural_augmentation': 1.2612, 'total_loss': 6.819}, LR: [0.001, 0.001], Avg. batch load time: 2.198, Elapsed time: 2023.99
2024-06-07 09:59:47 - [34m[1mLOGS   [0m - Epoch:  13 [   11387/  200000], loss: {'classification': 5.5362, 'neural_augmentation': 1.197, 'total_loss': 6.7331}, LR: [0.001, 0.001], Avg. batch load time: 1.398, Elapsed time: 3242.54
2024-06-07 10:17:52 - [34m[1mLOGS   [0m - Epoch:  13 [   11887/  200000], loss: {'classification': 5.4862, 'neural_augmentation': 1.1356, 'total_loss': 6.6217}, LR: [0.001, 0.001], Avg. batch load time: 1.037, Elapsed time: 4327.76
2024-06-07 10:20:07 - [34m[1mLOGS   [0m - *** Training summary for epoch 13
	 loss={'classification': 5.4791, 'neural_augmentation': 1.1266, 'total_loss': 6.6057}
2024-06-07 10:20:09 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at results_catlip/train/checkpoint_best.pt
2024-06-07 10:20:16 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 10:20:18 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 10:20:22 - [34m[1mLOGS   [0m - Training checkpoint for epoch 13/iteration 11960 is saved at: results_catlip/train/training_checkpoint_epoch_13_iter_11960.pt
2024-06-07 10:20:23 - [34m[1mLOGS   [0m - Model state for epoch 13/iteration 11960 is saved at: results_catlip/train/checkpoint_epoch_13_iter_11960.pt
2024-06-07 10:20:25 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 10:20:26 - [34m[1mLOGS   [0m - Best EMA checkpoint with score 0.00 is saved at results_catlip/train/checkpoint_ema_best.pt
2024-06-07 10:20:29 - [34m[1mLOGS   [0m - EMA model state for epoch 13/iteration 11960 is saved at: results_catlip/train/checkpoint_ema_epoch_13_iter_11960.pt
[31m===========================================================================[0m
2024-06-07 10:20:31 - [32m[1mINFO   [0m - Training epoch 14
2024-06-07 10:22:34 - [34m[1mLOGS   [0m - Epoch:  14 [   11961/  200000], loss: {'classification': 4.6891, 'neural_augmentation': 0.8802, 'total_loss': 5.5693}, LR: [0.001, 0.001], Avg. batch load time: 121.928, Elapsed time: 123.69
2024-06-07 10:44:35 - [34m[1mLOGS   [0m - Epoch:  14 [   12461/  200000], loss: {'classification': 5.2133, 'neural_augmentation': 0.8818, 'total_loss': 6.095}, LR: [0.001, 0.001], Avg. batch load time: 1.016, Elapsed time: 1444.77
2024-06-07 11:04:09 - [34m[1mLOGS   [0m - Epoch:  14 [   12961/  200000], loss: {'classification': 5.15, 'neural_augmentation': 0.833, 'total_loss': 5.983}, LR: [0.000999, 0.000999], Avg. batch load time: 0.759, Elapsed time: 2617.93
2024-06-07 11:22:28 - [34m[1mLOGS   [0m - Epoch:  14 [   13461/  200000], loss: {'classification': 5.1089, 'neural_augmentation': 0.7939, 'total_loss': 5.9028}, LR: [0.000999, 0.000999], Avg. batch load time: 0.623, Elapsed time: 3717.02
2024-06-07 11:23:29 - [34m[1mLOGS   [0m - *** Training summary for epoch 14
	 loss={'classification': 5.1041, 'neural_augmentation': 0.7913, 'total_loss': 5.8954}
2024-06-07 11:23:30 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at results_catlip/train/checkpoint_best.pt
2024-06-07 11:23:34 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 11:23:34 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 11:23:36 - [34m[1mLOGS   [0m - Training checkpoint for epoch 14/iteration 13494 is saved at: results_catlip/train/training_checkpoint_epoch_14_iter_13494.pt
2024-06-07 11:23:37 - [34m[1mLOGS   [0m - Model state for epoch 14/iteration 13494 is saved at: results_catlip/train/checkpoint_epoch_14_iter_13494.pt
2024-06-07 11:23:38 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 11:23:38 - [34m[1mLOGS   [0m - Best EMA checkpoint with score 0.00 is saved at results_catlip/train/checkpoint_ema_best.pt
2024-06-07 11:23:39 - [34m[1mLOGS   [0m - EMA model state for epoch 14/iteration 13494 is saved at: results_catlip/train/checkpoint_ema_epoch_14_iter_13494.pt
[31m===========================================================================[0m
2024-06-07 11:23:41 - [32m[1mINFO   [0m - Training epoch 15
2024-06-07 11:24:14 - [34m[1mLOGS   [0m - Epoch:  15 [   13495/  200000], loss: {'classification': 4.1818, 'neural_augmentation': 0.7159, 'total_loss': 4.8977}, LR: [0.000999, 0.000999], Avg. batch load time: 31.095, Elapsed time: 32.90
2024-06-07 11:40:57 - [34m[1mLOGS   [0m - Epoch:  15 [   13995/  200000], loss: {'classification': 4.8762, 'neural_augmentation': 0.6435, 'total_loss': 5.5196}, LR: [0.000999, 0.000999], Avg. batch load time: 0.163, Elapsed time: 1035.40
2024-06-07 11:56:33 - [34m[1mLOGS   [0m - Epoch:  15 [   14495/  200000], loss: {'classification': 4.8807, 'neural_augmentation': 0.6162, 'total_loss': 5.4969}, LR: [0.000999, 0.000999], Avg. batch load time: 0.084, Elapsed time: 1971.27
2024-06-07 12:12:00 - [34m[1mLOGS   [0m - Epoch:  15 [   14995/  200000], loss: {'classification': 4.8529, 'neural_augmentation': 0.5917, 'total_loss': 5.4446}, LR: [0.000998, 0.000998], Avg. batch load time: 0.057, Elapsed time: 2898.19
2024-06-07 12:12:13 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 12:12:14 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 12:12:19 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 14999 is saved at: results_catlip/train/training_checkpoint_epoch_15_iter_14999.pt
2024-06-07 12:12:20 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 14999 is saved at: results_catlip/train/checkpoint_epoch_15_iter_14999.pt
2024-06-07 12:12:22 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 12:12:23 - [34m[1mLOGS   [0m - EMA model state for epoch 15/iteration 14999 is saved at: results_catlip/train/checkpoint_ema_epoch_15_iter_14999.pt
2024-06-07 12:12:23 - [32m[1mINFO   [0m - Checkpoints saved after 14999 updates at: results_catlip/train
[31m======================================================================================================================================================[0m
2024-06-07 12:16:06 - [34m[1mLOGS   [0m - *** Training summary for epoch 15
	 loss={'classification': 4.8509, 'neural_augmentation': 0.5859, 'total_loss': 5.4368}
2024-06-07 12:16:07 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at results_catlip/train/checkpoint_best.pt
2024-06-07 12:16:14 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 12:16:15 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 12:16:20 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 15117 is saved at: results_catlip/train/training_checkpoint_epoch_15_iter_15117.pt
2024-06-07 12:16:21 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 15117 is saved at: results_catlip/train/checkpoint_epoch_15_iter_15117.pt
2024-06-07 12:16:22 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 12:16:24 - [34m[1mLOGS   [0m - Best EMA checkpoint with score 0.00 is saved at results_catlip/train/checkpoint_ema_best.pt
2024-06-07 12:16:26 - [34m[1mLOGS   [0m - EMA model state for epoch 15/iteration 15117 is saved at: results_catlip/train/checkpoint_ema_epoch_15_iter_15117.pt
[31m===========================================================================[0m
2024-06-07 12:16:28 - [32m[1mINFO   [0m - Training epoch 16
2024-06-07 12:17:04 - [34m[1mLOGS   [0m - Epoch:  16 [   15118/  200000], loss: {'classification': 4.8913, 'neural_augmentation': 0.6173, 'total_loss': 5.5085}, LR: [0.000998, 0.000998], Avg. batch load time: 34.125, Elapsed time: 35.92
2024-06-07 12:36:47 - [34m[1mLOGS   [0m - Epoch:  16 [   15618/  200000], loss: {'classification': 4.6404, 'neural_augmentation': 0.4946, 'total_loss': 5.135}, LR: [0.000998, 0.000998], Avg. batch load time: 0.400, Elapsed time: 1219.23
2024-06-07 12:53:43 - [34m[1mLOGS   [0m - Epoch:  16 [   16118/  200000], loss: {'classification': 4.6465, 'neural_augmentation': 0.4782, 'total_loss': 5.1247}, LR: [0.000997, 0.000997], Avg. batch load time: 0.205, Elapsed time: 2235.07
2024-06-07 13:11:36 - [34m[1mLOGS   [0m - Epoch:  16 [   16618/  200000], loss: {'classification': 4.6274, 'neural_augmentation': 0.464, 'total_loss': 5.0914}, LR: [0.000997, 0.000997], Avg. batch load time: 0.139, Elapsed time: 3307.86
2024-06-07 13:22:36 - [34m[1mLOGS   [0m - *** Training summary for epoch 16
	 loss={'classification': 4.6243, 'neural_augmentation': 0.4601, 'total_loss': 5.0844}
2024-06-07 13:23:04 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at results_catlip/train/checkpoint_best.pt
2024-06-07 13:24:10 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 13:24:20 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 13:24:56 - [34m[1mLOGS   [0m - Training checkpoint for epoch 16/iteration 16759 is saved at: results_catlip/train/training_checkpoint_epoch_16_iter_16759.pt
2024-06-07 13:25:13 - [34m[1mLOGS   [0m - Model state for epoch 16/iteration 16759 is saved at: results_catlip/train/checkpoint_epoch_16_iter_16759.pt
2024-06-07 13:25:28 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 13:25:45 - [34m[1mLOGS   [0m - Best EMA checkpoint with score 0.00 is saved at results_catlip/train/checkpoint_ema_best.pt
2024-06-07 13:26:11 - [34m[1mLOGS   [0m - EMA model state for epoch 16/iteration 16759 is saved at: results_catlip/train/checkpoint_ema_epoch_16_iter_16759.pt
[31m===========================================================================[0m
2024-06-07 13:26:13 - [32m[1mINFO   [0m - Training epoch 17
2024-06-07 13:30:09 - [34m[1mLOGS   [0m - Epoch:  17 [   16760/  200000], loss: {'classification': 4.1727, 'neural_augmentation': 0.42, 'total_loss': 4.5927}, LR: [0.000997, 0.000997], Avg. batch load time: 233.535, Elapsed time: 235.30
2024-06-07 13:49:21 - [34m[1mLOGS   [0m - Epoch:  17 [   17260/  200000], loss: {'classification': 4.4369, 'neural_augmentation': 0.4044, 'total_loss': 4.8413}, LR: [0.000996, 0.000996], Avg. batch load time: 0.705, Elapsed time: 1387.50
2024-06-07 14:04:57 - [34m[1mLOGS   [0m - Epoch:  17 [   17760/  200000], loss: {'classification': 4.4354, 'neural_augmentation': 0.3936, 'total_loss': 4.829}, LR: [0.000996, 0.000996], Avg. batch load time: 0.355, Elapsed time: 2323.46
2024-06-07 14:36:36 - [34m[1mLOGS   [0m - Epoch:  17 [   18260/  200000], loss: {'classification': 4.4384, 'neural_augmentation': 0.3836, 'total_loss': 4.822}, LR: [0.000995, 0.000995], Avg. batch load time: 0.239, Elapsed time: 4222.40
2024-06-07 14:47:50 - [34m[1mLOGS   [0m - *** Training summary for epoch 17
	 loss={'classification': 4.435, 'neural_augmentation': 0.3817, 'total_loss': 4.8167}
2024-06-07 14:47:54 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at results_catlip/train/checkpoint_best.pt
2024-06-07 14:54:21 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 14:55:40 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 14:56:05 - [34m[1mLOGS   [0m - Training checkpoint for epoch 17/iteration 18361 is saved at: results_catlip/train/training_checkpoint_epoch_17_iter_18361.pt
2024-06-07 14:56:07 - [34m[1mLOGS   [0m - Model state for epoch 17/iteration 18361 is saved at: results_catlip/train/checkpoint_epoch_17_iter_18361.pt
2024-06-07 14:56:08 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 14:56:09 - [34m[1mLOGS   [0m - Best EMA checkpoint with score 0.00 is saved at results_catlip/train/checkpoint_ema_best.pt
2024-06-07 14:56:12 - [34m[1mLOGS   [0m - EMA model state for epoch 17/iteration 18361 is saved at: results_catlip/train/checkpoint_ema_epoch_17_iter_18361.pt
[31m===========================================================================[0m
2024-06-07 14:56:14 - [32m[1mINFO   [0m - Training epoch 18
2024-06-07 15:01:17 - [34m[1mLOGS   [0m - Epoch:  18 [   18362/  200000], loss: {'classification': 4.3119, 'neural_augmentation': 0.3458, 'total_loss': 4.6577}, LR: [0.000995, 0.000995], Avg. batch load time: 301.376, Elapsed time: 303.19
/home/data_llm/anaconda3/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 346 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
