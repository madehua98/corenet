nohup: å¿½ç•¥è¾“å…¥
2024-06-07 20:51:58 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
2024-06-07 20:51:59 - [32m[1mINFO   [0m - Trainable parameters: ['cls_token', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_emb.0.block.conv.weight', 'patch_emb.0.block.norm.weight', 'patch_emb.0.block.norm.bias', 'patch_emb.1.block.conv.weight', 'patch_emb.1.block.norm.weight', 'patch_emb.1.block.norm.bias', 'patch_emb.2.block.conv.weight', 'patch_emb.2.block.conv.bias', 'post_transformer_norm.weight', 'post_transformer_norm.bias', 'transformer.0.pre_norm_mha.0.weight', 'transformer.0.pre_norm_mha.0.bias', 'transformer.0.pre_norm_mha.1.qkv_proj.weight', 'transformer.0.pre_norm_mha.1.qkv_proj.bias', 'transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'transformer.0.pre_norm_ffn.0.weight', 'transformer.0.pre_norm_ffn.0.bias', 'transformer.0.pre_norm_ffn.1.weight', 'transformer.0.pre_norm_ffn.1.bias', 'transformer.0.pre_norm_ffn.4.weight', 'transformer.0.pre_norm_ffn.4.bias', 'transformer.1.pre_norm_mha.0.weight', 'transformer.1.pre_norm_mha.0.bias', 'transformer.1.pre_norm_mha.1.qkv_proj.weight', 'transformer.1.pre_norm_mha.1.qkv_proj.bias', 'transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'transformer.1.pre_norm_ffn.0.weight', 'transformer.1.pre_norm_ffn.0.bias', 'transformer.1.pre_norm_ffn.1.weight', 'transformer.1.pre_norm_ffn.1.bias', 'transformer.1.pre_norm_ffn.4.weight', 'transformer.1.pre_norm_ffn.4.bias', 'transformer.2.pre_norm_mha.0.weight', 'transformer.2.pre_norm_mha.0.bias', 'transformer.2.pre_norm_mha.1.qkv_proj.weight', 'transformer.2.pre_norm_mha.1.qkv_proj.bias', 'transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'transformer.2.pre_norm_ffn.0.weight', 'transformer.2.pre_norm_ffn.0.bias', 'transformer.2.pre_norm_ffn.1.weight', 'transformer.2.pre_norm_ffn.1.bias', 'transformer.2.pre_norm_ffn.4.weight', 'transformer.2.pre_norm_ffn.4.bias', 'transformer.3.pre_norm_mha.0.weight', 'transformer.3.pre_norm_mha.0.bias', 'transformer.3.pre_norm_mha.1.qkv_proj.weight', 'transformer.3.pre_norm_mha.1.qkv_proj.bias', 'transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'transformer.3.pre_norm_ffn.0.weight', 'transformer.3.pre_norm_ffn.0.bias', 'transformer.3.pre_norm_ffn.1.weight', 'transformer.3.pre_norm_ffn.1.bias', 'transformer.3.pre_norm_ffn.4.weight', 'transformer.3.pre_norm_ffn.4.bias', 'transformer.4.pre_norm_mha.0.weight', 'transformer.4.pre_norm_mha.0.bias', 'transformer.4.pre_norm_mha.1.qkv_proj.weight', 'transformer.4.pre_norm_mha.1.qkv_proj.bias', 'transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'transformer.4.pre_norm_ffn.0.weight', 'transformer.4.pre_norm_ffn.0.bias', 'transformer.4.pre_norm_ffn.1.weight', 'transformer.4.pre_norm_ffn.1.bias', 'transformer.4.pre_norm_ffn.4.weight', 'transformer.4.pre_norm_ffn.4.bias', 'transformer.5.pre_norm_mha.0.weight', 'transformer.5.pre_norm_mha.0.bias', 'transformer.5.pre_norm_mha.1.qkv_proj.weight', 'transformer.5.pre_norm_mha.1.qkv_proj.bias', 'transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'transformer.5.pre_norm_ffn.0.weight', 'transformer.5.pre_norm_ffn.0.bias', 'transformer.5.pre_norm_ffn.1.weight', 'transformer.5.pre_norm_ffn.1.bias', 'transformer.5.pre_norm_ffn.4.weight', 'transformer.5.pre_norm_ffn.4.bias', 'transformer.6.pre_norm_mha.0.weight', 'transformer.6.pre_norm_mha.0.bias', 'transformer.6.pre_norm_mha.1.qkv_proj.weight', 'transformer.6.pre_norm_mha.1.qkv_proj.bias', 'transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'transformer.6.pre_norm_ffn.0.weight', 'transformer.6.pre_norm_ffn.0.bias', 'transformer.6.pre_norm_ffn.1.weight', 'transformer.6.pre_norm_ffn.1.bias', 'transformer.6.pre_norm_ffn.4.weight', 'transformer.6.pre_norm_ffn.4.bias', 'transformer.7.pre_norm_mha.0.weight', 'transformer.7.pre_norm_mha.0.bias', 'transformer.7.pre_norm_mha.1.qkv_proj.weight', 'transformer.7.pre_norm_mha.1.qkv_proj.bias', 'transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'transformer.7.pre_norm_ffn.0.weight', 'transformer.7.pre_norm_ffn.0.bias', 'transformer.7.pre_norm_ffn.1.weight', 'transformer.7.pre_norm_ffn.1.bias', 'transformer.7.pre_norm_ffn.4.weight', 'transformer.7.pre_norm_ffn.4.bias', 'transformer.8.pre_norm_mha.0.weight', 'transformer.8.pre_norm_mha.0.bias', 'transformer.8.pre_norm_mha.1.qkv_proj.weight', 'transformer.8.pre_norm_mha.1.qkv_proj.bias', 'transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'transformer.8.pre_norm_ffn.0.weight', 'transformer.8.pre_norm_ffn.0.bias', 'transformer.8.pre_norm_ffn.1.weight', 'transformer.8.pre_norm_ffn.1.bias', 'transformer.8.pre_norm_ffn.4.weight', 'transformer.8.pre_norm_ffn.4.bias', 'transformer.9.pre_norm_mha.0.weight', 'transformer.9.pre_norm_mha.0.bias', 'transformer.9.pre_norm_mha.1.qkv_proj.weight', 'transformer.9.pre_norm_mha.1.qkv_proj.bias', 'transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'transformer.9.pre_norm_ffn.0.weight', 'transformer.9.pre_norm_ffn.0.bias', 'transformer.9.pre_norm_ffn.1.weight', 'transformer.9.pre_norm_ffn.1.bias', 'transformer.9.pre_norm_ffn.4.weight', 'transformer.9.pre_norm_ffn.4.bias', 'transformer.10.pre_norm_mha.0.weight', 'transformer.10.pre_norm_mha.0.bias', 'transformer.10.pre_norm_mha.1.qkv_proj.weight', 'transformer.10.pre_norm_mha.1.qkv_proj.bias', 'transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'transformer.10.pre_norm_ffn.0.weight', 'transformer.10.pre_norm_ffn.0.bias', 'transformer.10.pre_norm_ffn.1.weight', 'transformer.10.pre_norm_ffn.1.bias', 'transformer.10.pre_norm_ffn.4.weight', 'transformer.10.pre_norm_ffn.4.bias', 'transformer.11.pre_norm_mha.0.weight', 'transformer.11.pre_norm_mha.0.bias', 'transformer.11.pre_norm_mha.1.qkv_proj.weight', 'transformer.11.pre_norm_mha.1.qkv_proj.bias', 'transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'transformer.11.pre_norm_ffn.0.weight', 'transformer.11.pre_norm_ffn.0.bias', 'transformer.11.pre_norm_ffn.1.weight', 'transformer.11.pre_norm_ffn.1.bias', 'transformer.11.pre_norm_ffn.4.weight', 'transformer.11.pre_norm_ffn.4.bias', 'classifier.weight', 'classifier.bias', 'pos_embed.pos_embed.pos_embed']
2024-06-07 20:51:59 - [34m[1mLOGS   [0m - [36mModel[0m
VisionTransformer(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_emb): Sequential(
    (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=GELU)
    (1): Conv2d(192, 192, kernel_size=(2, 2), stride=(2, 2), bias=False, normalization=BatchNorm2d, activation=GELU)
    (2): Conv2d(192, 768, kernel_size=(2, 2), stride=(2, 2))
  )
  (post_transformer_norm): LayerNormFP32((768,), eps=1e-06, elementwise_affine=True)
  (transformer): Sequential(
    (0): FlashTransformerEncoder
    (1): FlashTransformerEncoder
    (2): FlashTransformerEncoder
    (3): FlashTransformerEncoder
    (4): FlashTransformerEncoder
    (5): FlashTransformerEncoder
    (6): FlashTransformerEncoder
    (7): FlashTransformerEncoder
    (8): FlashTransformerEncoder
    (9): FlashTransformerEncoder
    (10): FlashTransformerEncoder
    (11): FlashTransformerEncoder
  )
  (classifier): LinearLayer(in_features=768, out_features=24320, bias=True, channel_first=False)
  (pos_embed): LearnablePositionalEmbedding(num_embeddings=196, embedding_dim=768, padding_idx=None, sequence_first=False)
  (emb_dropout): Dropout(p=0.0, inplace=False)
)
[31m=================================================================[0m
                  VisionTransformer Summary
[31m=================================================================[0m
Total parameters     =  104.657 M
Total trainable parameters =  104.657 M

2024-06-07 20:51:59 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-06-07 20:51:59 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.105G                 | 17.031G    |
|  cls_token                           |  (1, 1, 768)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_emb                           |  0.748M                |  0.262G    |
|   patch_emb.0.block                  |   9.6K                 |   30.106M  |
|    patch_emb.0.block.conv            |    9.216K              |    28.901M |
|    patch_emb.0.block.norm            |    0.384K              |    1.204M  |
|   patch_emb.1.block                  |   0.148M               |   0.116G   |
|    patch_emb.1.block.conv            |    0.147M              |    0.116G  |
|    patch_emb.1.block.norm            |    0.384K              |    0.301M  |
|   patch_emb.2.block.conv             |   0.591M               |   0.116G   |
|    patch_emb.2.block.conv.weight     |    (768, 192, 2, 2)    |            |
|    patch_emb.2.block.conv.bias       |    (768,)              |            |
|  post_transformer_norm               |  1.536K                |  0.756M    |
|   post_transformer_norm.weight       |   (768,)               |            |
|   post_transformer_norm.bias         |   (768,)               |            |
|  transformer                         |  85.054M               |  16.75G    |
|   transformer.0                      |   7.088M               |   1.396G   |
|    transformer.0.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.0.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.1                      |   7.088M               |   1.396G   |
|    transformer.1.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.1.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.2                      |   7.088M               |   1.396G   |
|    transformer.2.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.2.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.3                      |   7.088M               |   1.396G   |
|    transformer.3.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.3.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.4                      |   7.088M               |   1.396G   |
|    transformer.4.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.4.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.5                      |   7.088M               |   1.396G   |
|    transformer.5.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.5.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.6                      |   7.088M               |   1.396G   |
|    transformer.6.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.6.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.7                      |   7.088M               |   1.396G   |
|    transformer.7.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.7.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.8                      |   7.088M               |   1.396G   |
|    transformer.8.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.8.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.9                      |   7.088M               |   1.396G   |
|    transformer.9.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.9.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.10                     |   7.088M               |   1.396G   |
|    transformer.10.pre_norm_mha       |    2.364M              |    0.466G  |
|    transformer.10.pre_norm_ffn       |    4.724M              |    0.93G   |
|   transformer.11                     |   7.088M               |   1.396G   |
|    transformer.11.pre_norm_mha       |    2.364M              |    0.466G  |
|    transformer.11.pre_norm_ffn       |    4.724M              |    0.93G   |
|  classifier                          |  18.702M               |  18.678M   |
|   classifier.weight                  |   (24320, 768)         |            |
|   classifier.bias                    |   (24320,)             |            |
|  pos_embed.pos_embed                 |  0.151M                |  0         |
|   pos_embed.pos_embed.pos_embed      |   (1, 1, 196, 768)     |            |
2024-06-07 20:52:00 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-06-07 20:52:00 - [33m[1mWARNING[0m - Uncalled Modules:
{'transformer.2.drop_path', 'transformer.11.drop_path', 'neural_augmentor.brightness', 'transformer.3.drop_path', 'transformer.6.drop_path', 'transformer.9.drop_path', 'transformer.4.drop_path', 'transformer.10.drop_path', 'transformer.5.drop_path', 'neural_augmentor', 'neural_augmentor.contrast.max_fn', 'neural_augmentor.noise.min_fn', 'transformer.7.drop_path', 'transformer.1.drop_path', 'neural_augmentor.brightness.min_fn', 'transformer.8.drop_path', 'transformer.0.drop_path', 'neural_augmentor.contrast', 'neural_augmentor.noise', 'neural_augmentor.contrast.min_fn', 'neural_augmentor.noise.max_fn', 'neural_augmentor.brightness.max_fn'}
2024-06-07 20:52:00 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 25, 'aten::gelu': 14, 'aten::scaled_dot_product_attention': 12, 'aten::sub': 1})
[31m=================================================================[0m
2024-06-07 20:52:00 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-06-07 20:52:00 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-06-07 20:52:00 - [34m[1mLOGS   [0m - Available GPUs: 1
2024-06-07 20:52:00 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-06-07 20:52:00 - [34m[1mLOGS   [0m - Directory exists at: results_catlip/train
2024-06-07 20:52:00 - [34m[1mLOGS   [0m - Setting dataset.workers to 112.
2024-06-07 20:52:03 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://127.0.0.1:2345
2024-06-07 20:52:04 - [34m[1mLOGS   [0m - Training dataset details are given below
WordnetTaggedClassificationDataset(
	root= 
	is_training=True 
	num_samples=1040000
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	total_tar_files=104
	max_files_per_tar=10000
	num_synsets=24320
)
2024-06-07 20:52:04 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=True
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=256
	 scales=[(128, 128, 784), (144, 144, 619), (160, 160, 501), (176, 176, 414), (192, 192, 348), (208, 208, 296), (224, 224, 256), (240, 240, 223), (256, 256, 196), (272, 272, 173), (288, 288, 154), (304, 304, 138), (320, 320, 125)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-06-07 20:52:04 - [34m[1mLOGS   [0m - Number of data workers: 112
2024-06-07 20:52:05 - [32m[1mINFO   [0m - Trainable parameters: ['cls_token', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_emb.0.block.conv.weight', 'patch_emb.0.block.norm.weight', 'patch_emb.0.block.norm.bias', 'patch_emb.1.block.conv.weight', 'patch_emb.1.block.norm.weight', 'patch_emb.1.block.norm.bias', 'patch_emb.2.block.conv.weight', 'patch_emb.2.block.conv.bias', 'post_transformer_norm.weight', 'post_transformer_norm.bias', 'transformer.0.pre_norm_mha.0.weight', 'transformer.0.pre_norm_mha.0.bias', 'transformer.0.pre_norm_mha.1.qkv_proj.weight', 'transformer.0.pre_norm_mha.1.qkv_proj.bias', 'transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'transformer.0.pre_norm_ffn.0.weight', 'transformer.0.pre_norm_ffn.0.bias', 'transformer.0.pre_norm_ffn.1.weight', 'transformer.0.pre_norm_ffn.1.bias', 'transformer.0.pre_norm_ffn.4.weight', 'transformer.0.pre_norm_ffn.4.bias', 'transformer.1.pre_norm_mha.0.weight', 'transformer.1.pre_norm_mha.0.bias', 'transformer.1.pre_norm_mha.1.qkv_proj.weight', 'transformer.1.pre_norm_mha.1.qkv_proj.bias', 'transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'transformer.1.pre_norm_ffn.0.weight', 'transformer.1.pre_norm_ffn.0.bias', 'transformer.1.pre_norm_ffn.1.weight', 'transformer.1.pre_norm_ffn.1.bias', 'transformer.1.pre_norm_ffn.4.weight', 'transformer.1.pre_norm_ffn.4.bias', 'transformer.2.pre_norm_mha.0.weight', 'transformer.2.pre_norm_mha.0.bias', 'transformer.2.pre_norm_mha.1.qkv_proj.weight', 'transformer.2.pre_norm_mha.1.qkv_proj.bias', 'transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'transformer.2.pre_norm_ffn.0.weight', 'transformer.2.pre_norm_ffn.0.bias', 'transformer.2.pre_norm_ffn.1.weight', 'transformer.2.pre_norm_ffn.1.bias', 'transformer.2.pre_norm_ffn.4.weight', 'transformer.2.pre_norm_ffn.4.bias', 'transformer.3.pre_norm_mha.0.weight', 'transformer.3.pre_norm_mha.0.bias', 'transformer.3.pre_norm_mha.1.qkv_proj.weight', 'transformer.3.pre_norm_mha.1.qkv_proj.bias', 'transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'transformer.3.pre_norm_ffn.0.weight', 'transformer.3.pre_norm_ffn.0.bias', 'transformer.3.pre_norm_ffn.1.weight', 'transformer.3.pre_norm_ffn.1.bias', 'transformer.3.pre_norm_ffn.4.weight', 'transformer.3.pre_norm_ffn.4.bias', 'transformer.4.pre_norm_mha.0.weight', 'transformer.4.pre_norm_mha.0.bias', 'transformer.4.pre_norm_mha.1.qkv_proj.weight', 'transformer.4.pre_norm_mha.1.qkv_proj.bias', 'transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'transformer.4.pre_norm_ffn.0.weight', 'transformer.4.pre_norm_ffn.0.bias', 'transformer.4.pre_norm_ffn.1.weight', 'transformer.4.pre_norm_ffn.1.bias', 'transformer.4.pre_norm_ffn.4.weight', 'transformer.4.pre_norm_ffn.4.bias', 'transformer.5.pre_norm_mha.0.weight', 'transformer.5.pre_norm_mha.0.bias', 'transformer.5.pre_norm_mha.1.qkv_proj.weight', 'transformer.5.pre_norm_mha.1.qkv_proj.bias', 'transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'transformer.5.pre_norm_ffn.0.weight', 'transformer.5.pre_norm_ffn.0.bias', 'transformer.5.pre_norm_ffn.1.weight', 'transformer.5.pre_norm_ffn.1.bias', 'transformer.5.pre_norm_ffn.4.weight', 'transformer.5.pre_norm_ffn.4.bias', 'transformer.6.pre_norm_mha.0.weight', 'transformer.6.pre_norm_mha.0.bias', 'transformer.6.pre_norm_mha.1.qkv_proj.weight', 'transformer.6.pre_norm_mha.1.qkv_proj.bias', 'transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'transformer.6.pre_norm_ffn.0.weight', 'transformer.6.pre_norm_ffn.0.bias', 'transformer.6.pre_norm_ffn.1.weight', 'transformer.6.pre_norm_ffn.1.bias', 'transformer.6.pre_norm_ffn.4.weight', 'transformer.6.pre_norm_ffn.4.bias', 'transformer.7.pre_norm_mha.0.weight', 'transformer.7.pre_norm_mha.0.bias', 'transformer.7.pre_norm_mha.1.qkv_proj.weight', 'transformer.7.pre_norm_mha.1.qkv_proj.bias', 'transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'transformer.7.pre_norm_ffn.0.weight', 'transformer.7.pre_norm_ffn.0.bias', 'transformer.7.pre_norm_ffn.1.weight', 'transformer.7.pre_norm_ffn.1.bias', 'transformer.7.pre_norm_ffn.4.weight', 'transformer.7.pre_norm_ffn.4.bias', 'transformer.8.pre_norm_mha.0.weight', 'transformer.8.pre_norm_mha.0.bias', 'transformer.8.pre_norm_mha.1.qkv_proj.weight', 'transformer.8.pre_norm_mha.1.qkv_proj.bias', 'transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'transformer.8.pre_norm_ffn.0.weight', 'transformer.8.pre_norm_ffn.0.bias', 'transformer.8.pre_norm_ffn.1.weight', 'transformer.8.pre_norm_ffn.1.bias', 'transformer.8.pre_norm_ffn.4.weight', 'transformer.8.pre_norm_ffn.4.bias', 'transformer.9.pre_norm_mha.0.weight', 'transformer.9.pre_norm_mha.0.bias', 'transformer.9.pre_norm_mha.1.qkv_proj.weight', 'transformer.9.pre_norm_mha.1.qkv_proj.bias', 'transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'transformer.9.pre_norm_ffn.0.weight', 'transformer.9.pre_norm_ffn.0.bias', 'transformer.9.pre_norm_ffn.1.weight', 'transformer.9.pre_norm_ffn.1.bias', 'transformer.9.pre_norm_ffn.4.weight', 'transformer.9.pre_norm_ffn.4.bias', 'transformer.10.pre_norm_mha.0.weight', 'transformer.10.pre_norm_mha.0.bias', 'transformer.10.pre_norm_mha.1.qkv_proj.weight', 'transformer.10.pre_norm_mha.1.qkv_proj.bias', 'transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'transformer.10.pre_norm_ffn.0.weight', 'transformer.10.pre_norm_ffn.0.bias', 'transformer.10.pre_norm_ffn.1.weight', 'transformer.10.pre_norm_ffn.1.bias', 'transformer.10.pre_norm_ffn.4.weight', 'transformer.10.pre_norm_ffn.4.bias', 'transformer.11.pre_norm_mha.0.weight', 'transformer.11.pre_norm_mha.0.bias', 'transformer.11.pre_norm_mha.1.qkv_proj.weight', 'transformer.11.pre_norm_mha.1.qkv_proj.bias', 'transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'transformer.11.pre_norm_ffn.0.weight', 'transformer.11.pre_norm_ffn.0.bias', 'transformer.11.pre_norm_ffn.1.weight', 'transformer.11.pre_norm_ffn.1.bias', 'transformer.11.pre_norm_ffn.4.weight', 'transformer.11.pre_norm_ffn.4.bias', 'classifier.weight', 'classifier.bias', 'pos_embed.pos_embed.pos_embed']
2024-06-07 20:52:05 - [34m[1mLOGS   [0m - [36mModel[0m
VisionTransformer(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_emb): Sequential(
    (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=GELU)
    (1): Conv2d(192, 192, kernel_size=(2, 2), stride=(2, 2), bias=False, normalization=BatchNorm2d, activation=GELU)
    (2): Conv2d(192, 768, kernel_size=(2, 2), stride=(2, 2))
  )
  (post_transformer_norm): LayerNormFP32((768,), eps=1e-06, elementwise_affine=True)
  (transformer): Sequential(
    (0): FlashTransformerEncoder
    (1): FlashTransformerEncoder
    (2): FlashTransformerEncoder
    (3): FlashTransformerEncoder
    (4): FlashTransformerEncoder
    (5): FlashTransformerEncoder
    (6): FlashTransformerEncoder
    (7): FlashTransformerEncoder
    (8): FlashTransformerEncoder
    (9): FlashTransformerEncoder
    (10): FlashTransformerEncoder
    (11): FlashTransformerEncoder
  )
  (classifier): LinearLayer(in_features=768, out_features=24320, bias=True, channel_first=False)
  (pos_embed): LearnablePositionalEmbedding(num_embeddings=196, embedding_dim=768, padding_idx=None, sequence_first=False)
  (emb_dropout): Dropout(p=0.0, inplace=False)
)
[31m=================================================================[0m
                  VisionTransformer Summary
[31m=================================================================[0m
Total parameters     =  104.657 M
Total trainable parameters =  104.657 M

2024-06-07 20:52:06 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-06-07 20:52:06 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.105G                 | 17.031G    |
|  cls_token                           |  (1, 1, 768)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_emb                           |  0.748M                |  0.262G    |
|   patch_emb.0.block                  |   9.6K                 |   30.106M  |
|    patch_emb.0.block.conv            |    9.216K              |    28.901M |
|    patch_emb.0.block.norm            |    0.384K              |    1.204M  |
|   patch_emb.1.block                  |   0.148M               |   0.116G   |
|    patch_emb.1.block.conv            |    0.147M              |    0.116G  |
|    patch_emb.1.block.norm            |    0.384K              |    0.301M  |
|   patch_emb.2.block.conv             |   0.591M               |   0.116G   |
|    patch_emb.2.block.conv.weight     |    (768, 192, 2, 2)    |            |
|    patch_emb.2.block.conv.bias       |    (768,)              |            |
|  post_transformer_norm               |  1.536K                |  0.756M    |
|   post_transformer_norm.weight       |   (768,)               |            |
|   post_transformer_norm.bias         |   (768,)               |            |
|  transformer                         |  85.054M               |  16.75G    |
|   transformer.0                      |   7.088M               |   1.396G   |
|    transformer.0.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.0.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.1                      |   7.088M               |   1.396G   |
|    transformer.1.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.1.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.2                      |   7.088M               |   1.396G   |
|    transformer.2.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.2.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.3                      |   7.088M               |   1.396G   |
|    transformer.3.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.3.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.4                      |   7.088M               |   1.396G   |
|    transformer.4.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.4.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.5                      |   7.088M               |   1.396G   |
|    transformer.5.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.5.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.6                      |   7.088M               |   1.396G   |
|    transformer.6.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.6.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.7                      |   7.088M               |   1.396G   |
|    transformer.7.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.7.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.8                      |   7.088M               |   1.396G   |
|    transformer.8.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.8.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.9                      |   7.088M               |   1.396G   |
|    transformer.9.pre_norm_mha        |    2.364M              |    0.466G  |
|    transformer.9.pre_norm_ffn        |    4.724M              |    0.93G   |
|   transformer.10                     |   7.088M               |   1.396G   |
|    transformer.10.pre_norm_mha       |    2.364M              |    0.466G  |
|    transformer.10.pre_norm_ffn       |    4.724M              |    0.93G   |
|   transformer.11                     |   7.088M               |   1.396G   |
|    transformer.11.pre_norm_mha       |    2.364M              |    0.466G  |
|    transformer.11.pre_norm_ffn       |    4.724M              |    0.93G   |
|  classifier                          |  18.702M               |  18.678M   |
|   classifier.weight                  |   (24320, 768)         |            |
|   classifier.bias                    |   (24320,)             |            |
|  pos_embed.pos_embed                 |  0.151M                |  0         |
|   pos_embed.pos_embed.pos_embed      |   (1, 1, 196, 768)     |            |
2024-06-07 20:52:06 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-06-07 20:52:06 - [33m[1mWARNING[0m - Uncalled Modules:
{'transformer.3.drop_path', 'neural_augmentor.contrast.max_fn', 'transformer.9.drop_path', 'transformer.6.drop_path', 'transformer.5.drop_path', 'transformer.10.drop_path', 'neural_augmentor.contrast.min_fn', 'neural_augmentor.noise.max_fn', 'transformer.2.drop_path', 'neural_augmentor.noise', 'neural_augmentor.brightness.min_fn', 'neural_augmentor.contrast', 'transformer.0.drop_path', 'neural_augmentor.brightness.max_fn', 'transformer.11.drop_path', 'transformer.8.drop_path', 'transformer.7.drop_path', 'neural_augmentor.noise.min_fn', 'neural_augmentor', 'neural_augmentor.brightness', 'transformer.1.drop_path', 'transformer.4.drop_path'}
2024-06-07 20:52:06 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 25, 'aten::gelu': 14, 'aten::scaled_dot_product_attention': 12, 'aten::sub': 1})
[31m=================================================================[0m
2024-06-07 20:52:06 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-06-07 20:52:06 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-06-07 20:52:06 - [34m[1mLOGS   [0m - [36mOptimizer[0m
AdamWOptimizer (
	 amsgrad: [False, False]
	 betas: [(0.9, 0.999), (0.9, 0.999)]
	 capturable: [False, False]
	 differentiable: [False, False]
	 eps: [1e-08, 1e-08]
	 foreach: [None, None]
	 fused: [None, None]
	 lr: [0.1, 0.1]
	 maximize: [False, False]
	 weight_decay: [0.2, 0.0]
)
2024-06-07 20:52:06 - [34m[1mLOGS   [0m - Max. iteration for training: 200000
2024-06-07 20:52:06 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=1e-05
 	 max_lr=0.001
 	 period=190001
 	 warmup_init_lr=1e-06
 	 warmup_iters=10000
 )
2024-06-07 20:52:06 - [34m[1mLOGS   [0m - Using EMA
2024-06-07 20:52:08 - [34m[1mLOGS   [0m - Loaded checkpoint from results_catlip/train/training_checkpoint_last.pt
2024-06-07 20:52:08 - [34m[1mLOGS   [0m - Resuming training for epoch 18
2024-06-07 20:52:08 - [32m[1mINFO   [0m - Configuration file is stored here: [36mresults_catlip/train/config.yaml[0m
[31m===========================================================================[0m
2024-06-07 20:52:10 - [32m[1mINFO   [0m - Training epoch 18
2024-06-07 20:59:08 - [34m[1mLOGS   [0m - Epoch:  18 [   18363/  200000], loss: {'classification': 4.2663, 'neural_augmentation': 0.3253, 'total_loss': 4.5917}, LR: [0.000995, 0.000995], Avg. batch load time: 415.015, Elapsed time: 418.80
2024-06-07 21:07:13 - [34m[1mLOGS   [0m - Epoch:  18 [   18863/  200000], loss: {'classification': 4.9532, 'neural_augmentation': 0.3556, 'total_loss': 5.3088}, LR: [0.000995, 0.000995], Avg. batch load time: 0.857, Elapsed time: 903.80
2024-06-07 21:17:24 - [34m[1mLOGS   [0m - Epoch:  18 [   19363/  200000], loss: {'classification': 5.0275, 'neural_augmentation': 0.352, 'total_loss': 5.3794}, LR: [0.000994, 0.000994], Avg. batch load time: 0.573, Elapsed time: 1514.00
2024-06-07 21:29:05 - [34m[1mLOGS   [0m - Epoch:  18 [   19863/  200000], loss: {'classification': 5.0283, 'neural_augmentation': 0.3475, 'total_loss': 5.3758}, LR: [0.000993, 0.000993], Avg. batch load time: 0.535, Elapsed time: 2215.32
2024-06-07 21:33:30 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 21:33:30 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 21:33:33 - [34m[1mLOGS   [0m - Training checkpoint for epoch 18/iteration 19999 is saved at: results_catlip/train/training_checkpoint_epoch_18_iter_19999.pt
2024-06-07 21:33:33 - [34m[1mLOGS   [0m - Model state for epoch 18/iteration 19999 is saved at: results_catlip/train/checkpoint_epoch_18_iter_19999.pt
2024-06-07 21:33:34 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 21:33:34 - [34m[1mLOGS   [0m - EMA model state for epoch 18/iteration 19999 is saved at: results_catlip/train/checkpoint_ema_epoch_18_iter_19999.pt
2024-06-07 21:33:34 - [32m[1mINFO   [0m - Checkpoints saved after 19999 updates at: results_catlip/train
[31m======================================================================================================================================================[0m
2024-06-07 21:44:27 - [34m[1mLOGS   [0m - Epoch:  18 [   20363/  200000], loss: {'classification': 5.028, 'neural_augmentation': 0.3403, 'total_loss': 5.3683}, LR: [0.000993, 0.000993], Avg. batch load time: 0.627, Elapsed time: 3137.07
2024-06-07 21:55:44 - [34m[1mLOGS   [0m - Epoch:  18 [   20863/  200000], loss: {'classification': 5.0357, 'neural_augmentation': 0.3337, 'total_loss': 5.3694}, LR: [0.000992, 0.000992], Avg. batch load time: 0.586, Elapsed time: 3814.79
2024-06-07 22:09:49 - [34m[1mLOGS   [0m - Epoch:  18 [   21363/  200000], loss: {'classification': 5.0339, 'neural_augmentation': 0.3272, 'total_loss': 5.3611}, LR: [0.000991, 0.000991], Avg. batch load time: 0.615, Elapsed time: 4659.19
2024-06-07 22:13:13 - [34m[1mLOGS   [0m - *** Training summary for epoch 18
	 loss={'classification': 5.0286, 'neural_augmentation': 0.3243, 'total_loss': 5.3529}
2024-06-07 22:13:14 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at results_catlip/train/checkpoint_best.pt
2024-06-07 22:13:18 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 22:13:19 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 22:13:21 - [34m[1mLOGS   [0m - Training checkpoint for epoch 18/iteration 21579 is saved at: results_catlip/train/training_checkpoint_epoch_18_iter_21579.pt
2024-06-07 22:13:21 - [34m[1mLOGS   [0m - Model state for epoch 18/iteration 21579 is saved at: results_catlip/train/checkpoint_epoch_18_iter_21579.pt
2024-06-07 22:13:22 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 22:13:23 - [34m[1mLOGS   [0m - Best EMA checkpoint with score 0.00 is saved at results_catlip/train/checkpoint_ema_best.pt
2024-06-07 22:13:24 - [34m[1mLOGS   [0m - EMA model state for epoch 18/iteration 21579 is saved at: results_catlip/train/checkpoint_ema_epoch_18_iter_21579.pt
[31m===========================================================================[0m
2024-06-07 22:13:26 - [32m[1mINFO   [0m - Training epoch 19
2024-06-07 22:13:57 - [34m[1mLOGS   [0m - Epoch:  19 [   21580/  200000], loss: {'classification': 3.9264, 'neural_augmentation': 0.2577, 'total_loss': 4.1841}, LR: [0.000991, 0.000991], Avg. batch load time: 29.833, Elapsed time: 30.75
2024-06-07 22:22:34 - [34m[1mLOGS   [0m - Epoch:  19 [   22080/  200000], loss: {'classification': 4.8872, 'neural_augmentation': 0.2784, 'total_loss': 5.1655}, LR: [0.00099, 0.00099], Avg. batch load time: 0.158, Elapsed time: 548.04
2024-06-07 22:30:19 - [34m[1mLOGS   [0m - Epoch:  19 [   22580/  200000], loss: {'classification': 4.9122, 'neural_augmentation': 0.2737, 'total_loss': 5.1859}, LR: [0.000989, 0.000989], Avg. batch load time: 0.080, Elapsed time: 1012.58
2024-06-07 22:38:14 - [34m[1mLOGS   [0m - Epoch:  19 [   23080/  200000], loss: {'classification': 4.9217, 'neural_augmentation': 0.2694, 'total_loss': 5.1911}, LR: [0.000988, 0.000988], Avg. batch load time: 0.054, Elapsed time: 1487.67
2024-06-07 22:46:03 - [34m[1mLOGS   [0m - Epoch:  19 [   23580/  200000], loss: {'classification': 4.9309, 'neural_augmentation': 0.2649, 'total_loss': 5.1958}, LR: [0.000988, 0.000988], Avg. batch load time: 0.040, Elapsed time: 1956.78
2024-06-07 22:53:50 - [34m[1mLOGS   [0m - Epoch:  19 [   24080/  200000], loss: {'classification': 4.9339, 'neural_augmentation': 0.261, 'total_loss': 5.1949}, LR: [0.000987, 0.000987], Avg. batch load time: 0.033, Elapsed time: 2424.15
2024-06-07 23:01:39 - [34m[1mLOGS   [0m - Epoch:  19 [   24580/  200000], loss: {'classification': 4.9297, 'neural_augmentation': 0.2574, 'total_loss': 5.1871}, LR: [0.000986, 0.000986], Avg. batch load time: 0.027, Elapsed time: 2892.54
2024-06-07 23:04:07 - [34m[1mLOGS   [0m - *** Training summary for epoch 19
	 loss={'classification': 4.9269, 'neural_augmentation': 0.2561, 'total_loss': 5.183}
2024-06-07 23:04:08 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at results_catlip/train/checkpoint_best.pt
2024-06-07 23:04:12 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 23:04:13 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 23:04:16 - [34m[1mLOGS   [0m - Training checkpoint for epoch 19/iteration 24738 is saved at: results_catlip/train/training_checkpoint_epoch_19_iter_24738.pt
2024-06-07 23:04:17 - [34m[1mLOGS   [0m - Model state for epoch 19/iteration 24738 is saved at: results_catlip/train/checkpoint_epoch_19_iter_24738.pt
2024-06-07 23:04:18 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 23:04:18 - [34m[1mLOGS   [0m - Best EMA checkpoint with score 0.00 is saved at results_catlip/train/checkpoint_ema_best.pt
2024-06-07 23:04:20 - [34m[1mLOGS   [0m - EMA model state for epoch 19/iteration 24738 is saved at: results_catlip/train/checkpoint_ema_epoch_19_iter_24738.pt
[31m===========================================================================[0m
2024-06-07 23:04:22 - [32m[1mINFO   [0m - Training epoch 20
2024-06-07 23:04:46 - [34m[1mLOGS   [0m - Epoch:  20 [   24739/  200000], loss: {'classification': 3.8206, 'neural_augmentation': 0.2233, 'total_loss': 4.0439}, LR: [0.000985, 0.000985], Avg. batch load time: 23.266, Elapsed time: 24.20
2024-06-07 23:09:37 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 23:09:38 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 23:09:41 - [34m[1mLOGS   [0m - Training checkpoint for epoch 20/iteration 24999 is saved at: results_catlip/train/training_checkpoint_epoch_20_iter_24999.pt
2024-06-07 23:09:41 - [34m[1mLOGS   [0m - Model state for epoch 20/iteration 24999 is saved at: results_catlip/train/checkpoint_epoch_20_iter_24999.pt
2024-06-07 23:09:42 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 23:09:43 - [34m[1mLOGS   [0m - EMA model state for epoch 20/iteration 24999 is saved at: results_catlip/train/checkpoint_ema_epoch_20_iter_24999.pt
2024-06-07 23:09:43 - [32m[1mINFO   [0m - Checkpoints saved after 24999 updates at: results_catlip/train
[31m======================================================================================================================================================[0m
2024-06-07 23:13:28 - [34m[1mLOGS   [0m - Epoch:  20 [   25239/  200000], loss: {'classification': 4.7886, 'neural_augmentation': 0.2292, 'total_loss': 5.0179}, LR: [0.000984, 0.000984], Avg. batch load time: 0.139, Elapsed time: 546.24
2024-06-07 23:21:16 - [34m[1mLOGS   [0m - Epoch:  20 [   25739/  200000], loss: {'classification': 4.8095, 'neural_augmentation': 0.2272, 'total_loss': 5.0368}, LR: [0.000983, 0.000983], Avg. batch load time: 0.070, Elapsed time: 1013.80
2024-06-07 23:29:02 - [34m[1mLOGS   [0m - Epoch:  20 [   26239/  200000], loss: {'classification': 4.8183, 'neural_augmentation': 0.225, 'total_loss': 5.0433}, LR: [0.000982, 0.000982], Avg. batch load time: 0.047, Elapsed time: 1480.60
2024-06-07 23:36:51 - [34m[1mLOGS   [0m - Epoch:  20 [   26739/  200000], loss: {'classification': 4.8341, 'neural_augmentation': 0.2228, 'total_loss': 5.0569}, LR: [0.000981, 0.000981], Avg. batch load time: 0.035, Elapsed time: 1949.62
2024-06-07 23:45:17 - [34m[1mLOGS   [0m - Epoch:  20 [   27239/  200000], loss: {'classification': 4.8401, 'neural_augmentation': 0.2203, 'total_loss': 5.0605}, LR: [0.00098, 0.00098], Avg. batch load time: 0.043, Elapsed time: 2454.79
2024-06-07 23:53:05 - [34m[1mLOGS   [0m - Epoch:  20 [   27739/  200000], loss: {'classification': 4.8375, 'neural_augmentation': 0.2185, 'total_loss': 5.056}, LR: [0.000979, 0.000979], Avg. batch load time: 0.036, Elapsed time: 2923.54
2024-06-07 23:55:56 - [34m[1mLOGS   [0m - *** Training summary for epoch 20
	 loss={'classification': 4.8409, 'neural_augmentation': 0.2179, 'total_loss': 5.0588}
2024-06-07 23:55:57 - [34m[1mLOGS   [0m - Best checkpoint with score 0.00 saved at results_catlip/train/checkpoint_best.pt
2024-06-07 23:56:00 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: results_catlip/train/training_checkpoint_last.pt
2024-06-07 23:56:01 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: results_catlip/train/checkpoint_last.pt
2024-06-07 23:56:03 - [34m[1mLOGS   [0m - Training checkpoint for epoch 20/iteration 27921 is saved at: results_catlip/train/training_checkpoint_epoch_20_iter_27921.pt
2024-06-07 23:56:04 - [34m[1mLOGS   [0m - Model state for epoch 20/iteration 27921 is saved at: results_catlip/train/checkpoint_epoch_20_iter_27921.pt
2024-06-07 23:56:05 - [34m[1mLOGS   [0m - Last EMA model state is saved at: results_catlip/train/checkpoint_ema_last.pt
2024-06-07 23:56:05 - [34m[1mLOGS   [0m - Best EMA checkpoint with score 0.00 is saved at results_catlip/train/checkpoint_ema_best.pt
2024-06-07 23:56:07 - [34m[1mLOGS   [0m - EMA model state for epoch 20/iteration 27921 is saved at: results_catlip/train/checkpoint_ema_epoch_20_iter_27921.pt
[31m===========================================================================[0m
2024-06-07 23:56:09 - [32m[1mINFO   [0m - Training epoch 21
2024-06-07 23:56:39 - [34m[1mLOGS   [0m - Epoch:  21 [   27922/  200000], loss: {'classification': 4.3287, 'neural_augmentation': 0.2187, 'total_loss': 4.5473}, LR: [0.000978, 0.000978], Avg. batch load time: 29.801, Elapsed time: 30.69
2024-06-08 00:04:28 - [34m[1mLOGS   [0m - Epoch:  21 [   28422/  200000], loss: {'classification': 4.7291, 'neural_augmentation': 0.2018, 'total_loss': 4.9309}, LR: [0.000977, 0.000977], Avg. batch load time: 0.060, Elapsed time: 499.80
2024-06-08 00:12:19 - [34m[1mLOGS   [0m - Epoch:  21 [   28922/  200000], loss: {'classification': 4.7274, 'neural_augmentation': 0.2008, 'total_loss': 4.9282}, LR: [0.000976, 0.000976], Avg. batch load time: 0.031, Elapsed time: 970.78
2024-06-08 00:20:07 - [34m[1mLOGS   [0m - Epoch:  21 [   29422/  200000], loss: {'classification': 4.7408, 'neural_augmentation': 0.1992, 'total_loss': 4.94}, LR: [0.000975, 0.000975], Avg. batch load time: 0.021, Elapsed time: 1438.84
Traceback (most recent call last):
  File "/home/data_llm/madehua/corenet/corenet/cli/main_train.py", line 41, in <module>
    main_worker()
  File "/home/data_llm/madehua/corenet/corenet/cli/main_train.py", line 37, in main_worker
    launcher(callback)
  File "/home/data_llm/madehua/corenet/corenet/train_eval_pipelines/default_train_eval.py", line 310, in <lambda>
    return lambda callback: torch.multiprocessing.spawn(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data_llm/madehua/corenet/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 241, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data_llm/madehua/corenet/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File "/home/data_llm/madehua/corenet/venv/lib/python3.11/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGTERM
/home/data_llm/anaconda3/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 344 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
