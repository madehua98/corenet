nohup: ignoring input
2024-08-04 09:20:59 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
small
dci
2024-08-04 09:21:01 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/catlip_data/results_small_dci/train/checkpoint_epoch_9_iter_79046.pt
2024-08-04 09:21:01 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-08-04 09:21:01 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=353, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =   25.836 M
Total trainable parameters =   25.836 M

2024-08-04 09:21:01 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-08-04 09:21:01 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 25.836M                | 3.301G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  58.054M   |
|   pool.proj                          |   1.18M                |   57.803M  |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.251M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  25.69M    |
|   mlp.0                              |   0.263M               |   12.845M  |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   12.845M  |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  0.181M                |  0.181M    |
|   classifier.weight                  |   (353, 512)           |            |
|   classifier.bias                    |   (353,)               |            |
2024-08-04 09:21:01 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-08-04 09:21:01 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks.0.attn.q_norm', 'blocks1.4.attn.k_norm', 'blocks.4.attn.q_norm', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks1.6.attn.q_norm', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks.0.ls2', 'blocks.4.ls2', 'blocks1.2.attn.q_norm', 'blocks.0.attn.attn_drop', 'blocks1.5.ls1', 'patch_embed.backbone.stages.0.0.down', 'blocks1.4.attn.q_norm', 'blocks.6.attn.attn_drop', 'norm', 'patch_embed.backbone.stages.1.3.drop_path', 'patch_embed.backbone.stages.1.1.shortcut', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks1.0.attn.k_norm', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks1.6.ls2', 'blocks1.0.drop_path1', 'blocks1.3.attn.q_norm', 'norm_pre', 'blocks.3.drop_path1', 'blocks1.3.attn.attn_drop', 'blocks.4.drop_path2', 'blocks1.4.ls1', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks.0.attn.k_norm', 'blocks1.0.ls2', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks1.4.ls2', 'blocks1.1.drop_path1', 'patch_embed.backbone.stages.1.2.drop_path', 'neural_augmentor.noise', 'blocks.4.ls1', 'blocks.3.attn.q_norm', 'blocks1.1.ls1', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks.2.ls2', 'neural_augmentor.contrast.min_fn', 'blocks1.6.drop_path1', 'blocks.2.ls1', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.3.ls1', 'neural_augmentor.contrast', 'blocks.4.drop_path1', 'blocks.1.drop_path2', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'patch_embed.backbone.stem.norm1.drop', 'blocks.2.attn.q_norm', 'blocks1.2.drop_path2', 'neural_augmentor.brightness.min_fn', 'blocks.4.attn.k_norm', 'blocks.5.ls1', 'blocks.6.attn.k_norm', 'patch_embed.backbone.stages.1.1.down', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks1.3.drop_path2', 'blocks1.4.drop_path1', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks1.3.drop_path1', 'neural_augmentor.brightness', 'blocks.0.ls1', 'blocks1.2.attn.attn_drop', 'blocks1.0.drop_path2', 'blocks1.5.ls2', 'blocks.3.attn.attn_drop', 'blocks.6.drop_path2', 'neural_augmentor.noise.max_fn', 'neural_augmentor.contrast.max_fn', 'blocks.6.ls1', 'blocks.5.attn.q_norm', 'blocks.3.attn.k_norm', 'patch_embed.backbone.stages.0.1.down', 'blocks1.2.drop_path1', 'patch_embed.backbone.stages.1.2.down', 'blocks.5.ls2', 'blocks.2.attn.attn_drop', 'neural_augmentor.brightness.max_fn', 'blocks1.5.attn.k_norm', 'blocks.2.drop_path1', 'blocks1.2.attn.k_norm', 'blocks.1.attn.attn_drop', 'blocks1.5.attn.q_norm', 'blocks1.4.drop_path2', 'blocks.2.attn.k_norm', 'blocks1.1.attn.k_norm', 'blocks.6.attn.q_norm', 'blocks.3.ls2', 'blocks1.3.ls2', 'patch_embed.backbone.stages.1.3.down', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks1.5.drop_path1', 'blocks.0.drop_path2', 'blocks1.0.ls1', 'blocks.1.attn.q_norm', 'blocks1.2.ls1', 'blocks1.1.attn.attn_drop', 'blocks.3.drop_path2', 'patch_drop', 'blocks1.1.ls2', 'patch_embed.backbone.stages.1.0.down', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks1.3.attn.k_norm', 'blocks.1.attn.k_norm', 'blocks.1.ls1', 'blocks.1.ls2', 'blocks1.0.attn.q_norm', 'blocks.0.drop_path1', 'blocks.4.attn.attn_drop', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks.6.drop_path1', 'blocks.3.ls1', 'blocks.2.drop_path2', 'blocks1.6.drop_path2', 'blocks.5.attn.attn_drop', 'blocks1.2.ls2', 'patch_embed.proj', 'blocks.5.attn.k_norm', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks1.1.drop_path2', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks1.6.ls1', 'blocks.1.drop_path1', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks1.6.attn.attn_drop', 'blocks1.6.attn.k_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks.5.drop_path1', 'blocks1.0.attn.attn_drop', 'blocks1.5.drop_path2', 'blocks1.4.attn.attn_drop', 'neural_augmentor', 'blocks.5.drop_path2', 'blocks1.5.attn.attn_drop', 'blocks.6.ls2', 'blocks1.1.attn.q_norm', 'neural_augmentor.noise.min_fn'}
2024-08-04 09:21:01 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::avg_pool2d': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-08-04 09:21:01 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-08-04 09:21:01 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-08-04 09:21:01 - [34m[1mLOGS   [0m - Available GPUs: 4
2024-08-04 09:21:01 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-08-04 09:21:01 - [34m[1mLOGS   [0m - Setting --ddp.world-size the same as the number of available gpus.
2024-08-04 09:21:01 - [34m[1mLOGS   [0m - Directory created at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train
2024-08-04 09:21:05 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:30008
/ML-A100/team/mm/models/food172/food_172
2024-08-04 09:21:07 - [34m[1mLOGS   [0m - Training dataset details are given below
food172ingredient_lassification(
	root=/ML-A100/team/mm/models/food172/food_172 
	is_training=True 
	num_samples=77087
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
)
/ML-A100/team/mm/models/food172/food_172
2024-08-04 09:21:07 - [34m[1mLOGS   [0m - Validation dataset details are given below
food172ingredient_lassification(
	root=/ML-A100/team/mm/models/food172/food_172 
	is_training=False 
	num_samples=33154
	transforms=Compose(
			Resize(size=232, interpolation=bilinear, maintain_aspect_ratio=True), 
			CenterCrop(size=(h=224, w=224)), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
)
2024-08-04 09:21:07 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=128
	 scales=[(128, 128, 392), (144, 144, 309), (160, 160, 250), (176, 176, 207), (192, 192, 174), (208, 208, 148), (224, 224, 128), (240, 240, 111), (256, 256, 98), (272, 272, 86), (288, 288, 77), (304, 304, 69), (320, 320, 62)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-08-04 09:21:07 - [34m[1mLOGS   [0m - Validation sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=50
	 scales=[(224, 224, 50)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-08-04 09:21:07 - [34m[1mLOGS   [0m - Number of data workers: 64
small
dci
2024-08-04 09:21:10 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/catlip_data/results_small_dci/train/checkpoint_epoch_9_iter_79046.pt
2024-08-04 09:21:10 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-08-04 09:21:10 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=353, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =   25.836 M
Total trainable parameters =   25.836 M

2024-08-04 09:21:10 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-08-04 09:21:10 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 25.836M                | 3.301G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  58.054M   |
|   pool.proj                          |   1.18M                |   57.803M  |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.251M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  25.69M    |
|   mlp.0                              |   0.263M               |   12.845M  |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   12.845M  |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  0.181M                |  0.181M    |
|   classifier.weight                  |   (353, 512)           |            |
|   classifier.bias                    |   (353,)               |            |
2024-08-04 09:21:11 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-08-04 09:21:11 - [33m[1mWARNING[0m - Uncalled Modules:
{'norm_pre', 'blocks.3.ls1', 'blocks.3.attn.q_norm', 'blocks.4.attn.attn_drop', 'neural_augmentor.contrast.max_fn', 'blocks.1.ls2', 'blocks1.1.drop_path2', 'blocks.1.attn.attn_drop', 'patch_embed.proj', 'blocks1.5.ls1', 'blocks1.4.drop_path2', 'patch_drop', 'blocks1.3.attn.q_norm', 'neural_augmentor', 'blocks1.3.attn.attn_drop', 'blocks.6.ls2', 'blocks.6.drop_path2', 'blocks.3.drop_path1', 'norm', 'neural_augmentor.brightness.max_fn', 'blocks1.4.attn.q_norm', 'blocks1.1.drop_path1', 'blocks.4.drop_path2', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks1.0.drop_path2', 'patch_embed.backbone.stages.1.3.drop_path', 'patch_embed.backbone.stages.1.1.down', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks1.5.attn.k_norm', 'blocks.5.ls1', 'neural_augmentor.noise.max_fn', 'blocks1.4.ls1', 'blocks1.5.attn.q_norm', 'patch_embed.backbone.stages.0.1.down', 'blocks1.2.attn.q_norm', 'blocks1.2.drop_path2', 'blocks1.1.attn.attn_drop', 'blocks1.4.drop_path1', 'blocks.5.drop_path1', 'neural_augmentor.brightness.min_fn', 'blocks1.2.drop_path1', 'blocks1.5.attn.attn_drop', 'blocks.6.ls1', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks.2.ls1', 'blocks.0.drop_path1', 'blocks.6.drop_path1', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks.4.attn.q_norm', 'blocks1.3.drop_path1', 'blocks1.3.ls2', 'blocks1.1.attn.k_norm', 'patch_embed.backbone.stages.1.3.down', 'neural_augmentor.contrast.min_fn', 'blocks.2.drop_path2', 'blocks.4.ls2', 'blocks1.6.attn.attn_drop', 'blocks1.5.drop_path1', 'blocks.3.attn.k_norm', 'blocks.2.drop_path1', 'patch_embed.backbone.stages.1.1.shortcut', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks.6.attn.attn_drop', 'blocks1.1.ls1', 'blocks1.4.ls2', 'blocks1.6.drop_path2', 'blocks.1.drop_path2', 'blocks1.4.attn.attn_drop', 'blocks1.5.drop_path2', 'blocks1.0.drop_path1', 'blocks1.3.ls1', 'blocks.0.attn.attn_drop', 'blocks.4.attn.k_norm', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks1.0.attn.k_norm', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'patch_embed.backbone.stages.1.0.down', 'blocks.3.attn.attn_drop', 'neural_augmentor.noise', 'patch_embed.backbone.stages.0.1.shortcut', 'patch_embed.backbone.stages.1.2.down', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks.2.ls2', 'blocks1.0.attn.attn_drop', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks1.0.ls1', 'patch_embed.backbone.stem.norm1.drop', 'blocks1.6.ls2', 'blocks1.6.ls1', 'blocks.0.ls2', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks.0.ls1', 'blocks1.1.ls2', 'blocks.1.attn.q_norm', 'blocks1.4.attn.k_norm', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.3.drop_path2', 'patch_embed.backbone.stages.0.0.down', 'blocks.3.ls2', 'blocks.2.attn.q_norm', 'neural_augmentor.noise.min_fn', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks1.2.ls1', 'blocks1.6.attn.q_norm', 'blocks.1.drop_path1', 'blocks.0.drop_path2', 'blocks.5.attn.q_norm', 'blocks.5.drop_path2', 'blocks1.5.ls2', 'blocks.0.attn.q_norm', 'blocks1.0.attn.q_norm', 'blocks.3.drop_path2', 'blocks.5.attn.attn_drop', 'neural_augmentor.brightness', 'blocks1.6.attn.k_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks1.2.attn.attn_drop', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks.5.attn.k_norm', 'blocks.5.ls2', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks.2.attn.attn_drop', 'blocks.4.ls1', 'blocks1.3.attn.k_norm', 'blocks.1.ls1', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks.2.attn.k_norm', 'blocks1.2.ls2', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks.6.attn.q_norm', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks.0.attn.k_norm', 'blocks.6.attn.k_norm', 'blocks.4.drop_path1', 'neural_augmentor.contrast', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks1.0.ls2', 'blocks1.1.attn.q_norm', 'blocks.1.attn.k_norm', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks1.6.drop_path1', 'blocks1.2.attn.k_norm'}
2024-08-04 09:21:11 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::avg_pool2d': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-08-04 09:21:11 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-08-04 09:21:11 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-08-04 09:21:11 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-08-04 09:21:11 - [34m[1mLOGS   [0m - Max. epochs for training: 30
2024-08-04 09:21:11 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=5e-06
 	 max_lr=5e-05
 	 period=30
 	 warmup_init_lr=1e-06
 	 warmup_iters=500
 )
2024-08-04 09:21:11 - [34m[1mLOGS   [0m - No checkpoint found at '/ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt'
2024-08-04 09:21:11 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/config.yaml[0m
[31m===========================================================================[0m
2024-08-04 09:21:13 - [32m[1mINFO   [0m - Training epoch 0
2024-08-04 09:21:05 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:30008
/ML-A100/team/mm/models/food172/food_172
/ML-A100/team/mm/models/food172/food_172
small
dci
2024-08-04 09:21:05 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:30008
/ML-A100/team/mm/models/food172/food_172
/ML-A100/team/mm/models/food172/food_172
small
dci
2024-08-04 09:21:05 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:30008
/ML-A100/team/mm/models/food172/food_172
/ML-A100/team/mm/models/food172/food_172
small
dci
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-08-04 09:24:10 - [34m[1mLOGS   [0m - Epoch:   0 [       1/10000000], loss: {'classification': 367.3095, 'neural_augmentation': 0.3411, 'total_loss': 367.6506}, LR: [1e-06, 1e-06], Avg. batch load time: 174.290, Elapsed time: 176.95
2024-08-04 09:24:31 - [34m[1mLOGS   [0m - *** Training summary for epoch 0
	 loss={'classification': 210.5107, 'neural_augmentation': 0.3402, 'total_loss': 210.8509}
2024-08-04 09:27:54 - [34m[1mLOGS   [0m - *** Validation summary for epoch 0
	 loss={'classification': 26.7617, 'neural_augmentation': 0.0, 'total_loss': 26.7617} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.3589, 0.046, 0.0432, 0.0091, 0.0474, 0.001, 0.0024, 0.0083, 0.0901, 0.0229, 0.008, 0.0425, 0.0263, 0.0948, 0.0057, 0.2325, 0.089, 0.0985, 0.005, 0.0066, 0.0131, 0.2194, 0.0112, 0.0008, 0.0293, 0.0127, 0.0152, 0.0795, 0.0061, 0.068, 0.0136, 0.016, 0.0277, 0.0161, 0.0018, 0.0005, 0.0022, 0.0019, 0.0018, 0.0133, 0.0058, 0.012, 0.0054, 0.0183, 0.0019, 0.0167, 0.067, 0.004, 0.0052, 0.0216, 0.013, 0.0471, 0.0146, 0.0245, 0.0061, 0.0184, 0.0332, 0.0441, 0.1781, 0.1222, 0.1022, 0.0016, 0.0132, 0.07, 0.0027, 0.0154, 0.0107, 0.0063, 0.2412, 0.0181, 0.0039, 0.0185, 0.0022, 0.0362, 0.015, 0.0284, 0.0206, 0.0019, 0.0006, 0.0034, 0.0296, 0.0826, 0.0725, 0.1029, 0.0481, 0.0249, 0.0094, 0.0022, 0.0033, 0.0143, 0.048, 0.0045, 0.0189, 0.0004, 0.0348, 0.1103, 0.0159, 0.0253, 0.0402, 0.004, 0.0008, 0.0118, 0.0132, 0.0002, 0.0073, 0.011, 0.0003, 0.0012, 0.0286, 0.023, 0.0029, 0.0006, 0.0226, 0.1038, 0.0108, 0.0004, 0.0003, 0.0034, 0.0103, 0.0009, 0.0172, 0.0614, 0.0668, 0.0185, 0.0108, 0.0649, 0.0025, 0.0022, 0.0155, 0.0163, 0.0593, 0.0027, 0.0168, 0.0234, 0.0131, 0.1556, 0.0388, 0.002, 0.0205, 0.0028, 0.0247, 0.0125, 0.0169, 0.0058, 0.0115, 0.0023, 0.027, 0.0011, 0.0185, 0.0129, 0.0065, 0.0124, 0.0027, 0.0001, 0.0017, 0.0293, 0.0282, 0.0054, 0.0782, 0.0108, 0.0066, 0.0205, 0.0333, 0.0016, 0.0032, 0.0004, 0.0025, 0.0004, 0.001, 0.001, 0.0036, 0.0104, 0.0803, 0.0298, 0.0149, 0.0017, 0.001, 0.0686, 0.0153, 0.0032, 0.0138, 0.0116, 0.0024, 0.0012, 0.027, 0.01, 0.0002, 0.0253, 0.0045, 0.0152, 0.0151, 0.0007, 0.0085, 0.0026, 0.003, 0.0097, 0.0781, 0.0215, 0.0102, 0.0003, 0.0288, 0.0123, 0.0152, 0.0068, 0.0061, 0.0157, 0.0016, 0.0131, 0.0312, 0.006, 0.0258, 0.0532, 0.0011, 0.0314, 0.0004, 0.0007, 0.0077, 0.0252, 0.0005, 0.0133, 0.0125, 0.0133, 0.0056, 0.0282, 0.0104, 0.0004, 0.0019, 0.0009, 0.0007, 0.0494, 0.0104, 0.0093, 0.0013, 0.0163, 0.0004, 0.058, 0.0033, 0.0119, 0.0401, 0.0091, 0.0017, 0.0239, 0.0012, 0.0011, 0.0001, 0.0083, 0.0377, 0.0067, 0.0514, 0.0064, 0.0311, 0.0004, 0.018, 0.0208, 0.0456, 0.0145, 0.011, 0.017, 0.0234, 0.0002, 0.0182, 0.002, 0.0115, 0.0017, 0.0433, 0.0082, 0.0278, 0.0098, 0.0036, 0.01, 0.0104, 0.0139, 0.0021, 0.0078, 0.1524, 0.0048, 0.001, 0.0042, 0.0908, 0.0075, 0.0004, 0.0062, 0.0364, 0.0049, 0.0183, 0.0091, 0.0178, 0.0008, 0.0184, 0.0048, 0.0005, 0.0091, 0.0057, 0.0063, 0.0331, 0.047, 0.0162, 0.0111, 0.0002, 0.0591, 0.0113, 0.0426, 0.0057, 0.0134, 0.0065, 0.0045, 0.058, 0.0005, 0.0141, 0.0114, 0.0008, 0.0078, 0.0031, 0.0007, 0.0001, 0.0075, 0.0012, 0.0467, 0.0634, 0.0007, 0.0278, 0.0042, 0.0101, 0.011, 0.0749, 0.0146, 0.0101, 0.0191, 0.0682, 0.0327, 0.0143, 0.0154, 0.0003, 0.0181, 0.0013, 0.0227, 0.0186, 0.0202, 0.044, 0.0131, 0.0112, 0.0013, 0.0063, 0.0024, 0.0272, 0.0161, 0.0057, 0.003, 0.0114, 0.0148, 0.0109, 0.0023, 0.0001], 'AP': [0.2642, 0.0228, 0.02, 0.0041, 0.0212, 0.0004, 0.0008, 0.0018, 0.0423, 0.0112, 0.0024, 0.0209, 0.0131, 0.0453, 0.0025, 0.1231, 0.0469, 0.0519, 0.0014, 0.0029, 0.0051, 0.1247, 0.0041, 0.0003, 0.0114, 0.0024, 0.0064, 0.0359, 0.0021, 0.0287, 0.0048, 0.0055, 0.0121, 0.0067, 0.0007, 0.0002, 0.0009, 0.0007, 0.0007, 0.0057, 0.0022, 0.0045, 0.0025, 0.0082, 0.0007, 0.002, 0.0379, 0.0018, 0.002, 0.0094, 0.0009, 0.0232, 0.006, 0.0113, 0.0028, 0.0084, 0.0166, 0.0195, 0.0812, 0.0596, 0.0343, 0.0007, 0.0061, 0.0376, 0.0006, 0.0063, 0.0053, 0.001, 0.1537, 0.0047, 0.0018, 0.0055, 0.0005, 0.0139, 0.0071, 0.0097, 0.0072, 0.0008, 0.0003, 0.0007, 0.0117, 0.042, 0.0384, 0.054, 0.0219, 0.0116, 0.0043, 0.0007, 0.0009, 0.0062, 0.025, 0.0019, 0.0072, 0.0002, 0.0178, 0.0533, 0.0046, 0.0113, 0.012, 0.0012, 0.0003, 0.0051, 0.0059, 0.0001, 0.0025, 0.0041, 0.0001, 0.0005, 0.0098, 0.0111, 0.0007, 0.0002, 0.0069, 0.0514, 0.0044, 0.0002, 0.0001, 0.0013, 0.0037, 0.0003, 0.0065, 0.0244, 0.0353, 0.0079, 0.0049, 0.0302, 0.001, 0.0004, 0.0073, 0.0056, 0.0215, 0.0012, 0.006, 0.0084, 0.0028, 0.094, 0.0159, 0.0008, 0.0091, 0.0012, 0.0113, 0.005, 0.0076, 0.0026, 0.0053, 0.0009, 0.0134, 0.0004, 0.0084, 0.0058, 0.0025, 0.0056, 0.0011, 0.0001, 0.0006, 0.0099, 0.013, 0.0022, 0.0381, 0.0044, 0.0015, 0.0097, 0.016, 0.0004, 0.0012, 0.0002, 0.0006, 0.0002, 0.0004, 0.0003, 0.0015, 0.0038, 0.0422, 0.0119, 0.0058, 0.0007, 0.0004, 0.0255, 0.0069, 0.0012, 0.0052, 0.0042, 0.0008, 0.0005, 0.0092, 0.0042, 0.0001, 0.009, 0.0018, 0.0075, 0.0069, 0.0003, 0.0032, 0.0008, 0.0013, 0.0039, 0.0296, 0.0048, 0.0042, 0.0001, 0.0123, 0.0059, 0.0065, 0.0028, 0.0023, 0.0065, 0.0006, 0.0054, 0.0097, 0.0011, 0.0131, 0.0211, 0.0004, 0.0104, 0.0002, 0.0003, 0.0033, 0.0087, 0.0002, 0.0052, 0.0034, 0.0062, 0.0024, 0.0108, 0.0033, 0.0001, 0.0006, 0.0003, 0.0003, 0.0213, 0.0043, 0.0016, 0.0005, 0.0083, 0.0002, 0.026, 0.0015, 0.0052, 0.0141, 0.0014, 0.0007, 0.0108, 0.0005, 0.0002, 0.0001, 0.0035, 0.0184, 0.0029, 0.026, 0.0026, 0.0132, 0.0001, 0.0085, 0.0056, 0.0189, 0.0068, 0.0043, 0.0076, 0.0054, 0.0001, 0.0061, 0.0008, 0.0029, 0.0005, 0.0194, 0.0037, 0.0064, 0.0042, 0.0016, 0.0046, 0.0036, 0.0056, 0.0007, 0.0031, 0.096, 0.0021, 0.0004, 0.0017, 0.0477, 0.0032, 0.0001, 0.0024, 0.0126, 0.0019, 0.0087, 0.0016, 0.0058, 0.0003, 0.0084, 0.002, 0.0002, 0.0023, 0.0025, 0.0028, 0.0152, 0.022, 0.0059, 0.0044, 0.0001, 0.0195, 0.0051, 0.0168, 0.0025, 0.006, 0.0012, 0.0011, 0.0124, 0.0002, 0.0036, 0.0051, 0.0003, 0.0011, 0.0009, 0.0003, 0.0001, 0.0023, 0.0006, 0.0082, 0.0212, 0.0003, 0.0128, 0.0013, 0.0038, 0.0038, 0.026, 0.007, 0.0047, 0.0085, 0.0324, 0.0144, 0.0055, 0.0067, 0.0001, 0.0083, 0.0004, 0.0076, 0.0091, 0.0015, 0.0177, 0.0062, 0.0049, 0.0006, 0.0028, 0.0008, 0.0084, 0.0074, 0.0007, 0.0012, 0.0037, 0.0069, 0.005, 0.0009, 0.0001], 'Recall@P=50': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0006, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0071, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'micro': 0.0337, 'macro': 0.0103, 'weighted': 0.054}
2024-08-04 09:28:07 - [34m[1mLOGS   [0m - Best checkpoint with score 0.01 saved at /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_best.pt
2024-08-04 09:28:08 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt
2024-08-04 09:28:08 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_last.pt
2024-08-04 09:28:08 - [34m[1mLOGS   [0m - Training checkpoint for epoch 0/iteration 121 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_epoch_0_iter_121.pt
2024-08-04 09:28:08 - [34m[1mLOGS   [0m - Model state for epoch 0/iteration 121 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_epoch_0_iter_121.pt
[31m===========================================================================[0m
2024-08-04 09:28:10 - [32m[1mINFO   [0m - Training epoch 1
2024-08-04 09:28:12 - [34m[1mLOGS   [0m - Epoch:   1 [     122/10000000], loss: {'classification': 27.0368, 'neural_augmentation': 0.3291, 'total_loss': 27.3658}, LR: [1.3e-05, 1.3e-05], Avg. batch load time: 1.539, Elapsed time:  1.75
2024-08-04 09:28:31 - [34m[1mLOGS   [0m - *** Training summary for epoch 1
	 loss={'classification': 16.589, 'neural_augmentation': 0.3376, 'total_loss': 16.9265}
2024-08-04 09:29:00 - [34m[1mLOGS   [0m - *** Validation summary for epoch 1
	 loss={'classification': 12.661, 'neural_augmentation': 0.0, 'total_loss': 12.661} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.6968, 0.1461, 0.1344, 0.0231, 0.5157, 0.0009, 0.0015, 0.0039, 0.5898, 0.1031, 0.0025, 0.1832, 0.0381, 0.2888, 0.0101, 0.5535, 0.391, 0.4515, 0.0135, 0.013, 0.134, 0.5621, 0.0303, 0.0014, 0.3483, 0.0064, 0.0154, 0.2107, 0.0047, 0.6523, 0.1284, 0.2457, 0.0682, 0.0704, 0.0159, 0.001, 0.0012, 0.0027, 0.0088, 0.1808, 0.0232, 0.6081, 0.0058, 0.0441, 0.001, 0.0037, 0.214, 0.0488, 0.0066, 0.1658, 0.0019, 0.1445, 0.1288, 0.0355, 0.0164, 0.053, 0.0426, 0.494, 0.3941, 0.2955, 0.2299, 0.0014, 0.0294, 0.2628, 0.0049, 0.0158, 0.03, 0.0065, 0.5587, 0.0236, 0.1064, 0.0138, 0.0004, 0.1477, 0.0119, 0.3419, 0.1438, 0.0444, 0.0012, 0.0011, 0.2653, 0.2351, 0.4097, 0.2607, 0.1093, 0.0883, 0.023, 0.0241, 0.0062, 0.0474, 0.2025, 0.0039, 0.3612, 0.0005, 0.1497, 0.2653, 0.0319, 0.0421, 0.0344, 0.0023, 0.0009, 0.0772, 0.0138, 0.0007, 0.0075, 0.0067, 0.0013, 0.0021, 0.0209, 0.0777, 0.0008, 0.0014, 0.0119, 0.4629, 0.0794, 0.0006, 0.0004, 0.0167, 0.017, 0.0025, 0.0047, 0.4146, 0.2409, 0.0819, 0.0684, 0.2912, 0.0144, 0.0007, 0.0177, 0.0273, 0.3295, 0.0028, 0.2778, 0.0743, 0.0037, 0.528, 0.2348, 0.0024, 0.4168, 0.0034, 0.0796, 0.0313, 0.0909, 0.0052, 0.2342, 0.0046, 0.1833, 0.0004, 0.0236, 0.0456, 0.0127, 0.0598, 0.0365, 0.0034, 0.0016, 0.1697, 0.1627, 0.0124, 0.2149, 0.1385, 0.0115, 0.1242, 0.3623, 0.0011, 0.0036, 0.0008, 0.002, 0.0001, 0.0095, 0.0007, 0.0048, 0.0222, 0.2474, 0.3538, 0.0311, 0.0029, 0.0016, 0.1278, 0.0642, 0.0092, 0.1232, 0.053, 0.0094, 0.0036, 0.1281, 0.0629, 0.0003, 0.0252, 0.0048, 0.0615, 0.1667, 0.0016, 0.0108, 0.002, 0.029, 0.0098, 0.1226, 0.013, 0.0572, 0.0003, 0.1035, 0.0662, 0.0456, 0.0092, 0.0066, 0.1997, 0.0019, 0.2019, 0.1586, 0.0016, 0.1027, 0.4512, 0.0024, 0.0096, 0.0004, 0.0008, 0.011, 0.4181, 0.0022, 0.1216, 0.0075, 0.162, 0.008, 0.5146, 0.0157, 0.0003, 0.0021, 0.001, 0.0002, 0.1969, 0.1578, 0.007, 0.0015, 0.1041, 0.0029, 0.4244, 0.0069, 0.2562, 0.0486, 0.0183, 0.0014, 0.1463, 0.0019, 0.0004, 0.0025, 0.0088, 0.3261, 0.0053, 0.1801, 0.0101, 0.0614, 0.0011, 0.0933, 0.0161, 0.2423, 0.5472, 0.0115, 0.0813, 0.0068, 0.0008, 0.643, 0.0026, 0.0106, 0.0013, 0.3034, 0.0116, 0.0138, 0.0906, 0.0083, 0.0527, 0.0235, 0.0118, 0.0022, 0.0571, 0.5512, 0.0095, 0.0172, 0.0041, 0.2454, 0.0137, 0.0001, 0.0132, 0.0245, 0.0046, 0.4387, 0.0054, 0.1092, 0.0005, 0.2791, 0.0267, 0.0015, 0.0267, 0.0297, 0.0236, 0.064, 0.2625, 0.0741, 0.0505, 0.0008, 0.3189, 0.0165, 0.4435, 0.0057, 0.0602, 0.0012, 0.016, 0.0368, 0.0144, 0.0029, 0.0652, 0.0008, 0.0027, 0.0009, 0.0007, 0.0004, 0.011, 0.0023, 0.0474, 0.0731, 0.0006, 0.0227, 0.0017, 0.1963, 0.0805, 0.2419, 0.1148, 0.1127, 0.0389, 0.4373, 0.5072, 0.0495, 0.1578, 0.0003, 0.0429, 0.001, 0.3527, 0.1294, 0.0054, 0.2303, 0.1514, 0.2496, 0.0012, 0.0076, 0.0027, 0.0541, 0.0548, 0.0008, 0.0117, 0.0139, 0.0504, 0.2011, 0.0016, 0.0002], 'AP': [0.7434, 0.0726, 0.0737, 0.0095, 0.5163, 0.0003, 0.0007, 0.0016, 0.5967, 0.0426, 0.001, 0.1088, 0.0175, 0.2457, 0.0041, 0.583, 0.3507, 0.4135, 0.0026, 0.0043, 0.0635, 0.5889, 0.013, 0.0005, 0.2317, 0.0027, 0.0068, 0.1241, 0.0021, 0.6363, 0.0541, 0.1561, 0.0263, 0.0284, 0.0023, 0.0004, 0.0004, 0.001, 0.0009, 0.0733, 0.0064, 0.6414, 0.0027, 0.0184, 0.0004, 0.0014, 0.1423, 0.0107, 0.0023, 0.0777, 0.0004, 0.0893, 0.0702, 0.0166, 0.0053, 0.0198, 0.0199, 0.4974, 0.3177, 0.2247, 0.121, 0.0006, 0.0123, 0.2106, 0.0009, 0.0061, 0.0128, 0.0021, 0.5653, 0.0091, 0.0404, 0.0057, 0.0001, 0.0591, 0.0048, 0.2731, 0.0641, 0.0068, 0.0004, 0.0004, 0.1488, 0.181, 0.3815, 0.168, 0.0573, 0.0491, 0.0081, 0.0023, 0.0023, 0.0151, 0.1189, 0.0016, 0.2563, 0.0002, 0.0781, 0.1869, 0.0099, 0.0167, 0.0126, 0.0009, 0.0004, 0.0306, 0.0061, 0.0002, 0.0027, 0.0027, 0.0004, 0.0006, 0.0088, 0.037, 0.0004, 0.0003, 0.0054, 0.4553, 0.0286, 0.0002, 0.0002, 0.005, 0.0068, 0.0008, 0.0017, 0.3701, 0.1509, 0.0372, 0.0261, 0.1784, 0.0027, 0.0002, 0.0071, 0.0107, 0.2504, 0.001, 0.1872, 0.0346, 0.0014, 0.511, 0.1726, 0.0009, 0.3582, 0.0007, 0.0368, 0.0132, 0.0381, 0.0017, 0.1374, 0.0018, 0.1107, 0.0002, 0.0106, 0.0183, 0.0051, 0.02, 0.0125, 0.0012, 0.0006, 0.0811, 0.0868, 0.0035, 0.1359, 0.057, 0.0015, 0.0513, 0.2691, 0.0004, 0.0013, 0.0003, 0.0007, 0.0, 0.0009, 0.0003, 0.0018, 0.0097, 0.1588, 0.2932, 0.0139, 0.0011, 0.0006, 0.0536, 0.0257, 0.0014, 0.061, 0.019, 0.0014, 0.001, 0.0648, 0.018, 0.0001, 0.0087, 0.0016, 0.0218, 0.0791, 0.0004, 0.005, 0.0006, 0.0051, 0.0036, 0.0517, 0.0053, 0.0191, 0.0001, 0.0409, 0.0273, 0.0168, 0.0042, 0.0022, 0.0878, 0.0007, 0.1029, 0.0801, 0.0006, 0.046, 0.4431, 0.0009, 0.0039, 0.0001, 0.0003, 0.0046, 0.3617, 0.0007, 0.0485, 0.003, 0.0904, 0.0033, 0.5147, 0.0046, 0.0001, 0.0008, 0.0004, 0.0001, 0.1126, 0.065, 0.0023, 0.0003, 0.0464, 0.0005, 0.4094, 0.0027, 0.1689, 0.0204, 0.0016, 0.0005, 0.0913, 0.0007, 0.0001, 0.0012, 0.0034, 0.2732, 0.0021, 0.1021, 0.0038, 0.0298, 0.0004, 0.0414, 0.0058, 0.1563, 0.5972, 0.0046, 0.0313, 0.0026, 0.0003, 0.6621, 0.001, 0.0027, 0.0004, 0.1884, 0.005, 0.0058, 0.0363, 0.002, 0.0226, 0.0056, 0.0052, 0.0008, 0.0185, 0.5206, 0.0036, 0.0014, 0.0014, 0.1575, 0.0059, 0.0001, 0.0048, 0.0087, 0.0016, 0.359, 0.0023, 0.0517, 0.0002, 0.1845, 0.0095, 0.0005, 0.0073, 0.0116, 0.0063, 0.0274, 0.2115, 0.0227, 0.018, 0.0002, 0.245, 0.0073, 0.3686, 0.0022, 0.0259, 0.0004, 0.0053, 0.0133, 0.005, 0.0009, 0.0259, 0.0003, 0.001, 0.0004, 0.0003, 0.0002, 0.0039, 0.0007, 0.0157, 0.028, 0.0002, 0.0079, 0.0005, 0.1064, 0.0292, 0.1477, 0.044, 0.0402, 0.0164, 0.3889, 0.4868, 0.0177, 0.0737, 0.0001, 0.018, 0.0004, 0.2554, 0.0569, 0.0021, 0.1668, 0.0599, 0.123, 0.0005, 0.0034, 0.0011, 0.0233, 0.0219, 0.0002, 0.0039, 0.0054, 0.0174, 0.1128, 0.0006, 0.0001], 'Recall@P=50': [0.8591, 0.0, 0.0014, 0.0, 0.4989, 0.0, 0.0, 0.0, 0.0012, 0.0027, 0.0, 0.0072, 0.0, 0.1152, 0.0, 0.6158, 0.2671, 0.3242, 0.0, 0.0, 0.0, 0.6294, 0.0, 0.0, 0.0264, 0.0, 0.0, 0.0, 0.0, 0.7462, 0.0, 0.0075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4614, 0.0043, 0.0036, 0.0191, 0.0, 0.0, 0.0389, 0.0, 0.0, 0.0, 0.0, 0.6277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2172, 0.0, 0.0, 0.0, 0.0, 0.0033, 0.0664, 0.3051, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0015, 0.0, 0.0063, 0.0, 0.0, 0.0015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3159, 0.0009, 0.0, 0.0, 0.0009, 0.0, 0.0, 0.0, 0.0, 0.0149, 0.0, 0.0569, 0.0039, 0.0, 0.5515, 0.14, 0.0, 0.309, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0231, 0.0, 0.0072, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0024, 0.0, 0.0, 0.0, 0.0022, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0031, 0.1968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3992, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0189, 0.0, 0.0, 0.0, 0.0452, 0.0, 0.5053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3611, 0.0, 0.0101, 0.0, 0.0, 0.0, 0.0108, 0.0, 0.0, 0.0, 0.0, 0.176, 0.0, 0.0013, 0.0, 0.0019, 0.0, 0.0, 0.0, 0.0028, 0.5885, 0.0, 0.0, 0.0, 0.0, 0.8146, 0.0, 0.0, 0.0, 0.021, 0.0, 0.0, 0.0068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6107, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0035, 0.0, 0.0034, 0.0, 0.0312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0949, 0.0, 0.0, 0.0, 0.0541, 0.0, 0.0029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0119, 0.0, 0.0455, 0.0, 0.0, 0.0, 0.3748, 0.5115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0741, 0.0, 0.0, 0.0089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.028, 0.0, 0.0], 'micro': 0.2566, 'macro': 0.0725, 'weighted': 0.2615}
2024-08-04 09:29:15 - [34m[1mLOGS   [0m - Best checkpoint with score 0.07 saved at /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_best.pt
2024-08-04 09:29:16 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt
2024-08-04 09:29:16 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_last.pt
2024-08-04 09:29:16 - [34m[1mLOGS   [0m - Training checkpoint for epoch 1/iteration 240 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_epoch_1_iter_240.pt
2024-08-04 09:29:16 - [34m[1mLOGS   [0m - Model state for epoch 1/iteration 240 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_epoch_1_iter_240.pt
[31m===========================================================================[0m
2024-08-04 09:29:18 - [32m[1mINFO   [0m - Training epoch 2
2024-08-04 09:29:21 - [34m[1mLOGS   [0m - Epoch:   2 [     241/10000000], loss: {'classification': 12.9256, 'neural_augmentation': 0.3346, 'total_loss': 13.2602}, LR: [2.5e-05, 2.5e-05], Avg. batch load time: 2.370, Elapsed time:  2.59
2024-08-04 09:29:40 - [34m[1mLOGS   [0m - *** Training summary for epoch 2
	 loss={'classification': 10.6503, 'neural_augmentation': 0.3336, 'total_loss': 10.9838}
2024-08-04 09:30:10 - [34m[1mLOGS   [0m - *** Validation summary for epoch 2
	 loss={'classification': 8.0463, 'neural_augmentation': 0.0, 'total_loss': 8.0463} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.7667, 0.5795, 0.267, 0.4388, 0.7534, 0.0008, 0.0127, 0.0345, 0.6965, 0.3344, 0.0207, 0.5893, 0.5176, 0.5417, 0.1829, 0.6526, 0.5162, 0.6315, 0.0356, 0.2105, 0.7304, 0.753, 0.6824, 0.0038, 0.7215, 0.0372, 0.2559, 0.4151, 0.0168, 0.8094, 0.5106, 0.7573, 0.5548, 0.8168, 0.0131, 0.0833, 0.0009, 0.0417, 0.0182, 0.6179, 0.48, 0.9016, 0.0201, 0.1592, 0.001, 0.0051, 0.6422, 0.18, 0.0193, 0.6697, 0.0037, 0.5556, 0.8883, 0.4608, 0.0844, 0.6643, 0.4682, 0.648, 0.611, 0.4425, 0.4929, 0.0015, 0.5014, 0.6852, 0.0008, 0.0657, 0.2051, 0.0112, 0.8634, 0.4121, 0.2645, 0.1869, 0.001, 0.5649, 0.1784, 0.8821, 0.7473, 0.0769, 0.0073, 0.005, 0.5135, 0.6509, 0.7143, 0.6179, 0.4405, 0.5373, 0.1025, 0.0115, 0.0137, 0.5553, 0.4718, 0.0178, 0.7329, 0.0011, 0.234, 0.5322, 0.063, 0.4543, 0.2363, 0.0049, 0.0039, 0.5759, 0.407, 0.0013, 0.1919, 0.6957, 0.0011, 0.0025, 0.3397, 0.6133, 0.0036, 0.0071, 0.3889, 0.887, 0.5952, 0.0006, 0.0005, 0.0816, 0.8072, 0.0035, 0.4681, 0.779, 0.6353, 0.5109, 0.6188, 0.6764, 0.0011, 0.0008, 0.1705, 0.2809, 0.7421, 0.0031, 0.8858, 0.373, 0.0255, 0.7109, 0.4349, 0.0089, 0.6916, 0.0069, 0.3582, 0.3032, 0.7458, 0.0584, 0.8253, 0.0609, 0.5797, 0.0017, 0.3043, 0.605, 0.4718, 0.2762, 0.0533, 0.0006, 0.0018, 0.7925, 0.7561, 0.1527, 0.5651, 0.7871, 0.036, 0.7133, 0.7541, 0.001, 0.0021, 0.125, 0.0741, 0.0001, 0.0032, 0.0006, 0.1635, 0.6704, 0.6374, 0.6457, 0.5322, 0.0023, 0.0009, 0.6762, 0.3266, 0.087, 0.6823, 0.5782, 0.0021, 0.0035, 0.711, 0.1991, 0.0004, 0.3459, 0.0197, 0.7188, 0.7851, 0.0005, 0.2773, 0.002, 0.0203, 0.1579, 0.6018, 0.1619, 0.6785, 0.0004, 0.8014, 0.6506, 0.249, 0.0433, 0.4513, 0.893, 0.0047, 0.5352, 0.447, 0.0024, 0.6278, 0.8987, 0.0019, 0.7803, 0.0006, 0.002, 0.3002, 0.7438, 0.0014, 0.661, 0.2418, 0.7642, 0.5464, 0.8994, 0.3853, 0.0054, 0.0286, 0.0013, 0.0006, 0.5164, 0.7593, 0.0661, 0.0036, 0.6108, 0.0005, 0.7225, 0.0371, 0.7755, 0.7943, 0.0952, 0.0013, 0.3738, 0.0038, 0.0003, 0.0008, 0.5323, 0.7816, 0.0186, 0.6691, 0.4066, 0.7114, 0.0019, 0.6805, 0.1746, 0.5608, 0.9224, 0.7082, 0.7086, 0.028, 0.0007, 0.8718, 0.0838, 0.0732, 0.0009, 0.446, 0.2884, 0.3594, 0.4833, 0.0303, 0.7285, 0.1029, 0.0633, 0.022, 0.5648, 0.8011, 0.4366, 0.029, 0.0145, 0.5664, 0.3585, 0.0002, 0.0903, 0.5342, 0.0611, 0.8348, 0.1071, 0.4104, 0.0005, 0.6967, 0.375, 0.0008, 0.0265, 0.5551, 0.1765, 0.533, 0.7625, 0.2533, 0.5, 0.0006, 0.7181, 0.7866, 0.6304, 0.2395, 0.6439, 0.0333, 0.1951, 0.3182, 0.0015, 0.0012, 0.5549, 0.0006, 0.0371, 0.0007, 0.0016, 0.0003, 0.0399, 0.0144, 0.3497, 0.7063, 0.0087, 0.7116, 0.002, 0.8391, 0.5249, 0.7203, 0.3929, 0.3675, 0.3399, 0.6275, 0.8464, 0.6842, 0.8633, 0.0054, 0.2996, 0.0089, 0.8514, 0.7047, 0.1084, 0.6746, 0.7444, 0.6368, 0.0013, 0.07, 0.0054, 0.5957, 0.81, 0.0008, 0.0857, 0.7018, 0.5995, 0.814, 0.0092, 0.0005], 'AP': [0.8351, 0.5708, 0.1784, 0.3972, 0.7191, 0.0003, 0.0023, 0.0056, 0.6964, 0.2893, 0.0047, 0.5687, 0.5082, 0.5593, 0.0783, 0.7326, 0.5185, 0.6782, 0.007, 0.0779, 0.749, 0.8214, 0.6184, 0.0005, 0.7021, 0.0079, 0.1432, 0.3523, 0.0043, 0.8559, 0.5036, 0.8149, 0.512, 0.8367, 0.0032, 0.0081, 0.0003, 0.0045, 0.002, 0.6149, 0.4394, 0.8839, 0.0053, 0.0858, 0.0003, 0.0019, 0.6605, 0.0684, 0.0069, 0.7139, 0.0008, 0.5731, 0.9487, 0.4313, 0.0269, 0.6752, 0.4401, 0.6855, 0.6439, 0.4181, 0.4612, 0.0006, 0.4771, 0.7327, 0.0002, 0.022, 0.1041, 0.0039, 0.9254, 0.3212, 0.1614, 0.0789, 0.0004, 0.5549, 0.1055, 0.9264, 0.7915, 0.0144, 0.0011, 0.0008, 0.4457, 0.6956, 0.7735, 0.6626, 0.4097, 0.5138, 0.0437, 0.0038, 0.0043, 0.5756, 0.4687, 0.0056, 0.7778, 0.0004, 0.1582, 0.5509, 0.0199, 0.418, 0.1474, 0.0021, 0.0008, 0.5718, 0.3497, 0.0005, 0.0871, 0.6928, 0.0003, 0.0009, 0.2609, 0.6286, 0.0007, 0.001, 0.3189, 0.9275, 0.5681, 0.0002, 0.0002, 0.0221, 0.8265, 0.001, 0.3277, 0.8436, 0.6731, 0.5265, 0.6332, 0.7191, 0.0005, 0.0003, 0.0784, 0.1672, 0.7958, 0.0011, 0.9359, 0.3395, 0.0039, 0.7617, 0.3864, 0.0026, 0.7126, 0.0016, 0.2805, 0.2056, 0.7418, 0.0172, 0.8661, 0.0296, 0.5926, 0.0004, 0.2404, 0.5775, 0.3613, 0.1909, 0.0205, 0.0002, 0.0007, 0.8325, 0.7653, 0.0505, 0.6057, 0.7746, 0.0086, 0.7483, 0.765, 0.0004, 0.0007, 0.0325, 0.0098, 0.0, 0.001, 0.0002, 0.0697, 0.7107, 0.6474, 0.6838, 0.5354, 0.001, 0.0003, 0.7485, 0.1888, 0.0192, 0.744, 0.5805, 0.0009, 0.0011, 0.7304, 0.1096, 0.0002, 0.2807, 0.0073, 0.7605, 0.8151, 0.0002, 0.1538, 0.0007, 0.0047, 0.0721, 0.6216, 0.0825, 0.7014, 0.0002, 0.8285, 0.6868, 0.1578, 0.0141, 0.4405, 0.9532, 0.0015, 0.5148, 0.4456, 0.0009, 0.6588, 0.9314, 0.0007, 0.7875, 0.0002, 0.0005, 0.2103, 0.813, 0.0004, 0.6116, 0.1779, 0.8126, 0.4837, 0.8967, 0.2317, 0.0025, 0.0019, 0.0005, 0.0003, 0.5208, 0.7965, 0.0309, 0.0005, 0.5874, 0.0002, 0.7763, 0.0108, 0.7767, 0.7866, 0.0528, 0.0005, 0.342, 0.0014, 0.0001, 0.0004, 0.4906, 0.8508, 0.0067, 0.6892, 0.2897, 0.7787, 0.0006, 0.7441, 0.0639, 0.5143, 0.9657, 0.7393, 0.7614, 0.0089, 0.0003, 0.9363, 0.0189, 0.0189, 0.0003, 0.3977, 0.1835, 0.2329, 0.4433, 0.0076, 0.7995, 0.0314, 0.0155, 0.0063, 0.5543, 0.8582, 0.366, 0.0018, 0.0028, 0.5672, 0.2288, 0.0001, 0.0461, 0.5387, 0.0144, 0.9084, 0.0329, 0.3287, 0.0002, 0.7606, 0.2787, 0.0003, 0.0071, 0.4279, 0.0874, 0.5566, 0.8257, 0.1255, 0.4584, 0.0002, 0.7572, 0.7902, 0.5932, 0.1631, 0.6827, 0.0022, 0.113, 0.2015, 0.0006, 0.0004, 0.5154, 0.0002, 0.009, 0.0002, 0.0006, 0.0001, 0.0108, 0.0027, 0.2727, 0.7684, 0.0016, 0.7115, 0.0008, 0.897, 0.4845, 0.7471, 0.2916, 0.2875, 0.2409, 0.652, 0.8794, 0.6928, 0.9152, 0.0013, 0.2206, 0.0014, 0.9066, 0.7361, 0.0361, 0.6828, 0.8051, 0.6685, 0.0005, 0.0258, 0.0015, 0.535, 0.8475, 0.0002, 0.0304, 0.7553, 0.6021, 0.873, 0.0009, 0.0003], 'Recall@P=50': [0.9372, 0.6006, 0.0055, 0.3681, 0.8565, 0.0, 0.0, 0.0, 0.0012, 0.2384, 0.0, 0.618, 0.4935, 0.5655, 0.0, 0.7991, 0.5308, 0.7318, 0.0, 0.0202, 0.7982, 0.9002, 0.6952, 0.0, 0.6673, 0.0, 0.0047, 0.0007, 0.0, 0.8872, 0.505, 0.8545, 0.5746, 0.8471, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6441, 0.0192, 0.8756, 0.0, 0.0066, 0.0, 0.0, 0.7206, 0.0, 0.0, 0.7147, 0.0, 0.5921, 0.9864, 0.4154, 0.0, 0.7844, 0.4124, 0.7661, 0.7316, 0.3726, 0.4268, 0.0, 0.4901, 0.7717, 0.0, 0.0, 0.0058, 0.0, 0.978, 0.2105, 0.0328, 0.0, 0.0, 0.5743, 0.0459, 0.9548, 0.7931, 0.0, 0.0, 0.0, 0.482, 0.7406, 0.83, 0.704, 0.0013, 0.549, 0.0, 0.0, 0.0, 0.5866, 0.4206, 0.0, 0.836, 0.0, 0.0343, 0.5382, 0.0, 0.3834, 0.0096, 0.0, 0.0, 0.6193, 0.281, 0.0, 0.0, 0.7849, 0.0, 0.0, 0.1228, 0.617, 0.0, 0.0, 0.2101, 0.0006, 0.5926, 0.0, 0.0, 0.0, 0.8602, 0.0, 0.0606, 0.8991, 0.746, 0.5113, 0.6724, 0.7341, 0.0, 0.0, 0.0083, 0.0, 0.8503, 0.0, 0.9609, 0.2802, 0.0, 0.881, 0.0023, 0.0, 0.7376, 0.0, 0.0035, 0.1311, 0.8119, 0.0, 0.9075, 0.0263, 0.6468, 0.0, 0.0968, 0.6736, 0.1321, 0.1469, 0.0, 0.0, 0.0, 0.865, 0.8, 0.0, 0.6026, 0.767, 0.0, 0.8267, 0.7533, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0351, 0.7558, 0.7329, 0.6938, 0.5226, 0.0, 0.0, 0.8537, 0.0042, 0.0, 0.8261, 0.6186, 0.0, 0.0, 0.804, 0.0183, 0.0, 0.0084, 0.0, 0.794, 0.8421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6443, 0.0202, 0.8415, 0.0, 0.8224, 0.7027, 0.124, 0.0, 0.3922, 0.9767, 0.0, 0.5213, 0.3704, 0.0, 0.6776, 0.9435, 0.0, 0.8054, 0.0, 0.0, 0.009, 0.87, 0.0, 0.6239, 0.0702, 0.8342, 0.5375, 0.8842, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5141, 0.8107, 0.0, 0.0, 0.6379, 0.0, 0.854, 0.0, 0.8894, 0.8449, 0.05, 0.0, 0.2243, 0.0, 0.0, 0.0, 0.5145, 0.9111, 0.0, 0.739, 0.0308, 0.8438, 0.0, 0.8267, 0.0, 0.5904, 0.9835, 0.75, 0.7951, 0.0, 0.0, 0.9951, 0.0, 0.0, 0.0, 0.3753, 0.0092, 0.0748, 0.4452, 0.0, 0.8654, 0.0, 0.0, 0.0, 0.6, 0.9731, 0.274, 0.0, 0.0, 0.6266, 0.0645, 0.0, 0.0202, 0.4854, 0.0, 0.9582, 0.0, 0.1544, 0.0, 0.8672, 0.0536, 0.0, 0.0, 0.4725, 0.0, 0.5574, 0.8613, 0.0, 0.4539, 0.0, 0.777, 0.7883, 0.7543, 0.0106, 0.7433, 0.0, 0.0, 0.0167, 0.0, 0.0, 0.6023, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2281, 0.8365, 0.0, 0.7929, 0.0, 0.9167, 0.551, 0.7692, 0.0439, 0.1465, 0.1086, 0.6676, 0.9049, 0.7689, 0.9609, 0.0, 0.0763, 0.0, 0.9506, 0.7967, 0.0, 0.7143, 0.8686, 0.7371, 0.0, 0.0, 0.0, 0.6598, 0.8584, 0.0, 0.0, 0.7906, 0.6168, 0.9091, 0.0, 0.0], 'micro': 0.6136, 'macro': 0.337, 'weighted': 0.6233}
2024-08-04 09:30:23 - [34m[1mLOGS   [0m - Best checkpoint with score 0.34 saved at /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_best.pt
2024-08-04 09:30:23 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt
2024-08-04 09:30:23 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_last.pt
2024-08-04 09:30:24 - [34m[1mLOGS   [0m - Training checkpoint for epoch 2/iteration 361 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_epoch_2_iter_361.pt
2024-08-04 09:30:24 - [34m[1mLOGS   [0m - Model state for epoch 2/iteration 361 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_epoch_2_iter_361.pt
[31m===========================================================================[0m
2024-08-04 09:30:26 - [32m[1mINFO   [0m - Training epoch 3
2024-08-04 09:30:28 - [34m[1mLOGS   [0m - Epoch:   3 [     362/10000000], loss: {'classification': 8.4655, 'neural_augmentation': 0.3122, 'total_loss': 8.7777}, LR: [3.6e-05, 3.6e-05], Avg. batch load time: 2.281, Elapsed time:  2.49
2024-08-04 09:30:48 - [34m[1mLOGS   [0m - *** Training summary for epoch 3
	 loss={'classification': 7.2134, 'neural_augmentation': 0.3263, 'total_loss': 7.5397}
2024-08-04 09:31:19 - [34m[1mLOGS   [0m - *** Validation summary for epoch 3
	 loss={'classification': 5.6726, 'neural_augmentation': 0.0, 'total_loss': 5.6726} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.7923, 0.6275, 0.3724, 0.7886, 0.7836, 0.0028, 0.0038, 0.16, 0.7207, 0.4818, 0.2222, 0.6809, 0.6469, 0.6133, 0.6825, 0.6968, 0.597, 0.6792, 0.011, 0.2679, 0.8668, 0.791, 0.7308, 0.004, 0.7584, 0.0779, 0.3611, 0.5029, 0.0485, 0.8289, 0.7195, 0.8647, 0.6689, 0.8917, 0.0291, 0.069, 0.0007, 0.0175, 0.0909, 0.734, 0.6981, 0.9316, 0.0332, 0.2641, 0.0011, 0.3944, 0.7223, 0.337, 0.0924, 0.7418, 0.0162, 0.7644, 0.9543, 0.5835, 0.1825, 0.8015, 0.6459, 0.7505, 0.664, 0.4973, 0.6446, 0.0138, 0.7416, 0.7704, 0.0023, 0.1151, 0.3496, 0.2154, 0.906, 0.6214, 0.5965, 0.5801, 0.0017, 0.766, 0.5772, 0.9443, 0.9241, 0.1639, 0.0027, 0.08, 0.6513, 0.7662, 0.8047, 0.7475, 0.6043, 0.6928, 0.4953, 0.3, 0.2439, 0.8065, 0.6046, 0.1892, 0.7884, 0.0083, 0.3538, 0.608, 0.5405, 0.6236, 0.5477, 0.1389, 0.0175, 0.7889, 0.791, 0.0351, 0.7343, 0.9392, 0.0005, 0.0112, 0.7869, 0.7701, 0.004, 0.0076, 0.5517, 0.9155, 0.8062, 0.0009, 0.0021, 0.4156, 0.9503, 0.0357, 0.596, 0.8897, 0.7781, 0.6644, 0.8862, 0.7952, 0.0021, 0.0041, 0.7019, 0.4293, 0.8181, 0.0035, 0.9474, 0.6217, 0.1111, 0.7741, 0.5417, 0.1912, 0.7769, 0.0262, 0.5155, 0.7263, 0.8057, 0.2857, 0.9181, 0.1333, 0.7672, 0.0007, 0.7, 0.8478, 0.7435, 0.6537, 0.2553, 0.0016, 0.0052, 0.9012, 0.8241, 0.5614, 0.738, 0.8554, 0.2647, 0.8, 0.8128, 0.0039, 0.0037, 0.2857, 0.0087, 0.0001, 0.0187, 0.0011, 0.3265, 0.8631, 0.7675, 0.8176, 0.7049, 0.0021, 0.1053, 0.7969, 0.5646, 0.4, 0.8839, 0.8317, 0.0115, 0.0054, 0.8041, 0.6512, 0.0003, 0.6838, 0.5311, 0.8731, 0.8927, 0.0034, 0.4955, 0.0149, 0.2021, 0.343, 0.7366, 0.6734, 0.8511, 0.0007, 0.8991, 0.8701, 0.5617, 0.3208, 0.8729, 0.9631, 0.0517, 0.7709, 0.7053, 0.0424, 0.8336, 0.9446, 0.0025, 0.8935, 0.0005, 0.0042, 0.8571, 0.878, 0.0003, 0.7565, 0.4121, 0.9125, 0.6341, 0.9168, 0.4848, 0.2222, 0.0833, 0.038, 0.0022, 0.6112, 0.8867, 0.3185, 0.0005, 0.6463, 0.0004, 0.791, 0.2857, 0.866, 0.8548, 0.0759, 0.0014, 0.5041, 0.0115, 0.0003, 0.0019, 0.7757, 0.8577, 0.0557, 0.7988, 0.5326, 0.8153, 0.0115, 0.8053, 0.383, 0.6765, 0.9651, 0.9008, 0.8491, 0.3053, 0.0022, 0.9125, 0.5172, 0.4186, 0.0007, 0.4595, 0.8155, 0.8173, 0.6, 0.1311, 0.8961, 0.2819, 0.2, 0.0407, 0.8908, 0.8895, 0.5495, 0.0028, 0.0233, 0.7267, 0.7164, 0.0004, 0.201, 0.8466, 0.3, 0.9036, 0.3368, 0.7029, 0.0008, 0.8968, 0.7521, 0.0003, 0.4865, 0.6946, 0.7111, 0.7597, 0.8877, 0.4123, 0.7698, 0.0006, 0.9141, 0.8923, 0.6783, 0.3429, 0.7865, 0.129, 0.4595, 0.8269, 0.0015, 0.0012, 0.7389, 0.0013, 0.1194, 0.003, 0.0026, 0.0018, 0.1074, 0.0049, 0.7083, 0.9091, 0.0038, 0.9231, 0.4407, 0.9294, 0.7857, 0.8417, 0.5628, 0.6737, 0.6013, 0.7609, 0.9079, 0.8079, 0.9336, 0.0645, 0.6653, 0.4667, 0.9148, 0.8456, 0.3774, 0.8706, 0.8746, 0.8746, 0.0038, 0.5806, 0.087, 0.7478, 0.8863, 0.001, 0.3276, 0.8775, 0.7752, 0.9429, 0.0046, 0.001], 'AP': [0.8622, 0.6573, 0.3121, 0.808, 0.7639, 0.0008, 0.0012, 0.0804, 0.7648, 0.4607, 0.1062, 0.6902, 0.6805, 0.6429, 0.6402, 0.7822, 0.6334, 0.7491, 0.003, 0.122, 0.8825, 0.8636, 0.6657, 0.0006, 0.7864, 0.0156, 0.2705, 0.4612, 0.0133, 0.8779, 0.7174, 0.9155, 0.682, 0.8993, 0.0054, 0.0065, 0.0003, 0.0053, 0.0286, 0.7129, 0.7167, 0.9285, 0.0083, 0.1795, 0.0003, 0.2747, 0.7551, 0.1989, 0.0389, 0.8211, 0.0031, 0.8146, 0.9806, 0.6055, 0.0892, 0.8007, 0.6625, 0.7773, 0.7074, 0.5, 0.6469, 0.0019, 0.7939, 0.8341, 0.0004, 0.0486, 0.2258, 0.0831, 0.9615, 0.6228, 0.5405, 0.5286, 0.0006, 0.816, 0.5427, 0.9762, 0.9708, 0.0709, 0.0007, 0.0153, 0.6838, 0.8377, 0.8677, 0.7977, 0.6002, 0.7161, 0.4126, 0.1694, 0.1018, 0.8594, 0.6567, 0.0944, 0.8461, 0.0015, 0.2972, 0.6524, 0.4926, 0.6052, 0.5029, 0.0368, 0.0038, 0.8226, 0.8381, 0.0093, 0.754, 0.9614, 0.0002, 0.0017, 0.84, 0.8223, 0.0013, 0.0011, 0.4705, 0.9589, 0.8009, 0.0004, 0.0007, 0.2826, 0.9621, 0.0032, 0.4408, 0.9381, 0.8271, 0.707, 0.9325, 0.8421, 0.0007, 0.0015, 0.7379, 0.4165, 0.8778, 0.0014, 0.9698, 0.6328, 0.0304, 0.8266, 0.5167, 0.0823, 0.805, 0.0064, 0.5001, 0.7627, 0.7994, 0.2283, 0.9341, 0.0407, 0.83, 0.0003, 0.7487, 0.9216, 0.8062, 0.6819, 0.1415, 0.0006, 0.002, 0.9381, 0.8448, 0.4604, 0.8134, 0.8534, 0.176, 0.8539, 0.8549, 0.0014, 0.0013, 0.0855, 0.0019, 0.0001, 0.0037, 0.0004, 0.2171, 0.9366, 0.8098, 0.871, 0.7421, 0.0008, 0.0163, 0.8688, 0.513, 0.37, 0.9461, 0.9, 0.0031, 0.0015, 0.8442, 0.6417, 0.0001, 0.7366, 0.381, 0.912, 0.929, 0.0008, 0.433, 0.0017, 0.1461, 0.3063, 0.7657, 0.6416, 0.91, 0.0003, 0.9025, 0.9158, 0.6132, 0.1678, 0.9172, 0.9899, 0.0093, 0.8115, 0.7099, 0.0094, 0.8767, 0.9477, 0.0009, 0.9292, 0.0002, 0.0009, 0.9052, 0.9089, 0.0001, 0.7898, 0.3127, 0.9346, 0.5453, 0.9309, 0.3814, 0.0756, 0.0145, 0.0057, 0.0008, 0.6587, 0.9016, 0.1652, 0.0002, 0.6786, 0.0001, 0.8494, 0.1822, 0.8029, 0.8751, 0.0208, 0.0006, 0.5115, 0.0035, 0.0001, 0.001, 0.8421, 0.9171, 0.0187, 0.8507, 0.3843, 0.8787, 0.0019, 0.8826, 0.2895, 0.6917, 0.9882, 0.9485, 0.9065, 0.187, 0.0006, 0.9551, 0.4331, 0.346, 0.0003, 0.4322, 0.868, 0.8702, 0.5791, 0.03, 0.9545, 0.1112, 0.0825, 0.0125, 0.9121, 0.9405, 0.5504, 0.0004, 0.0051, 0.7838, 0.7583, 0.0002, 0.0905, 0.894, 0.1775, 0.9631, 0.2515, 0.7288, 0.0003, 0.9327, 0.693, 0.0001, 0.4406, 0.5986, 0.7307, 0.8302, 0.9312, 0.2964, 0.7993, 0.0002, 0.9476, 0.9122, 0.7013, 0.2756, 0.8567, 0.051, 0.3792, 0.8198, 0.0006, 0.0003, 0.7914, 0.0003, 0.0406, 0.0011, 0.0007, 0.0008, 0.0304, 0.0019, 0.7792, 0.9692, 0.0011, 0.9624, 0.4229, 0.9743, 0.8546, 0.8812, 0.5338, 0.665, 0.6207, 0.8023, 0.9324, 0.8698, 0.9766, 0.0087, 0.7012, 0.3522, 0.9582, 0.8829, 0.2975, 0.9121, 0.9079, 0.9299, 0.0014, 0.6215, 0.019, 0.7202, 0.9445, 0.0004, 0.227, 0.9191, 0.8189, 0.9701, 0.0014, 0.0003], 'Recall@P=50': [0.9487, 0.6757, 0.2011, 0.9097, 0.8656, 0.0, 0.0, 0.0426, 0.8815, 0.4493, 0.0244, 0.7654, 0.6744, 0.6837, 0.6842, 0.8571, 0.6775, 0.8331, 0.0, 0.0202, 0.9128, 0.936, 0.738, 0.0, 0.7769, 0.0, 0.0093, 0.0015, 0.0, 0.8829, 0.7426, 0.9366, 0.7524, 0.9059, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7252, 0.8654, 0.9204, 0.0, 0.033, 0.0, 0.1277, 0.8629, 0.0781, 0.0116, 0.8532, 0.0, 0.8819, 0.991, 0.6, 0.0316, 0.8662, 0.7172, 0.8841, 0.8053, 0.0009, 0.707, 0.0, 0.8812, 0.8883, 0.0, 0.0, 0.0877, 0.0, 0.9911, 0.6608, 0.623, 0.5652, 0.0, 0.8434, 0.5357, 0.9864, 0.9931, 0.0, 0.0, 0.0, 0.777, 0.9053, 0.905, 0.8808, 0.6692, 0.7389, 0.447, 0.0278, 0.0455, 0.8939, 0.7118, 0.05, 0.918, 0.0, 0.1643, 0.6808, 0.4468, 0.6626, 0.5598, 0.0, 0.0, 0.8629, 0.8714, 0.0, 0.7583, 0.9785, 0.0, 0.0, 0.8713, 0.8743, 0.0, 0.0, 0.413, 0.9818, 0.8074, 0.0, 0.0, 0.1395, 0.957, 0.0, 0.2727, 0.961, 0.9168, 0.7476, 0.954, 0.8553, 0.0, 0.0, 0.7801, 0.3077, 0.9158, 0.0, 0.9715, 0.6848, 0.0, 0.9405, 0.5711, 0.0303, 0.828, 0.0, 0.4983, 0.8204, 0.9128, 0.1765, 0.948, 0.0, 0.9093, 0.0, 0.8065, 0.9741, 0.8208, 0.6993, 0.0256, 0.0, 0.0, 0.9448, 0.8606, 0.5333, 0.8667, 0.8466, 0.1, 0.9, 0.8634, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0175, 0.9651, 0.8841, 0.9185, 0.8107, 0.0, 0.0, 0.961, 0.5992, 0.16, 0.9783, 0.9588, 0.0, 0.0, 0.9405, 0.7064, 0.0, 0.7983, 0.1892, 0.9227, 0.9665, 0.0, 0.4275, 0.0, 0.0526, 0.1338, 0.7984, 0.7576, 0.9756, 0.0, 0.9013, 0.9459, 0.62, 0.0, 0.9412, 1.0, 0.0, 0.8152, 0.7685, 0.0, 0.9144, 0.9556, 0.0, 0.9463, 0.0, 0.0, 0.9279, 0.9291, 0.0, 0.7936, 0.1404, 0.9347, 0.7625, 0.9298, 0.3333, 0.0, 0.0, 0.0, 0.0, 0.718, 0.8994, 0.0, 0.0, 0.6996, 0.0, 0.93, 0.0278, 0.005, 0.9144, 0.0, 0.0, 0.5027, 0.0, 0.0, 0.0, 0.8841, 0.9495, 0.0, 0.8988, 0.3846, 0.9657, 0.0, 0.9233, 0.05, 0.791, 0.9959, 0.9783, 0.9435, 0.0196, 0.0, 1.0, 0.5, 0.125, 0.0, 0.3054, 0.9174, 0.9065, 0.5685, 0.0, 0.9808, 0.0253, 0.0068, 0.0, 0.9308, 0.9947, 0.4795, 0.0, 0.0, 0.8758, 0.7823, 0.0, 0.0101, 0.9126, 0.039, 0.9861, 0.0299, 0.7383, 0.0, 0.9648, 0.9286, 0.0, 0.4259, 0.0549, 0.7766, 0.8899, 0.9664, 0.313, 0.8289, 0.0, 0.9527, 0.9197, 0.9086, 0.0957, 0.9251, 0.0526, 0.087, 0.8167, 0.0, 0.0, 0.8409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8421, 0.9937, 0.0, 0.9714, 0.3793, 0.9881, 0.898, 0.9091, 0.614, 0.6879, 0.633, 0.8628, 0.9639, 0.916, 0.9922, 0.0, 0.7252, 0.3333, 0.9691, 0.9033, 0.2647, 0.9286, 0.9029, 0.9371, 0.0, 0.5978, 0.0, 0.9485, 0.968, 0.0, 0.1042, 0.9424, 0.8645, 0.972, 0.0, 0.0], 'micro': 0.7576, 'macro': 0.4841, 'weighted': 0.7525}
2024-08-04 09:31:32 - [34m[1mLOGS   [0m - Best checkpoint with score 0.48 saved at /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_best.pt
2024-08-04 09:31:33 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt
2024-08-04 09:31:33 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_last.pt
2024-08-04 09:31:33 - [34m[1mLOGS   [0m - Training checkpoint for epoch 3/iteration 490 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_epoch_3_iter_490.pt
2024-08-04 09:31:33 - [34m[1mLOGS   [0m - Model state for epoch 3/iteration 490 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_epoch_3_iter_490.pt
[31m===========================================================================[0m
2024-08-04 09:31:35 - [32m[1mINFO   [0m - Training epoch 4
2024-08-04 09:31:36 - [34m[1mLOGS   [0m - Epoch:   4 [     491/10000000], loss: {'classification': 5.457, 'neural_augmentation': 0.2851, 'total_loss': 5.7421}, LR: [4.9e-05, 4.9e-05], Avg. batch load time: 0.971, Elapsed time:  1.13
2024-08-04 09:31:56 - [34m[1mLOGS   [0m - *** Training summary for epoch 4
	 loss={'classification': 5.6546, 'neural_augmentation': 0.3209, 'total_loss': 5.9755}
2024-08-04 09:32:27 - [34m[1mLOGS   [0m - *** Validation summary for epoch 4
	 loss={'classification': 4.6685, 'neural_augmentation': 0.0, 'total_loss': 4.6685} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.8118, 0.6504, 0.4188, 0.8378, 0.7818, 0.0351, 0.0063, 0.2261, 0.7415, 0.5328, 0.3011, 0.7143, 0.6957, 0.655, 0.7857, 0.7211, 0.6356, 0.7051, 0.125, 0.3187, 0.8873, 0.8223, 0.7399, 0.0039, 0.7754, 0.1058, 0.4489, 0.5589, 0.1053, 0.8451, 0.7543, 0.8911, 0.7297, 0.8986, 0.0678, 0.0426, 0.0007, 0.0769, 0.0909, 0.7546, 0.6917, 0.9457, 0.0755, 0.4063, 0.0011, 0.4058, 0.76, 0.4364, 0.2675, 0.8, 0.0845, 0.8374, 0.9747, 0.599, 0.2098, 0.8318, 0.6984, 0.747, 0.6895, 0.5537, 0.6808, 0.0432, 0.8161, 0.8243, 0.003, 0.148, 0.377, 0.2796, 0.9199, 0.6891, 0.704, 0.6109, 0.0015, 0.8206, 0.6892, 0.9573, 0.973, 0.1538, 0.0143, 0.3529, 0.7778, 0.7934, 0.8465, 0.8037, 0.6761, 0.7524, 0.5244, 0.3046, 0.4138, 0.8638, 0.6719, 0.4643, 0.8323, 0.0052, 0.4655, 0.6495, 0.7143, 0.6441, 0.6327, 0.2059, 0.0108, 0.8265, 0.8557, 0.1818, 0.8263, 0.9617, 0.0007, 0.0132, 0.858, 0.8061, 0.0278, 0.1667, 0.5855, 0.9356, 0.8571, 0.0017, 0.0023, 0.4071, 0.973, 0.0157, 0.6557, 0.9101, 0.8067, 0.7148, 0.9122, 0.8322, 0.0526, 0.0476, 0.818, 0.6142, 0.8289, 0.0059, 0.9589, 0.7071, 0.1538, 0.7916, 0.5561, 0.2105, 0.8196, 0.1026, 0.5803, 0.7719, 0.8186, 0.4118, 0.9499, 0.2597, 0.823, 0.0006, 0.8322, 0.9258, 0.8612, 0.764, 0.2991, 0.0052, 0.0435, 0.9457, 0.8397, 0.6497, 0.8143, 0.875, 0.3614, 0.8247, 0.8568, 0.0214, 0.0054, 0.2857, 0.064, 0.0001, 0.0845, 0.0055, 0.4142, 0.9048, 0.83, 0.8504, 0.7948, 0.0023, 0.1176, 0.8768, 0.62, 0.6087, 0.931, 0.9105, 0.1579, 0.0183, 0.8455, 0.7296, 0.0002, 0.8246, 0.5732, 0.8914, 0.936, 0.0048, 0.6165, 0.0515, 0.3175, 0.4509, 0.7955, 0.7594, 0.8997, 0.0011, 0.9244, 0.8967, 0.6953, 0.4957, 0.9314, 0.975, 0.0438, 0.8248, 0.7692, 0.2182, 0.884, 0.951, 0.0177, 0.9424, 0.0004, 0.087, 0.8826, 0.9051, 0.0003, 0.8079, 0.4921, 0.9385, 0.6104, 0.9185, 0.5421, 0.1905, 0.2735, 0.3784, 0.0036, 0.6745, 0.9259, 0.3497, 0.0008, 0.7045, 0.0031, 0.8246, 0.4673, 0.8775, 0.8705, 0.2308, 0.0022, 0.5771, 0.08, 0.0003, 0.0247, 0.8582, 0.8871, 0.1791, 0.8657, 0.5379, 0.8618, 0.1667, 0.8367, 0.4427, 0.7137, 0.9777, 0.9379, 0.8869, 0.3361, 0.0026, 0.9372, 0.5316, 0.5263, 0.0008, 0.4922, 0.8768, 0.8952, 0.6803, 0.2321, 0.9389, 0.2667, 0.2392, 0.1296, 0.917, 0.9053, 0.6272, 0.002, 0.0392, 0.778, 0.7545, 0.0005, 0.2787, 0.9108, 0.439, 0.9362, 0.4225, 0.8087, 0.0016, 0.9112, 0.7559, 0.0004, 0.619, 0.722, 0.7869, 0.836, 0.917, 0.4076, 0.8713, 0.0022, 0.9263, 0.952, 0.7332, 0.3966, 0.8262, 0.3846, 0.6316, 0.8785, 0.0079, 0.0028, 0.8146, 0.0048, 0.4318, 0.1, 0.0122, 0.6667, 0.1843, 0.0149, 0.8584, 0.9563, 0.0026, 0.9677, 0.4848, 0.9676, 0.8881, 0.8768, 0.6092, 0.7299, 0.6654, 0.7827, 0.9181, 0.8467, 0.9616, 0.2, 0.7511, 0.6207, 0.9367, 0.8528, 0.5385, 0.9163, 0.9333, 0.9006, 0.1176, 0.7251, 0.4865, 0.766, 0.9498, 0.0026, 0.3317, 0.918, 0.8634, 0.9682, 0.0351, 0.0013], 'AP': [0.8795, 0.6936, 0.3685, 0.853, 0.7977, 0.0103, 0.0019, 0.1148, 0.7894, 0.4995, 0.1258, 0.7322, 0.7546, 0.7062, 0.7698, 0.8038, 0.6775, 0.7819, 0.0563, 0.1834, 0.9217, 0.8927, 0.6833, 0.0006, 0.8114, 0.0273, 0.4072, 0.5435, 0.0355, 0.899, 0.7512, 0.933, 0.7545, 0.9258, 0.0115, 0.004, 0.0003, 0.0137, 0.0333, 0.7496, 0.6532, 0.9392, 0.0201, 0.3299, 0.0004, 0.3596, 0.8051, 0.2531, 0.2055, 0.8709, 0.0154, 0.8877, 0.9895, 0.6555, 0.1237, 0.8241, 0.7381, 0.7977, 0.7419, 0.5684, 0.735, 0.0087, 0.8682, 0.8817, 0.0005, 0.0834, 0.2711, 0.1791, 0.9673, 0.7104, 0.6611, 0.5752, 0.0006, 0.8707, 0.6592, 0.9829, 0.9947, 0.0532, 0.0013, 0.2023, 0.7957, 0.8657, 0.9005, 0.8684, 0.6892, 0.7771, 0.4884, 0.2328, 0.2793, 0.9031, 0.731, 0.4011, 0.8958, 0.0017, 0.4282, 0.6962, 0.7806, 0.6544, 0.5936, 0.0564, 0.0024, 0.8716, 0.8979, 0.0489, 0.8338, 0.9914, 0.0002, 0.002, 0.8918, 0.8669, 0.0051, 0.0382, 0.5906, 0.971, 0.8465, 0.0007, 0.0008, 0.3446, 0.9909, 0.0028, 0.533, 0.9587, 0.8625, 0.7691, 0.9592, 0.8815, 0.0111, 0.0099, 0.8586, 0.6796, 0.893, 0.0022, 0.974, 0.7489, 0.0483, 0.8541, 0.5538, 0.0646, 0.8553, 0.0379, 0.5949, 0.8191, 0.8157, 0.3669, 0.9523, 0.1585, 0.8892, 0.0002, 0.8954, 0.973, 0.9159, 0.8094, 0.1368, 0.0018, 0.0115, 0.9668, 0.8788, 0.6102, 0.8766, 0.8853, 0.2501, 0.878, 0.9043, 0.0054, 0.0017, 0.0851, 0.0197, 0.0001, 0.0186, 0.0012, 0.2656, 0.9616, 0.8739, 0.8953, 0.858, 0.0009, 0.0311, 0.9274, 0.6456, 0.6368, 0.9838, 0.9495, 0.0517, 0.004, 0.9011, 0.7337, 0.0001, 0.882, 0.5439, 0.9346, 0.9558, 0.0013, 0.5592, 0.012, 0.2512, 0.4149, 0.8389, 0.734, 0.9487, 0.0004, 0.9486, 0.9563, 0.7476, 0.3207, 0.9743, 0.9913, 0.0103, 0.8698, 0.8103, 0.0935, 0.9249, 0.9489, 0.0026, 0.9609, 0.0002, 0.0105, 0.9226, 0.9322, 0.0002, 0.8475, 0.4159, 0.9502, 0.5503, 0.9511, 0.4499, 0.0834, 0.1375, 0.1788, 0.0017, 0.7524, 0.9393, 0.1891, 0.0003, 0.7408, 0.0004, 0.8815, 0.2815, 0.8512, 0.9087, 0.0913, 0.0008, 0.5973, 0.0248, 0.0001, 0.0125, 0.9052, 0.9385, 0.0709, 0.9159, 0.4798, 0.9235, 0.0445, 0.9043, 0.3371, 0.7623, 0.9926, 0.9828, 0.9427, 0.2232, 0.0006, 0.9613, 0.4475, 0.5062, 0.0003, 0.486, 0.9406, 0.9477, 0.7075, 0.1101, 0.9807, 0.1506, 0.1428, 0.048, 0.9482, 0.9519, 0.6378, 0.0006, 0.0074, 0.8309, 0.8246, 0.0002, 0.1919, 0.9491, 0.3757, 0.9811, 0.3526, 0.8441, 0.0006, 0.9621, 0.7688, 0.0002, 0.5745, 0.6841, 0.8585, 0.8914, 0.953, 0.3011, 0.9268, 0.0006, 0.9694, 0.9653, 0.7623, 0.3307, 0.8979, 0.311, 0.5716, 0.9002, 0.0023, 0.0008, 0.8613, 0.0009, 0.3017, 0.0297, 0.0023, 0.5033, 0.0906, 0.0037, 0.9092, 0.988, 0.0008, 0.9872, 0.4845, 0.9881, 0.9282, 0.9281, 0.614, 0.7572, 0.6964, 0.8374, 0.949, 0.9149, 0.9914, 0.0429, 0.7976, 0.5631, 0.9684, 0.9107, 0.4648, 0.9613, 0.9546, 0.9592, 0.0314, 0.7666, 0.3843, 0.7471, 0.9751, 0.0009, 0.2218, 0.9524, 0.8881, 0.985, 0.0053, 0.0004], 'Recall@P=50': [0.9634, 0.7342, 0.2955, 0.9583, 0.9112, 0.0, 0.0, 0.0426, 0.8872, 0.5288, 0.0, 0.8097, 0.8036, 0.7477, 0.8158, 0.8839, 0.746, 0.8643, 0.0244, 0.0404, 0.9404, 0.9501, 0.7433, 0.0, 0.8073, 0.0, 0.3458, 0.0022, 0.0, 0.9197, 0.7525, 0.959, 0.8254, 0.9333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7613, 0.0192, 0.9353, 0.0, 0.2079, 0.0, 0.2979, 0.8979, 0.0156, 0.1512, 0.9169, 0.0, 0.9276, 0.9955, 0.6831, 0.0211, 0.8996, 0.7628, 0.9185, 0.8473, 0.5929, 0.8057, 0.0, 0.9109, 0.9222, 0.0, 0.013, 0.117, 0.0256, 0.9899, 0.807, 0.8033, 0.6159, 0.0, 0.9076, 0.6531, 0.991, 1.0, 0.0, 0.0, 0.0625, 0.8492, 0.9385, 0.9259, 0.9395, 0.0013, 0.816, 0.4773, 0.0556, 0.1364, 0.9218, 0.7971, 0.375, 0.9338, 0.0, 0.4242, 0.7412, 0.8723, 0.7086, 0.6029, 0.0, 0.0, 0.9086, 0.919, 0.0, 0.8333, 1.0, 0.0, 0.0, 0.9123, 0.9211, 0.0, 0.0, 0.6812, 0.9912, 0.8593, 0.0, 0.0, 0.0233, 1.0, 0.0, 0.6364, 0.9762, 0.9416, 0.8155, 0.977, 0.8976, 0.0, 0.0, 0.8838, 0.7372, 0.9376, 0.0, 0.9715, 0.8132, 0.0, 0.9494, 0.6095, 0.0, 0.8717, 0.0, 0.6159, 0.8738, 0.922, 0.3294, 0.9538, 0.0526, 0.9666, 0.0, 0.9309, 0.9896, 0.934, 0.8671, 0.0, 0.0, 0.0, 0.9755, 0.9121, 0.6444, 0.9233, 0.8977, 0.05, 0.9167, 0.9141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9884, 0.9275, 0.9304, 0.9136, 0.0, 0.0, 0.9659, 0.73, 0.56, 1.0, 0.9639, 0.0, 0.0, 0.967, 0.8532, 0.0, 0.8992, 0.6081, 0.9528, 0.9665, 0.0, 0.0072, 0.0, 0.1316, 0.3631, 0.8419, 0.8283, 0.9756, 0.0, 0.9507, 0.9838, 0.788, 0.2364, 1.0, 0.9922, 0.0, 0.8815, 0.8796, 0.0, 0.9673, 0.9556, 0.0, 0.9664, 0.0, 0.0, 0.9369, 0.948, 0.0, 0.8578, 0.4211, 0.9548, 0.6875, 0.9649, 0.5294, 0.0, 0.0, 0.0, 0.0, 0.833, 0.9349, 0.0, 0.0, 0.7819, 0.0, 0.9432, 0.0, 0.0101, 0.9679, 0.0, 0.0, 0.6351, 0.0, 0.0, 0.0, 0.9348, 0.9721, 0.0, 0.9561, 0.0154, 0.9733, 0.0, 0.9467, 0.0833, 0.8672, 0.9959, 0.9946, 0.9647, 0.049, 0.0, 1.0, 0.0625, 0.5, 0.0, 0.4709, 0.9633, 0.972, 0.6781, 0.0, 0.9872, 0.0127, 0.1284, 0.0, 0.9538, 0.9976, 0.726, 0.0, 0.0, 0.9221, 0.8306, 0.0, 0.0202, 0.9806, 0.0649, 0.993, 0.1343, 0.8691, 0.0, 0.9922, 0.9464, 0.0, 0.7407, 0.989, 0.9043, 0.9227, 0.981, 0.2957, 0.9342, 0.0, 0.973, 0.9708, 0.9286, 0.0106, 0.9626, 0.1053, 0.3478, 0.9167, 0.0, 0.0, 0.9489, 0.0, 0.0303, 0.0, 0.0, 0.5, 0.0132, 0.0, 0.9474, 0.9937, 0.0, 0.9929, 0.3448, 0.994, 0.9524, 0.9476, 0.7061, 0.7898, 0.7266, 0.9095, 0.9672, 0.958, 0.9961, 0.0, 0.8244, 0.6, 0.9691, 0.9333, 0.5588, 0.9821, 0.96, 0.9829, 0.0, 0.7717, 0.3571, 0.9897, 0.9817, 0.0, 0.0833, 0.9686, 0.9159, 0.986, 0.0, 0.0], 'micro': 0.8142, 'macro': 0.5377, 'weighted': 0.7976}
2024-08-04 09:32:40 - [34m[1mLOGS   [0m - Best checkpoint with score 0.54 saved at /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_best.pt
2024-08-04 09:32:40 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt
2024-08-04 09:32:40 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_last.pt
2024-08-04 09:32:41 - [34m[1mLOGS   [0m - Training checkpoint for epoch 4/iteration 612 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_epoch_4_iter_612.pt
2024-08-04 09:32:41 - [34m[1mLOGS   [0m - Model state for epoch 4/iteration 612 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_epoch_4_iter_612.pt
[31m===========================================================================[0m
2024-08-04 09:32:43 - [32m[1mINFO   [0m - Training epoch 5
2024-08-04 09:32:55 - [34m[1mLOGS   [0m - Epoch:   5 [     613/10000000], loss: {'classification': 5.3033, 'neural_augmentation': 0.3152, 'total_loss': 5.6185}, LR: [4.7e-05, 4.7e-05], Avg. batch load time: 11.957, Elapsed time: 12.13
2024-08-04 09:33:14 - [34m[1mLOGS   [0m - *** Training summary for epoch 5
	 loss={'classification': 4.9296, 'neural_augmentation': 0.315, 'total_loss': 5.2446}
2024-08-04 09:33:46 - [34m[1mLOGS   [0m - *** Validation summary for epoch 5
	 loss={'classification': 4.1933, 'neural_augmentation': 0.0, 'total_loss': 4.1933} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.8157, 0.6773, 0.45, 0.8784, 0.7885, 0.3333, 0.0077, 0.2313, 0.7538, 0.5615, 0.3111, 0.7332, 0.72, 0.6732, 0.7887, 0.737, 0.6457, 0.7324, 0.1757, 0.319, 0.8931, 0.8314, 0.7395, 0.0054, 0.7973, 0.2289, 0.5051, 0.5966, 0.1772, 0.8499, 0.7912, 0.9084, 0.757, 0.9091, 0.1667, 0.0345, 0.0008, 0.1875, 0.087, 0.7676, 0.7222, 0.9396, 0.1263, 0.4851, 0.0025, 0.5952, 0.7793, 0.4286, 0.4082, 0.8116, 0.1239, 0.8585, 0.9729, 0.6486, 0.2821, 0.8487, 0.713, 0.7903, 0.6925, 0.5736, 0.7007, 0.126, 0.8515, 0.8275, 0.0024, 0.1976, 0.406, 0.303, 0.9246, 0.7246, 0.7536, 0.6932, 0.0186, 0.828, 0.7202, 0.9615, 0.9827, 0.2188, 0.0133, 0.5, 0.7798, 0.8039, 0.862, 0.8201, 0.702, 0.7492, 0.5833, 0.38, 0.4211, 0.9038, 0.718, 0.5714, 0.8494, 0.0241, 0.5204, 0.6615, 0.8, 0.6709, 0.6819, 0.2558, 0.0086, 0.8691, 0.8536, 0.0741, 0.8571, 0.9674, 0.0003, 0.05, 0.8949, 0.8426, 0.0882, 0.1818, 0.6071, 0.9396, 0.8906, 0.0019, 0.0545, 0.4262, 0.973, 0.0476, 0.6533, 0.9334, 0.8327, 0.7378, 0.944, 0.8481, 0.2667, 0.1008, 0.8682, 0.7152, 0.8545, 0.129, 0.9676, 0.752, 0.2178, 0.801, 0.5865, 0.1923, 0.828, 0.1875, 0.6102, 0.8155, 0.8287, 0.4724, 0.9558, 0.3377, 0.8512, 0.0009, 0.8426, 0.9504, 0.8966, 0.779, 0.2885, 0.125, 0.3922, 0.9565, 0.852, 0.6748, 0.8418, 0.8875, 0.3784, 0.8495, 0.877, 0.1429, 0.2025, 0.1667, 0.1684, 0.0002, 0.1111, 0.1053, 0.4371, 0.9097, 0.8537, 0.865, 0.8279, 0.0034, 0.1481, 0.9171, 0.6694, 0.7619, 0.9575, 0.9186, 0.2778, 0.1277, 0.8756, 0.719, 0.0002, 0.9035, 0.6375, 0.9336, 0.9455, 0.01, 0.6355, 0.2759, 0.3736, 0.5206, 0.8223, 0.7822, 0.9164, 0.0034, 0.9338, 0.9359, 0.7396, 0.5506, 0.9388, 0.9786, 0.1356, 0.8579, 0.806, 0.25, 0.9126, 0.9572, 0.3636, 0.9485, 0.0004, 0.1333, 0.9082, 0.9111, 0.0004, 0.8095, 0.4643, 0.9463, 0.6125, 0.9222, 0.6, 0.3333, 0.3095, 0.4615, 0.004, 0.7273, 0.9338, 0.3497, 0.0012, 0.7608, 0.0016, 0.8423, 0.4783, 0.8867, 0.8942, 0.2857, 0.0156, 0.6042, 0.2041, 0.002, 0.1667, 0.8635, 0.9052, 0.2627, 0.8904, 0.5736, 0.8717, 0.5, 0.8673, 0.4937, 0.7383, 0.9797, 0.9485, 0.9168, 0.3862, 0.0115, 0.9476, 0.5227, 0.575, 0.0009, 0.4856, 0.9065, 0.9159, 0.7236, 0.3586, 0.9431, 0.2927, 0.3049, 0.1453, 0.9339, 0.9143, 0.7092, 0.0146, 0.1633, 0.8053, 0.8282, 0.0011, 0.3258, 0.932, 0.5402, 0.9504, 0.45, 0.8439, 0.0047, 0.9359, 0.7523, 0.0005, 0.7007, 0.7532, 0.8404, 0.8582, 0.9232, 0.4396, 0.922, 0.0065, 0.9556, 0.9562, 0.7652, 0.4171, 0.8451, 0.48, 0.6, 0.9217, 0.0588, 0.0063, 0.8596, 0.1111, 0.4819, 0.129, 0.0195, 0.4, 0.1961, 0.0263, 0.8745, 0.9653, 0.0144, 0.9855, 0.5417, 0.9706, 0.8889, 0.8881, 0.6439, 0.7574, 0.7262, 0.8119, 0.9305, 0.8774, 0.9681, 0.3333, 0.7814, 0.7333, 0.9568, 0.8797, 0.7273, 0.9385, 0.9415, 0.9422, 0.1463, 0.7821, 0.6296, 0.795, 0.9673, 0.0645, 0.3113, 0.9251, 0.8861, 0.9823, 0.1111, 0.0087], 'AP': [0.8876, 0.7262, 0.4246, 0.8818, 0.8158, 0.1204, 0.0024, 0.1108, 0.8137, 0.5591, 0.1708, 0.7661, 0.7826, 0.7271, 0.7715, 0.8241, 0.6969, 0.7989, 0.1035, 0.227, 0.9263, 0.8954, 0.6847, 0.0009, 0.8404, 0.0968, 0.468, 0.5988, 0.0603, 0.9138, 0.7831, 0.9498, 0.7953, 0.9504, 0.0539, 0.0034, 0.0003, 0.0454, 0.037, 0.7544, 0.6933, 0.9534, 0.0383, 0.4365, 0.0008, 0.5487, 0.8315, 0.2522, 0.3793, 0.8819, 0.0309, 0.9076, 0.9908, 0.7094, 0.1668, 0.8489, 0.7557, 0.8258, 0.7481, 0.5948, 0.7593, 0.0404, 0.8864, 0.8947, 0.0005, 0.1063, 0.3166, 0.2031, 0.9729, 0.7479, 0.7095, 0.669, 0.0063, 0.8925, 0.7398, 0.984, 0.9967, 0.0683, 0.0014, 0.3931, 0.8104, 0.8829, 0.9119, 0.8886, 0.7156, 0.809, 0.5577, 0.3247, 0.3515, 0.9264, 0.7717, 0.4931, 0.9091, 0.0064, 0.5005, 0.7212, 0.8633, 0.6764, 0.6506, 0.0939, 0.0022, 0.8987, 0.9078, 0.0257, 0.8591, 0.9916, 0.0001, 0.0052, 0.9252, 0.8893, 0.0272, 0.0561, 0.6224, 0.9727, 0.8692, 0.0009, 0.0174, 0.3304, 0.9947, 0.0062, 0.61, 0.9713, 0.882, 0.7877, 0.9747, 0.8997, 0.0999, 0.0321, 0.8966, 0.7779, 0.9151, 0.0421, 0.9796, 0.8202, 0.1, 0.8691, 0.5822, 0.0727, 0.8774, 0.0697, 0.6269, 0.8419, 0.8261, 0.4483, 0.9555, 0.256, 0.9129, 0.0003, 0.9016, 0.9823, 0.9536, 0.8421, 0.1486, 0.0366, 0.2399, 0.9818, 0.89, 0.6906, 0.9006, 0.9083, 0.3624, 0.8958, 0.9236, 0.0956, 0.0651, 0.0406, 0.0553, 0.0001, 0.075, 0.0219, 0.2961, 0.9749, 0.8956, 0.904, 0.8764, 0.0013, 0.1068, 0.953, 0.7062, 0.7599, 0.9879, 0.9569, 0.1744, 0.038, 0.9221, 0.7487, 0.0001, 0.9275, 0.6037, 0.9686, 0.9603, 0.0031, 0.618, 0.1362, 0.2552, 0.5515, 0.8794, 0.7511, 0.9546, 0.0012, 0.9634, 0.9732, 0.7962, 0.3847, 0.9738, 0.9923, 0.0394, 0.902, 0.8658, 0.1525, 0.9426, 0.9601, 0.2831, 0.9626, 0.0002, 0.0388, 0.9557, 0.9418, 0.0002, 0.8782, 0.4404, 0.9523, 0.6028, 0.9597, 0.5158, 0.1662, 0.1769, 0.3744, 0.0016, 0.7923, 0.96, 0.2406, 0.0005, 0.7936, 0.0003, 0.8995, 0.3391, 0.8838, 0.9319, 0.1245, 0.0015, 0.6369, 0.0716, 0.0006, 0.0909, 0.923, 0.9546, 0.1736, 0.9314, 0.5055, 0.9386, 0.4837, 0.9262, 0.4161, 0.7865, 0.9953, 0.989, 0.9577, 0.2869, 0.0023, 0.9691, 0.4913, 0.6114, 0.0003, 0.4735, 0.954, 0.9614, 0.7673, 0.2044, 0.9856, 0.2083, 0.2134, 0.0727, 0.9585, 0.9636, 0.7147, 0.0019, 0.0784, 0.847, 0.9018, 0.0005, 0.2242, 0.9585, 0.5238, 0.9877, 0.4428, 0.878, 0.0017, 0.9731, 0.7642, 0.0002, 0.6907, 0.7447, 0.9005, 0.9182, 0.9606, 0.3733, 0.9628, 0.0022, 0.9842, 0.9718, 0.831, 0.3638, 0.9162, 0.5187, 0.5701, 0.9594, 0.0145, 0.0018, 0.9242, 0.0287, 0.3292, 0.0531, 0.0045, 0.1767, 0.1287, 0.0052, 0.9371, 0.9904, 0.0022, 0.9922, 0.5482, 0.9941, 0.9378, 0.9332, 0.6498, 0.8187, 0.7549, 0.8815, 0.9607, 0.9292, 0.9895, 0.2221, 0.8394, 0.6533, 0.9765, 0.9283, 0.7148, 0.9761, 0.956, 0.9747, 0.1203, 0.8363, 0.556, 0.8013, 0.9812, 0.0111, 0.2107, 0.9595, 0.9035, 0.9907, 0.0228, 0.0022], 'Recall@P=50': [0.9692, 0.7748, 0.3817, 0.9792, 0.9226, 0.0, 0.0, 0.0638, 0.9056, 0.5671, 0.122, 0.8426, 0.8424, 0.7721, 0.8289, 0.9023, 0.7698, 0.8809, 0.0488, 0.1717, 0.9404, 0.9509, 0.7701, 0.0, 0.8458, 0.0, 0.486, 0.6526, 0.0, 0.9458, 0.7822, 0.9664, 0.8857, 0.9686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7568, 0.0192, 0.9552, 0.0, 0.4653, 0.0, 0.5957, 0.9175, 0.0156, 0.314, 0.9446, 0.0, 0.9465, 0.9955, 0.7446, 0.0316, 0.9071, 0.7956, 0.9421, 0.8612, 0.6555, 0.8471, 0.0, 0.9109, 0.9362, 0.0, 0.0065, 0.2105, 0.1538, 0.9935, 0.8363, 0.918, 0.6739, 0.0, 0.9398, 0.7143, 0.991, 1.0, 0.0, 0.0, 0.375, 0.859, 0.9505, 0.9442, 0.9521, 0.8226, 0.8516, 0.5833, 0.1389, 0.3182, 0.9385, 0.8338, 0.5, 0.9464, 0.0, 0.5379, 0.7686, 0.9149, 0.7331, 0.6316, 0.0, 0.0, 0.9188, 0.9429, 0.0, 0.8583, 1.0, 0.0, 0.0, 0.9415, 0.9298, 0.0, 0.1111, 0.7464, 0.9906, 0.8741, 0.0, 0.0, 0.0465, 1.0, 0.0, 0.8182, 0.9829, 0.0009, 0.8479, 0.9943, 0.9209, 0.0, 0.0, 0.9004, 0.8269, 0.9544, 0.0, 0.9822, 0.856, 0.0, 0.956, 0.6659, 0.0, 0.898, 0.0, 0.6886, 0.8689, 0.922, 0.3765, 0.948, 0.2105, 0.9737, 0.0, 0.9263, 0.9845, 0.9811, 0.9021, 0.0256, 0.0, 0.0417, 0.9877, 0.9152, 0.7, 0.9351, 0.9148, 0.25, 0.9367, 0.9361, 0.0714, 0.0, 0.0, 0.0, 0.0, 0.0588, 0.0, 0.0877, 0.9942, 0.9462, 0.9344, 0.9136, 0.0, 0.0714, 0.9756, 0.8059, 0.76, 0.9957, 0.9691, 0.1154, 0.0, 0.967, 0.8899, 0.0, 0.9328, 0.7297, 0.9828, 0.9617, 0.0, 0.7754, 0.0, 0.0263, 0.5414, 0.9012, 0.0101, 0.9756, 0.0, 0.9704, 0.9892, 0.852, 0.0091, 0.9804, 0.9961, 0.0, 0.9147, 0.9167, 0.12, 0.9723, 0.9597, 0.25, 0.9664, 0.0, 0.0, 0.973, 0.9504, 0.0, 0.8991, 0.3509, 0.9548, 0.0125, 0.9754, 0.4314, 0.25, 0.0, 0.1538, 0.0, 0.8872, 0.9586, 0.0645, 0.0, 0.8477, 0.0, 0.9615, 0.0833, 0.995, 0.9893, 0.0, 0.0, 0.6459, 0.0, 0.0, 0.0, 0.9493, 0.9774, 0.1029, 0.9694, 0.6154, 0.9905, 0.2857, 0.96, 0.4167, 0.9068, 0.9959, 1.0, 0.9717, 0.1275, 0.0, 1.0, 0.5, 0.575, 0.0, 0.4685, 0.9725, 0.9626, 0.7808, 0.0179, 1.0, 0.1392, 0.1959, 0.0, 0.9692, 0.9952, 0.7945, 0.0, 0.0588, 0.0008, 0.9274, 0.0, 0.1717, 0.9709, 0.026, 1.0, 0.403, 0.8926, 0.0, 0.9961, 1.0, 0.0, 0.8889, 0.989, 0.9574, 0.9578, 0.9796, 0.3652, 0.9737, 0.0, 0.9865, 0.9781, 0.9429, 0.234, 0.9786, 0.3158, 0.5217, 0.9667, 0.0, 0.0, 0.9773, 0.0, 0.0303, 0.0, 0.0, 0.0, 0.0132, 0.0, 0.9649, 0.9937, 0.0, 0.9929, 0.5172, 1.0, 0.9728, 0.951, 0.7895, 0.8726, 0.7715, 0.9335, 0.9869, 0.9664, 0.9961, 0.2, 0.8931, 0.7333, 0.9815, 0.9533, 0.7941, 0.9866, 0.9657, 0.9771, 0.0526, 0.8696, 0.0357, 1.0, 0.9817, 0.0, 0.0, 0.9843, 0.9299, 0.993, 0.0, 0.0], 'micro': 0.8374, 'macro': 0.5723, 'weighted': 0.8199}
2024-08-04 09:34:00 - [34m[1mLOGS   [0m - Best checkpoint with score 0.57 saved at /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_best.pt
2024-08-04 09:34:00 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_score_0.0103.pt
2024-08-04 09:34:00 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.0725.pt', 'checkpoint_score_0.3370.pt', 'checkpoint_score_0.4841.pt', 'checkpoint_score_0.5377.pt', 'checkpoint_score_0.5723.pt']
2024-08-04 09:34:02 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_avg.pt
2024-08-04 09:34:02 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt
2024-08-04 09:34:02 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_last.pt
2024-08-04 09:34:03 - [34m[1mLOGS   [0m - Training checkpoint for epoch 5/iteration 735 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_epoch_5_iter_735.pt
2024-08-04 09:34:03 - [34m[1mLOGS   [0m - Model state for epoch 5/iteration 735 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_epoch_5_iter_735.pt
[31m===========================================================================[0m
2024-08-04 09:34:05 - [32m[1mINFO   [0m - Training epoch 6
2024-08-04 09:34:06 - [34m[1mLOGS   [0m - Epoch:   6 [     736/10000000], loss: {'classification': 4.3578, 'neural_augmentation': 0.3235, 'total_loss': 4.6813}, LR: [4.6e-05, 4.6e-05], Avg. batch load time: 1.196, Elapsed time:  1.35
2024-08-04 09:34:26 - [34m[1mLOGS   [0m - *** Training summary for epoch 6
	 loss={'classification': 4.4737, 'neural_augmentation': 0.3075, 'total_loss': 4.7813}
2024-08-04 09:34:57 - [34m[1mLOGS   [0m - *** Validation summary for epoch 6
	 loss={'classification': 3.9032, 'neural_augmentation': 0.0, 'total_loss': 3.9032} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.8183, 0.6932, 0.4524, 0.8851, 0.7902, 0.3529, 0.0476, 0.2602, 0.764, 0.5806, 0.275, 0.7467, 0.7137, 0.688, 0.7801, 0.7439, 0.6573, 0.7529, 0.1667, 0.3504, 0.906, 0.8384, 0.7356, 0.087, 0.792, 0.2745, 0.5408, 0.6157, 0.2484, 0.8618, 0.7979, 0.916, 0.7778, 0.9194, 0.1414, 0.0143, 0.0009, 0.1702, 0.1333, 0.7733, 0.7652, 0.9534, 0.2, 0.5173, 0.0152, 0.716, 0.7901, 0.375, 0.5263, 0.8285, 0.2857, 0.8726, 0.9819, 0.6678, 0.3158, 0.8592, 0.7301, 0.8092, 0.7061, 0.5972, 0.7324, 0.2353, 0.8747, 0.8426, 0.0133, 0.2281, 0.4372, 0.4048, 0.9286, 0.7619, 0.7874, 0.7095, 0.0125, 0.8498, 0.7606, 0.9644, 0.9793, 0.1538, 0.0185, 0.5, 0.8034, 0.8114, 0.8694, 0.8298, 0.7162, 0.8073, 0.614, 0.4259, 0.4308, 0.9075, 0.728, 0.7105, 0.8644, 0.0377, 0.531, 0.6753, 0.7925, 0.674, 0.6833, 0.3429, 0.0232, 0.8737, 0.8802, 0.3333, 0.8848, 0.9727, 0.0015, 0.0606, 0.9102, 0.8437, 0.0938, 0.2, 0.6431, 0.9479, 0.8984, 0.0026, 0.0909, 0.4878, 0.9733, 0.04, 0.6742, 0.9439, 0.8355, 0.7504, 0.9598, 0.8569, 0.2456, 0.2449, 0.867, 0.7509, 0.8557, 0.1802, 0.9694, 0.8049, 0.2519, 0.8076, 0.6084, 0.2025, 0.8272, 0.2857, 0.5894, 0.8223, 0.8326, 0.5034, 0.9555, 0.4198, 0.8599, 0.001, 0.8537, 0.9485, 0.8945, 0.8175, 0.2951, 0.1667, 0.4082, 0.9557, 0.8586, 0.7143, 0.8534, 0.8957, 0.5, 0.8581, 0.8773, 0.25, 0.2703, 0.4, 0.3429, 0.0001, 0.087, 0.4, 0.4176, 0.9277, 0.8581, 0.8747, 0.8638, 0.0196, 0.2105, 0.9227, 0.7029, 0.8085, 0.9582, 0.9487, 0.3077, 0.2593, 0.8791, 0.7383, 0.0002, 0.9027, 0.6433, 0.9476, 0.9484, 0.0476, 0.6403, 0.3143, 0.4211, 0.5412, 0.8471, 0.802, 0.9259, 0.0097, 0.932, 0.9477, 0.7417, 0.5424, 0.9552, 0.9825, 0.28, 0.8713, 0.8507, 0.3226, 0.9221, 0.9593, 0.6087, 0.9658, 0.0005, 0.2143, 0.9057, 0.9262, 0.0037, 0.8115, 0.4769, 0.9583, 0.6063, 0.9397, 0.6379, 0.1538, 0.3617, 0.3571, 0.029, 0.7329, 0.9379, 0.3636, 0.0033, 0.7778, 0.0027, 0.852, 0.5306, 0.8998, 0.8987, 0.3103, 0.069, 0.6163, 0.3243, 0.0292, 0.3333, 0.8872, 0.9115, 0.4132, 0.8987, 0.5797, 0.8922, 0.8333, 0.8761, 0.5789, 0.762, 0.9897, 0.9398, 0.9138, 0.3831, 0.0225, 0.9522, 0.5672, 0.6197, 0.001, 0.5218, 0.9395, 0.939, 0.7317, 0.4112, 0.9644, 0.3886, 0.3518, 0.2524, 0.9565, 0.9215, 0.6963, 0.0185, 0.2535, 0.803, 0.8534, 0.0154, 0.3953, 0.9604, 0.6012, 0.9544, 0.4409, 0.8633, 0.0264, 0.9405, 0.7519, 0.0013, 0.6964, 0.7665, 0.875, 0.8617, 0.9345, 0.4353, 0.9293, 0.0488, 0.9627, 0.9597, 0.7686, 0.4751, 0.8638, 0.6154, 0.5455, 0.9381, 0.4762, 0.0105, 0.8889, 0.1176, 0.56, 0.1633, 0.0741, 0.1818, 0.2857, 0.029, 0.9041, 0.9778, 0.0149, 0.9821, 0.6182, 0.9792, 0.9338, 0.8897, 0.6732, 0.7606, 0.7515, 0.8402, 0.9336, 0.8696, 0.9768, 0.3636, 0.7918, 0.6452, 0.9509, 0.8873, 0.7097, 0.933, 0.9467, 0.9486, 0.1727, 0.8111, 0.6552, 0.8, 0.9725, 0.1905, 0.2857, 0.9319, 0.8955, 0.9822, 0.1091, 0.6667], 'AP': [0.889, 0.7463, 0.4213, 0.909, 0.8262, 0.2583, 0.0117, 0.1406, 0.8207, 0.5975, 0.1551, 0.7815, 0.7859, 0.7382, 0.7842, 0.8286, 0.7147, 0.8185, 0.1195, 0.2832, 0.9408, 0.9032, 0.7058, 0.0097, 0.8521, 0.1431, 0.5207, 0.6301, 0.1001, 0.9141, 0.7887, 0.9501, 0.8191, 0.9552, 0.0464, 0.0018, 0.0004, 0.0662, 0.0518, 0.7804, 0.7229, 0.961, 0.0705, 0.5035, 0.0016, 0.6397, 0.8374, 0.2547, 0.4731, 0.8994, 0.1441, 0.9283, 0.9921, 0.7356, 0.2287, 0.8724, 0.7877, 0.8471, 0.7704, 0.6136, 0.7752, 0.1746, 0.899, 0.9049, 0.0018, 0.1302, 0.3283, 0.243, 0.9739, 0.7764, 0.7807, 0.7359, 0.0041, 0.9095, 0.8001, 0.9875, 0.9939, 0.0477, 0.0016, 0.4741, 0.8234, 0.8928, 0.9243, 0.8988, 0.727, 0.8485, 0.574, 0.3376, 0.4183, 0.9401, 0.785, 0.6085, 0.9242, 0.0083, 0.5196, 0.7311, 0.8866, 0.6884, 0.66, 0.2016, 0.0068, 0.9073, 0.9219, 0.1189, 0.8898, 0.9878, 0.0004, 0.0076, 0.9358, 0.8916, 0.0335, 0.1115, 0.6734, 0.9779, 0.8798, 0.0008, 0.0282, 0.3571, 0.9933, 0.006, 0.5816, 0.971, 0.8946, 0.8136, 0.9836, 0.916, 0.1009, 0.1175, 0.9063, 0.825, 0.9197, 0.0636, 0.9811, 0.8701, 0.134, 0.8719, 0.6133, 0.0772, 0.8771, 0.1315, 0.6258, 0.8548, 0.8572, 0.4731, 0.9589, 0.3553, 0.9147, 0.0004, 0.9183, 0.9855, 0.9507, 0.8652, 0.2112, 0.051, 0.318, 0.9831, 0.9018, 0.6883, 0.912, 0.9276, 0.4929, 0.8989, 0.93, 0.0923, 0.0989, 0.1751, 0.1095, 0.0001, 0.0238, 0.2422, 0.2875, 0.9758, 0.9025, 0.9063, 0.8988, 0.0033, 0.115, 0.9618, 0.7459, 0.8511, 0.9907, 0.9695, 0.255, 0.1042, 0.9304, 0.7905, 0.0001, 0.9245, 0.6689, 0.9734, 0.9625, 0.0153, 0.6529, 0.162, 0.3877, 0.5808, 0.8999, 0.7797, 0.9558, 0.0033, 0.9676, 0.9797, 0.7967, 0.4026, 0.9894, 0.9955, 0.1373, 0.9254, 0.9061, 0.2046, 0.9501, 0.9619, 0.5558, 0.9644, 0.0002, 0.1516, 0.954, 0.9531, 0.0006, 0.8713, 0.4244, 0.9604, 0.5858, 0.9675, 0.5658, 0.0641, 0.2506, 0.2964, 0.0103, 0.8157, 0.9617, 0.2851, 0.0009, 0.8177, 0.0004, 0.9137, 0.3985, 0.9141, 0.9332, 0.169, 0.0082, 0.6535, 0.1771, 0.0059, 0.2, 0.9358, 0.9541, 0.3356, 0.9386, 0.4934, 0.9484, 0.869, 0.9338, 0.4469, 0.8209, 0.996, 0.9866, 0.9587, 0.2888, 0.0056, 0.9678, 0.6031, 0.627, 0.0004, 0.5199, 0.9698, 0.9762, 0.7818, 0.3047, 0.9932, 0.3179, 0.2896, 0.1298, 0.9686, 0.9708, 0.7276, 0.0031, 0.1672, 0.8595, 0.8983, 0.004, 0.3093, 0.968, 0.5743, 0.9885, 0.4518, 0.8957, 0.0067, 0.9742, 0.7954, 0.0005, 0.6804, 0.7915, 0.9231, 0.932, 0.967, 0.42, 0.9783, 0.0182, 0.9912, 0.9836, 0.852, 0.4006, 0.935, 0.6386, 0.4868, 0.9728, 0.2938, 0.0028, 0.9398, 0.0462, 0.4052, 0.0661, 0.0197, 0.0662, 0.1832, 0.0084, 0.9524, 0.9915, 0.0025, 0.9921, 0.603, 0.9943, 0.9597, 0.943, 0.6976, 0.8201, 0.7842, 0.8994, 0.9691, 0.9375, 0.9973, 0.2847, 0.8621, 0.6313, 0.9785, 0.9353, 0.7125, 0.9755, 0.9615, 0.9806, 0.084, 0.8656, 0.6192, 0.8265, 0.9868, 0.1355, 0.1997, 0.9648, 0.9105, 0.9898, 0.0332, 0.5001], 'Recall@P=50': [0.9704, 0.8003, 0.394, 0.9792, 0.9317, 0.1, 0.0, 0.0426, 0.916, 0.611, 0.0, 0.8584, 0.8346, 0.7934, 0.8158, 0.9062, 0.7838, 0.8904, 0.0732, 0.2424, 0.9725, 0.9596, 0.7701, 0.0, 0.8763, 0.0769, 0.5794, 0.0007, 0.0, 0.9349, 0.7921, 0.9664, 0.8794, 0.9725, 0.0, 0.0, 0.0, 0.0588, 0.0, 0.7658, 0.9615, 0.9602, 0.0, 0.5182, 0.0, 0.0426, 0.9237, 0.0156, 0.4884, 0.9446, 0.125, 0.9638, 0.9955, 0.7877, 0.1263, 0.9071, 0.8376, 0.9485, 0.8744, 0.6936, 0.828, 0.125, 0.9158, 0.9432, 0.0, 0.0065, 0.0058, 0.0513, 0.9917, 0.8713, 0.9508, 0.8333, 0.0, 0.9317, 0.8418, 0.991, 0.9931, 0.0, 0.0, 0.4375, 0.8721, 0.964, 0.9582, 0.9611, 0.8277, 0.8902, 0.6212, 0.0278, 0.2727, 0.9441, 0.8559, 0.675, 0.959, 0.0, 0.556, 0.7779, 0.9362, 0.7239, 0.6507, 0.0294, 0.0, 0.9289, 0.9429, 0.0, 0.9, 0.9892, 0.0, 0.0, 0.9474, 0.9357, 0.0, 0.1111, 0.7681, 0.9917, 0.8963, 0.0, 0.0, 0.1628, 1.0, 0.0, 0.0152, 0.9838, 0.9619, 0.8576, 1.0, 0.9407, 0.0, 0.0, 0.917, 0.8974, 0.9584, 0.0, 0.9822, 0.9261, 0.0217, 0.9616, 0.7043, 0.0, 0.9038, 0.0, 0.6713, 0.8932, 0.9312, 0.4706, 0.9653, 0.2895, 0.0024, 0.0, 0.9447, 0.9896, 0.9811, 0.9091, 0.0256, 0.0, 0.1667, 0.9877, 0.9273, 0.7, 0.9528, 0.9318, 0.425, 0.93, 0.9515, 0.0, 0.0, 0.3333, 0.0, 0.0, 0.0, 0.25, 0.0526, 1.0, 0.001, 0.9284, 0.9424, 0.0, 0.0, 0.9707, 0.827, 0.88, 1.0, 0.9742, 0.1923, 0.0526, 0.978, 0.9266, 0.0, 0.9328, 0.7703, 0.9785, 0.9617, 0.0, 0.7754, 0.0, 0.2895, 0.5796, 0.917, 0.8889, 0.9756, 0.0, 0.977, 0.9892, 0.864, 0.0091, 1.0, 1.0, 0.0, 0.9526, 0.9259, 0.2, 0.9647, 0.9556, 0.5625, 0.9664, 0.0, 0.0833, 0.964, 0.9574, 0.0, 0.9128, 0.2456, 0.9598, 0.0125, 0.9825, 0.7255, 0.0, 0.0625, 0.0769, 0.0, 0.9002, 0.9645, 0.0968, 0.0, 0.8519, 0.0, 0.9696, 0.0278, 0.9899, 0.9893, 0.1, 0.0, 0.6757, 0.1176, 0.0, 0.0, 0.942, 0.9774, 0.2206, 0.972, 0.0308, 0.9924, 0.8571, 0.9567, 0.0167, 0.9266, 0.9959, 0.9946, 0.9717, 0.0392, 0.0, 1.0, 0.5938, 0.75, 0.0, 0.5268, 0.9817, 1.0, 0.8014, 0.25, 1.0, 0.2658, 0.2432, 0.0294, 0.9692, 0.9971, 0.7671, 0.0, 0.1176, 0.9399, 0.9194, 0.0, 0.0101, 0.9903, 0.6494, 1.0, 0.3582, 0.906, 0.0, 0.9922, 1.0, 0.0, 0.9259, 0.989, 0.9574, 0.9625, 0.9825, 0.3304, 0.9934, 0.0, 1.0, 0.9927, 0.96, 0.0213, 0.984, 0.6316, 0.2174, 0.9667, 0.1111, 0.0, 0.9943, 0.0, 0.4242, 0.0, 0.0, 0.0, 0.1053, 0.0, 0.9737, 0.9937, 0.0, 0.9929, 0.5862, 1.0, 0.9728, 0.958, 0.8289, 0.879, 0.794, 0.9477, 0.9934, 0.9748, 1.0, 0.2, 0.8969, 0.6667, 0.9815, 0.9567, 0.8235, 1.0, 0.96, 0.9943, 0.0, 0.913, 0.0714, 1.0, 0.9909, 0.0769, 0.0, 0.9791, 0.9299, 0.993, 0.0, 0.5], 'micro': 0.8515, 'macro': 0.5985, 'weighted': 0.833}
2024-08-04 09:35:11 - [34m[1mLOGS   [0m - Best checkpoint with score 0.60 saved at /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_best.pt
2024-08-04 09:35:11 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_score_0.0725.pt
2024-08-04 09:35:11 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.3370.pt', 'checkpoint_score_0.4841.pt', 'checkpoint_score_0.5377.pt', 'checkpoint_score_0.5723.pt', 'checkpoint_score_0.5985.pt']
2024-08-04 09:35:12 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_avg.pt
2024-08-04 09:35:13 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt
2024-08-04 09:35:13 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_last.pt
2024-08-04 09:35:13 - [34m[1mLOGS   [0m - Training checkpoint for epoch 6/iteration 854 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_epoch_6_iter_854.pt
2024-08-04 09:35:13 - [34m[1mLOGS   [0m - Model state for epoch 6/iteration 854 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_epoch_6_iter_854.pt
[31m===========================================================================[0m
2024-08-04 09:35:15 - [32m[1mINFO   [0m - Training epoch 7
2024-08-04 09:35:17 - [34m[1mLOGS   [0m - Epoch:   7 [     855/10000000], loss: {'classification': 4.2368, 'neural_augmentation': 0.305, 'total_loss': 4.5418}, LR: [4.4e-05, 4.4e-05], Avg. batch load time: 1.034, Elapsed time:  1.19
2024-08-04 09:35:37 - [34m[1mLOGS   [0m - *** Training summary for epoch 7
	 loss={'classification': 4.1325, 'neural_augmentation': 0.3018, 'total_loss': 4.4343}
2024-08-04 09:36:09 - [34m[1mLOGS   [0m - *** Validation summary for epoch 7
	 loss={'classification': 3.6674, 'neural_augmentation': 0.0, 'total_loss': 3.6674} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.826, 0.7014, 0.4916, 0.8851, 0.8014, 0.375, 0.2083, 0.2553, 0.7803, 0.5986, 0.2826, 0.7386, 0.7375, 0.7133, 0.8, 0.7464, 0.6739, 0.7473, 0.2073, 0.3791, 0.9116, 0.8349, 0.7443, 0.069, 0.804, 0.2857, 0.5856, 0.6316, 0.25, 0.8628, 0.8287, 0.9257, 0.7776, 0.9293, 0.1887, 0.0286, 0.0008, 0.2143, 0.2162, 0.7787, 0.7414, 0.9514, 0.1868, 0.5481, 0.1081, 0.7179, 0.7971, 0.4645, 0.5912, 0.8571, 0.4348, 0.9055, 0.9864, 0.7011, 0.3516, 0.8436, 0.7342, 0.812, 0.7146, 0.6048, 0.731, 0.25, 0.8816, 0.8456, 0.0833, 0.2229, 0.4169, 0.381, 0.9291, 0.7702, 0.7857, 0.7244, 0.0303, 0.8696, 0.7967, 0.9686, 0.9828, 0.2254, 0.0033, 0.625, 0.8094, 0.8249, 0.8796, 0.8489, 0.7129, 0.8065, 0.6585, 0.4, 0.5085, 0.9306, 0.7307, 0.75, 0.8669, 0.0193, 0.5548, 0.6884, 0.8571, 0.6751, 0.6852, 0.2889, 0.0625, 0.8827, 0.8942, 0.0364, 0.8739, 0.9733, 0.0019, 0.0909, 0.9124, 0.8493, 0.1277, 0.2, 0.6351, 0.9472, 0.8992, 0.0026, 0.1538, 0.4615, 0.9735, 0.0124, 0.6923, 0.9463, 0.8474, 0.7539, 0.9486, 0.8678, 0.2564, 0.3333, 0.8879, 0.7918, 0.87, 0.2031, 0.9765, 0.8214, 0.3265, 0.8177, 0.6216, 0.2069, 0.8453, 0.2703, 0.6547, 0.808, 0.8433, 0.5103, 0.9558, 0.4872, 0.8797, 0.0052, 0.8694, 0.9561, 0.9208, 0.8413, 0.3, 0.3333, 0.4918, 0.9602, 0.8683, 0.716, 0.8584, 0.908, 0.4737, 0.8661, 0.8857, 0.3846, 0.3103, 0.25, 0.3125, 0.0002, 0.1667, 0.4, 0.4339, 0.9524, 0.8635, 0.8825, 0.883, 0.0682, 0.16, 0.9261, 0.7067, 0.8085, 0.9646, 0.9385, 0.4211, 0.3125, 0.8872, 0.7647, 0.0001, 0.9198, 0.6797, 0.9538, 0.961, 0.4, 0.6752, 0.3529, 0.4789, 0.6263, 0.8584, 0.82, 0.9354, 0.011, 0.9524, 0.9503, 0.7619, 0.5446, 0.9703, 0.9844, 0.3636, 0.8651, 0.8768, 0.3636, 0.9225, 0.9593, 0.6429, 0.9693, 0.0004, 0.24, 0.9179, 0.9233, 0.0045, 0.8252, 0.5417, 0.9509, 0.628, 0.9403, 0.6667, 0.25, 0.3894, 0.3704, 0.1538, 0.7572, 0.9444, 0.3186, 0.019, 0.7854, 0.0308, 0.8648, 0.5455, 0.8894, 0.914, 0.3043, 0.0833, 0.6543, 0.3333, 0.125, 1.0, 0.9132, 0.9192, 0.4161, 0.9108, 0.5634, 0.901, 0.9333, 0.8759, 0.5496, 0.8062, 0.9836, 0.9504, 0.9278, 0.3715, 0.0129, 0.9573, 0.7097, 0.6486, 0.0049, 0.5304, 0.9366, 0.9581, 0.7794, 0.4727, 0.961, 0.3939, 0.4648, 0.4242, 0.9612, 0.9248, 0.7287, 0.0455, 0.4086, 0.8215, 0.8684, 0.0083, 0.4575, 0.951, 0.6424, 0.9645, 0.6412, 0.8712, 0.0741, 0.9507, 0.8361, 0.0105, 0.7213, 0.7793, 0.8619, 0.8667, 0.9385, 0.4623, 0.9439, 0.1481, 0.9695, 0.974, 0.7758, 0.4952, 0.876, 0.75, 0.6957, 0.9492, 0.8, 0.0308, 0.9, 0.1333, 0.5172, 0.1728, 0.1667, 0.2, 0.3025, 0.0401, 0.9211, 0.9779, 0.0161, 0.9855, 0.6522, 0.9822, 0.9209, 0.8922, 0.7072, 0.785, 0.7639, 0.8377, 0.9425, 0.8928, 0.9802, 0.5, 0.8201, 0.6667, 0.9533, 0.9028, 0.7429, 0.9532, 0.9464, 0.9435, 0.2078, 0.8353, 0.6909, 0.7897, 0.9772, 0.3077, 0.3077, 0.9399, 0.8983, 0.9786, 0.2623, 0.6667], 'AP': [0.8967, 0.7518, 0.4613, 0.9225, 0.8394, 0.208, 0.0529, 0.1913, 0.8371, 0.6218, 0.1375, 0.7889, 0.818, 0.7656, 0.7899, 0.8359, 0.727, 0.8212, 0.1238, 0.2993, 0.9469, 0.9007, 0.7238, 0.0087, 0.8668, 0.1452, 0.5754, 0.6408, 0.1456, 0.9226, 0.8167, 0.9599, 0.8212, 0.9596, 0.0649, 0.0046, 0.0003, 0.0809, 0.0911, 0.7965, 0.718, 0.967, 0.0782, 0.5238, 0.0201, 0.6581, 0.8498, 0.3263, 0.5055, 0.9211, 0.2356, 0.9386, 0.9927, 0.7651, 0.2854, 0.8694, 0.7908, 0.8683, 0.7765, 0.6316, 0.7929, 0.2037, 0.9129, 0.9061, 0.0111, 0.1441, 0.3489, 0.2394, 0.9765, 0.792, 0.7932, 0.7532, 0.0117, 0.9189, 0.8247, 0.9853, 0.996, 0.0755, 0.001, 0.5564, 0.8344, 0.9017, 0.9255, 0.9131, 0.7457, 0.8559, 0.613, 0.3932, 0.483, 0.9618, 0.7912, 0.6484, 0.9201, 0.0062, 0.5569, 0.7541, 0.9087, 0.7, 0.6822, 0.1372, 0.016, 0.9121, 0.9364, 0.0106, 0.894, 0.9898, 0.0006, 0.0191, 0.9378, 0.9023, 0.0472, 0.112, 0.6429, 0.9777, 0.8884, 0.0009, 0.0412, 0.3909, 0.9947, 0.0036, 0.6207, 0.9753, 0.9093, 0.8209, 0.9849, 0.9233, 0.1097, 0.1257, 0.9198, 0.8521, 0.9262, 0.0682, 0.984, 0.8804, 0.2194, 0.8815, 0.6308, 0.0723, 0.8963, 0.147, 0.6607, 0.8643, 0.8543, 0.5208, 0.966, 0.4427, 0.9237, 0.001, 0.921, 0.9876, 0.965, 0.872, 0.2087, 0.1324, 0.3383, 0.9858, 0.9153, 0.7426, 0.9136, 0.933, 0.3973, 0.8972, 0.9375, 0.2529, 0.1414, 0.0769, 0.1138, 0.0001, 0.0634, 0.279, 0.3513, 0.9837, 0.9075, 0.9081, 0.914, 0.0145, 0.0883, 0.9658, 0.7694, 0.8553, 0.9902, 0.9699, 0.3857, 0.1664, 0.9452, 0.8092, 0.0001, 0.9361, 0.7155, 0.9777, 0.972, 0.1877, 0.6896, 0.1736, 0.3601, 0.6668, 0.9162, 0.8376, 0.9641, 0.0044, 0.9712, 0.9826, 0.8231, 0.4282, 0.9923, 0.9966, 0.1891, 0.9256, 0.9197, 0.2537, 0.9531, 0.9642, 0.601, 0.9668, 0.0002, 0.1743, 0.9623, 0.9587, 0.0007, 0.8852, 0.4855, 0.9558, 0.609, 0.969, 0.652, 0.1363, 0.2999, 0.3362, 0.0513, 0.8422, 0.9646, 0.2601, 0.0044, 0.8416, 0.0035, 0.9209, 0.4633, 0.9163, 0.9367, 0.1917, 0.016, 0.6864, 0.146, 0.0255, 1.0, 0.9505, 0.9603, 0.3669, 0.9485, 0.4998, 0.96, 0.9379, 0.9274, 0.4289, 0.8568, 0.9953, 0.9884, 0.9653, 0.2587, 0.0031, 0.9688, 0.6879, 0.6949, 0.0009, 0.54, 0.9797, 0.9864, 0.8145, 0.4103, 0.993, 0.321, 0.4272, 0.268, 0.9696, 0.9735, 0.7583, 0.0087, 0.3304, 0.8659, 0.9358, 0.0022, 0.3829, 0.9545, 0.6655, 0.9905, 0.6301, 0.9174, 0.0143, 0.9798, 0.8765, 0.0017, 0.6934, 0.8052, 0.9371, 0.9369, 0.9703, 0.4734, 0.9867, 0.0402, 0.9924, 0.9871, 0.8606, 0.4401, 0.9328, 0.7919, 0.6971, 0.9686, 0.7354, 0.0081, 0.9448, 0.0477, 0.4282, 0.0711, 0.0491, 0.0748, 0.2106, 0.0137, 0.9629, 0.9943, 0.0029, 0.9923, 0.6172, 0.9958, 0.9659, 0.9444, 0.7337, 0.841, 0.8048, 0.9045, 0.9723, 0.9439, 0.9957, 0.3881, 0.8783, 0.6559, 0.9808, 0.9427, 0.7585, 0.9892, 0.9608, 0.9815, 0.1241, 0.8958, 0.6714, 0.8038, 0.9926, 0.2371, 0.2208, 0.9632, 0.9114, 0.9884, 0.104, 0.5001], 'Recall@P=50': [0.972, 0.8018, 0.461, 0.9792, 0.9476, 0.3, 0.0, 0.0426, 0.9206, 0.6575, 0.0, 0.8856, 0.8863, 0.8239, 0.8158, 0.9144, 0.7944, 0.8975, 0.0732, 0.2929, 0.9679, 0.9558, 0.7861, 0.0, 0.8824, 0.0, 0.6215, 0.0007, 0.0, 0.9544, 0.8218, 0.9701, 0.8889, 0.9686, 0.0, 0.0, 0.0, 0.0588, 0.0, 0.7613, 0.9231, 0.9751, 0.0, 0.5941, 0.0, 0.7021, 0.9381, 0.0312, 0.6279, 0.9529, 0.0625, 0.9685, 0.991, 0.8123, 0.2, 0.9145, 0.8668, 0.9528, 0.8905, 0.7099, 0.8344, 0.125, 0.9307, 0.9432, 0.0, 0.0065, 0.0058, 0.1026, 0.9958, 0.883, 0.9672, 0.8333, 0.0, 0.9438, 0.8316, 0.991, 1.0, 0.0, 0.0, 0.625, 0.8721, 0.9604, 0.959, 0.9682, 0.8403, 0.9021, 0.6515, 0.25, 0.2727, 0.9665, 0.8515, 0.725, 0.9495, 0.0, 0.6083, 0.811, 0.9362, 0.7454, 0.6794, 0.0294, 0.0, 0.9289, 0.9714, 0.0, 0.9083, 0.9892, 0.0, 0.0, 0.9532, 0.9298, 0.0, 0.1111, 0.7464, 0.9906, 0.8963, 0.0, 0.0, 0.0698, 1.0, 0.0, 0.0909, 0.9914, 0.9726, 0.8641, 1.0, 0.9533, 0.0, 0.0, 0.9212, 0.9103, 0.9584, 0.0, 0.9858, 0.9377, 0.087, 0.9695, 0.7156, 0.0, 0.9184, 0.0, 0.6817, 0.9126, 0.9404, 0.4588, 0.9653, 0.4211, 0.9785, 0.0, 0.9585, 0.9948, 0.9906, 0.9161, 0.1026, 0.0, 0.0833, 0.9939, 0.9455, 0.7778, 0.9552, 0.9375, 0.15, 0.94, 0.9559, 0.1429, 0.0323, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0175, 0.9884, 0.001, 0.9423, 0.9465, 0.0, 0.0, 0.9805, 0.8523, 0.92, 0.9957, 0.9742, 0.3077, 0.1053, 0.9912, 0.9174, 0.0, 0.9412, 0.8378, 0.9828, 0.9761, 0.3333, 0.8188, 0.0, 0.3947, 0.6943, 0.9486, 0.9596, 0.9756, 0.0, 0.9671, 0.9946, 0.868, 0.5818, 1.0, 1.0, 0.1481, 0.9573, 0.9259, 0.04, 0.9723, 0.9758, 0.625, 0.9664, 0.0, 0.0833, 0.982, 0.9716, 0.0, 0.9128, 0.4737, 0.9447, 0.8125, 0.9825, 0.8039, 0.0, 0.0938, 0.1538, 0.0, 0.9219, 0.9645, 0.1613, 0.0, 0.9012, 0.0, 0.9746, 0.5556, 0.995, 0.9893, 0.15, 0.0, 0.7297, 0.0, 0.0, 1.0, 0.9638, 0.9756, 0.2647, 0.98, 0.6154, 0.9867, 1.0, 0.96, 0.5333, 0.9492, 0.9959, 1.0, 0.9753, 0.0, 0.0, 1.0, 0.7188, 0.75, 0.0, 0.5618, 0.9817, 0.9907, 0.8082, 0.375, 1.0, 0.1899, 0.4189, 0.2059, 0.9769, 0.9976, 0.8082, 0.0, 0.2941, 0.0008, 0.9758, 0.0, 0.3737, 0.9903, 0.7532, 0.9965, 0.6567, 0.9295, 0.0, 0.9961, 1.0, 0.0, 0.9259, 0.989, 0.9681, 0.9672, 0.9825, 0.4087, 1.0, 0.0, 1.0, 0.9927, 0.9629, 0.4362, 0.9733, 0.8421, 0.8696, 0.9833, 0.6667, 0.0, 0.9886, 0.0, 0.2121, 0.0, 0.0, 0.0, 0.0132, 0.0, 0.9737, 1.0, 0.0, 0.9929, 0.6207, 1.0, 0.9796, 0.9545, 0.8465, 0.9363, 0.8315, 0.9505, 0.9967, 0.9664, 0.9961, 0.4, 0.916, 0.7333, 0.9938, 0.97, 0.7941, 1.0, 0.9657, 0.9943, 0.0, 0.9348, 0.8214, 1.0, 0.9954, 0.0769, 0.0625, 0.9791, 0.9299, 0.993, 0.0, 0.5], 'micro': 0.8635, 'macro': 0.6211, 'weighted': 0.8441}
2024-08-04 09:36:22 - [34m[1mLOGS   [0m - Best checkpoint with score 0.62 saved at /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_best.pt
2024-08-04 09:36:22 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_score_0.3370.pt
2024-08-04 09:36:22 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.4841.pt', 'checkpoint_score_0.5377.pt', 'checkpoint_score_0.5723.pt', 'checkpoint_score_0.5985.pt', 'checkpoint_score_0.6211.pt']
2024-08-04 09:36:24 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_avg.pt
2024-08-04 09:36:24 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt
2024-08-04 09:36:25 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_last.pt
2024-08-04 09:36:25 - [34m[1mLOGS   [0m - Training checkpoint for epoch 7/iteration 977 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_epoch_7_iter_977.pt
2024-08-04 09:36:25 - [34m[1mLOGS   [0m - Model state for epoch 7/iteration 977 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_epoch_7_iter_977.pt
[31m===========================================================================[0m
2024-08-04 09:36:27 - [32m[1mINFO   [0m - Training epoch 8
2024-08-04 09:36:29 - [34m[1mLOGS   [0m - Epoch:   8 [     978/10000000], loss: {'classification': 3.8912, 'neural_augmentation': 0.2997, 'total_loss': 4.1909}, LR: [4.3e-05, 4.3e-05], Avg. batch load time: 1.914, Elapsed time:  2.07
2024-08-04 09:36:48 - [34m[1mLOGS   [0m - *** Training summary for epoch 8
	 loss={'classification': 3.8888, 'neural_augmentation': 0.2975, 'total_loss': 4.1863}
2024-08-04 09:37:35 - [34m[1mLOGS   [0m - *** Validation summary for epoch 8
	 loss={'classification': 3.5367, 'neural_augmentation': 0.0, 'total_loss': 3.5367} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.8283, 0.7182, 0.5084, 0.8977, 0.807, 0.25, 0.2778, 0.2857, 0.7823, 0.6143, 0.2991, 0.7696, 0.7628, 0.7147, 0.7943, 0.7547, 0.6878, 0.7675, 0.2524, 0.4, 0.9159, 0.8511, 0.7449, 0.043, 0.8079, 0.3, 0.5921, 0.6364, 0.2857, 0.8764, 0.8202, 0.9343, 0.8, 0.9333, 0.1798, 0.0357, 0.0014, 0.1481, 0.2857, 0.7928, 0.7563, 0.9514, 0.25, 0.5626, 0.2727, 0.747, 0.7975, 0.4848, 0.6444, 0.8414, 0.3415, 0.8929, 0.9817, 0.7189, 0.4398, 0.8725, 0.7582, 0.8195, 0.7207, 0.6397, 0.7422, 0.4118, 0.8856, 0.8546, 0.2222, 0.2385, 0.4835, 0.4262, 0.9364, 0.7842, 0.8125, 0.7584, 0.25, 0.8652, 0.793, 0.9731, 0.9863, 0.1795, 0.0062, 0.6923, 0.8241, 0.8197, 0.8733, 0.8484, 0.7268, 0.8095, 0.6584, 0.419, 0.5075, 0.9507, 0.7479, 0.7436, 0.8734, 0.1053, 0.5557, 0.6818, 0.866, 0.6678, 0.7083, 0.2637, 0.0816, 0.8954, 0.8976, 0.2222, 0.8919, 0.984, 0.0061, 0.1538, 0.9231, 0.8618, 0.1667, 0.2, 0.6752, 0.9495, 0.8835, 0.0012, 0.0741, 0.5053, 0.984, 0.0155, 0.6905, 0.9475, 0.8624, 0.7778, 0.9545, 0.8661, 0.3077, 0.3265, 0.9107, 0.8117, 0.8716, 0.2074, 0.9784, 0.8139, 0.3738, 0.8228, 0.6126, 0.2419, 0.8558, 0.3103, 0.6362, 0.8238, 0.8604, 0.5695, 0.9677, 0.4364, 0.8766, 0.0079, 0.8862, 0.9554, 0.9109, 0.8259, 0.3333, 0.4, 0.4906, 0.9592, 0.8649, 0.7355, 0.8768, 0.9075, 0.525, 0.8706, 0.897, 0.4, 0.2769, 0.2857, 0.2759, 0.0002, 0.1481, 0.4, 0.4706, 0.9499, 0.8712, 0.8852, 0.8802, 0.1091, 0.2778, 0.9343, 0.7198, 0.8, 0.9803, 0.9485, 0.48, 0.4615, 0.8889, 0.7845, 0.0002, 0.9298, 0.7083, 0.9628, 0.9657, 0.5, 0.6886, 0.3582, 0.481, 0.6242, 0.8828, 0.8378, 0.9387, 0.0096, 0.947, 0.9532, 0.7632, 0.5581, 0.9648, 0.9844, 0.3333, 0.8623, 0.8976, 0.3529, 0.9274, 0.9592, 0.8, 0.9761, 0.0007, 0.3158, 0.9238, 0.9253, 0.0112, 0.8475, 0.5763, 0.9558, 0.6322, 0.952, 0.6723, 0.1818, 0.4308, 0.48, 0.0588, 0.7757, 0.9524, 0.3529, 0.1905, 0.8025, 0.0044, 0.874, 0.4938, 0.8949, 0.9115, 0.2759, 0.1481, 0.6444, 0.4737, 0.0377, 1.0, 0.9158, 0.9243, 0.4706, 0.9168, 0.5765, 0.924, 0.9333, 0.8859, 0.5738, 0.8011, 0.9856, 0.967, 0.9355, 0.3794, 0.027, 0.96, 0.6333, 0.7045, 0.0038, 0.5332, 0.9573, 0.9615, 0.7559, 0.495, 0.9679, 0.4767, 0.589, 0.4194, 0.9565, 0.9296, 0.75, 0.25, 0.4956, 0.8299, 0.8898, 0.0101, 0.4444, 0.9619, 0.6974, 0.9627, 0.6338, 0.8952, 0.087, 0.955, 0.8182, 0.0081, 0.7077, 0.7685, 0.9206, 0.886, 0.9363, 0.4793, 0.9632, 0.1818, 0.9689, 0.9779, 0.7664, 0.4961, 0.8918, 0.7273, 0.76, 0.9565, 0.7778, 0.0092, 0.9248, 0.1667, 0.5424, 0.186, 0.08, 0.4, 0.3206, 0.0439, 0.9386, 0.9906, 0.0171, 0.9892, 0.5965, 0.9791, 0.9485, 0.8934, 0.6951, 0.7889, 0.7649, 0.8557, 0.9443, 0.8855, 0.9861, 0.5455, 0.8421, 0.6667, 0.9623, 0.9075, 0.7541, 0.9524, 0.9504, 0.9565, 0.2857, 0.8523, 0.7059, 0.7892, 0.9791, 0.4, 0.3333, 0.9465, 0.914, 0.9822, 0.2963, 0.6667], 'AP': [0.8999, 0.7722, 0.4937, 0.9204, 0.8435, 0.2348, 0.1119, 0.1592, 0.8406, 0.6364, 0.1653, 0.8022, 0.8303, 0.7736, 0.7875, 0.8398, 0.7447, 0.8377, 0.1562, 0.3212, 0.957, 0.9107, 0.7404, 0.0061, 0.8791, 0.1918, 0.5611, 0.677, 0.1469, 0.9279, 0.8029, 0.9591, 0.8395, 0.9585, 0.067, 0.0049, 0.0006, 0.0504, 0.115, 0.8137, 0.6978, 0.9703, 0.126, 0.5348, 0.1538, 0.7012, 0.849, 0.3272, 0.5533, 0.9156, 0.1972, 0.938, 0.9921, 0.7922, 0.3286, 0.8788, 0.8103, 0.8769, 0.7815, 0.6686, 0.8078, 0.3003, 0.9214, 0.9169, 0.0531, 0.1536, 0.3722, 0.3037, 0.9774, 0.8105, 0.8171, 0.7593, 0.0903, 0.9226, 0.8317, 0.9868, 0.9964, 0.0676, 0.0016, 0.5998, 0.834, 0.9029, 0.9295, 0.9156, 0.7603, 0.8645, 0.6544, 0.3949, 0.4841, 0.965, 0.8139, 0.6777, 0.9292, 0.0282, 0.561, 0.7469, 0.9122, 0.7089, 0.701, 0.1358, 0.0243, 0.9199, 0.9449, 0.068, 0.8974, 0.9931, 0.0016, 0.0345, 0.9435, 0.9085, 0.06, 0.1129, 0.6792, 0.9777, 0.8977, 0.0006, 0.0211, 0.4447, 0.9967, 0.0048, 0.6724, 0.978, 0.9128, 0.8339, 0.9801, 0.9252, 0.131, 0.1514, 0.9285, 0.8593, 0.932, 0.071, 0.9853, 0.8908, 0.3092, 0.8896, 0.6463, 0.0818, 0.9038, 0.2025, 0.6627, 0.8543, 0.8761, 0.5638, 0.975, 0.433, 0.9403, 0.0012, 0.9306, 0.9864, 0.9665, 0.8864, 0.2449, 0.1806, 0.3491, 0.9839, 0.9152, 0.755, 0.9342, 0.9347, 0.4971, 0.9007, 0.9481, 0.3222, 0.1208, 0.0952, 0.1166, 0.0001, 0.105, 0.3104, 0.3885, 0.986, 0.9216, 0.9107, 0.9061, 0.0186, 0.0989, 0.9678, 0.7767, 0.8354, 0.9964, 0.9765, 0.4592, 0.3158, 0.9447, 0.8307, 0.0001, 0.9395, 0.7412, 0.9783, 0.9716, 0.4261, 0.663, 0.1693, 0.3473, 0.6822, 0.923, 0.8737, 0.9645, 0.004, 0.9673, 0.9816, 0.8127, 0.4436, 0.9924, 0.9973, 0.2662, 0.932, 0.9377, 0.293, 0.9573, 0.9675, 0.7756, 0.9692, 0.0003, 0.134, 0.967, 0.957, 0.0016, 0.9068, 0.5415, 0.9643, 0.6372, 0.9696, 0.6353, 0.072, 0.3266, 0.5106, 0.0291, 0.8582, 0.9672, 0.3119, 0.0924, 0.844, 0.0014, 0.9278, 0.4481, 0.9368, 0.9269, 0.1711, 0.0697, 0.6939, 0.2909, 0.0073, 1.0, 0.9626, 0.9611, 0.3785, 0.9603, 0.5376, 0.9717, 0.9094, 0.9338, 0.4782, 0.8576, 0.9964, 0.9895, 0.9688, 0.2788, 0.0066, 0.9779, 0.6752, 0.7752, 0.0009, 0.5497, 0.9882, 0.9928, 0.817, 0.4296, 0.9936, 0.4376, 0.5297, 0.3021, 0.9672, 0.9772, 0.7559, 0.1296, 0.4665, 0.8724, 0.9472, 0.0027, 0.3963, 0.946, 0.6858, 0.9915, 0.6398, 0.93, 0.0183, 0.9815, 0.8464, 0.002, 0.6972, 0.7896, 0.9629, 0.9482, 0.9711, 0.4746, 0.9916, 0.0627, 0.9961, 0.9906, 0.8648, 0.4216, 0.9395, 0.7578, 0.7773, 0.9896, 0.7618, 0.0024, 0.9607, 0.0938, 0.5027, 0.0817, 0.0225, 0.1952, 0.2278, 0.013, 0.9678, 0.9963, 0.0027, 0.9926, 0.6546, 0.9977, 0.9761, 0.9467, 0.7384, 0.8492, 0.8142, 0.9156, 0.9809, 0.9503, 0.9974, 0.4388, 0.9002, 0.6383, 0.9841, 0.9561, 0.7829, 0.9858, 0.9631, 0.9812, 0.2633, 0.9052, 0.6749, 0.85, 0.9912, 0.2818, 0.2267, 0.9719, 0.9287, 0.9912, 0.1281, 0.5001], 'Recall@P=50': [0.976, 0.8183, 0.4979, 0.9792, 0.9385, 0.1, 0.1667, 0.0426, 0.9217, 0.6767, 0.0488, 0.8956, 0.8734, 0.8379, 0.8421, 0.9195, 0.8067, 0.9064, 0.0732, 0.3131, 0.9771, 0.9576, 0.7861, 0.0, 0.9087, 0.0256, 0.6495, 0.7314, 0.0, 0.9501, 0.802, 0.9739, 0.9111, 0.9686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8153, 0.9231, 0.9751, 0.0114, 0.5842, 0.0588, 0.0213, 0.9423, 0.0469, 0.7209, 0.9529, 0.0625, 0.9701, 0.9955, 0.8492, 0.0105, 0.9257, 0.8832, 0.9549, 0.8902, 0.7443, 0.8822, 0.2917, 0.9406, 0.9561, 0.0, 0.0519, 0.2982, 0.0256, 0.9952, 0.9181, 0.9508, 0.8116, 0.0, 0.9558, 0.852, 0.991, 1.0, 0.0, 0.0, 0.625, 0.8852, 0.9661, 0.9608, 0.9712, 0.8579, 0.9139, 0.697, 0.2778, 0.3182, 0.9665, 0.8735, 0.75, 0.9621, 0.0, 0.6029, 0.7877, 0.9574, 0.7423, 0.7033, 0.0882, 0.0, 0.9391, 0.981, 0.0, 0.9167, 1.0, 0.0, 0.0, 0.9474, 0.9327, 0.0, 0.1111, 0.8261, 0.9895, 0.9111, 0.0, 0.0, 0.3721, 1.0, 0.0, 0.9091, 0.9876, 0.9735, 0.8608, 1.0, 0.9506, 0.0, 0.0, 0.9253, 0.8974, 0.9653, 0.0, 0.9858, 0.9533, 0.1522, 0.9724, 0.7291, 0.0, 0.9184, 0.1304, 0.699, 0.8932, 0.9404, 0.5765, 0.9769, 0.3684, 0.9905, 0.0, 0.9585, 0.9896, 0.9906, 0.951, 0.1026, 0.0, 0.1667, 0.9939, 0.9303, 0.8, 0.9634, 0.9489, 0.525, 0.9333, 0.9648, 0.2857, 0.0, 0.0, 0.0, 0.0, 0.0588, 0.25, 0.3509, 0.9942, 0.9617, 0.002, 0.9465, 0.0, 0.0, 0.9805, 0.8565, 0.84, 1.0, 0.9845, 0.4231, 0.3158, 0.9912, 0.9358, 0.0, 0.9412, 0.8378, 0.9828, 0.9713, 0.0, 0.7826, 0.0, 0.2105, 0.7325, 0.9368, 0.9899, 0.9756, 0.0, 0.9704, 0.9892, 0.872, 0.0636, 1.0, 1.0, 0.2222, 0.9716, 0.9537, 0.24, 0.9798, 0.9798, 0.8125, 0.9664, 0.0, 0.0, 0.982, 0.9787, 0.0, 0.9358, 0.614, 0.9648, 0.8125, 0.9825, 0.902, 0.0, 0.0312, 0.3846, 0.0, 0.9284, 0.9645, 0.0968, 0.1111, 0.8848, 0.0, 0.9746, 0.1944, 1.0, 0.9893, 0.05, 0.0526, 0.7351, 0.1765, 0.0, 1.0, 0.9855, 0.9808, 0.0147, 0.988, 0.0154, 0.9924, 1.0, 0.96, 0.0667, 0.9576, 1.0, 0.9946, 0.9788, 0.0098, 0.0, 1.0, 0.6562, 0.8, 0.0, 0.5594, 1.0, 1.0, 0.8562, 0.4643, 1.0, 0.3038, 0.6284, 0.1765, 0.9769, 0.9976, 0.8082, 0.1667, 0.4265, 0.0008, 0.9677, 0.0, 0.0202, 0.9903, 0.7403, 1.0, 0.7015, 0.9463, 0.0, 0.9922, 1.0, 0.0, 0.963, 0.989, 0.9787, 0.9719, 0.9883, 0.4522, 1.0, 0.0, 1.0, 0.9927, 0.9629, 0.0213, 0.9786, 0.7368, 0.8696, 1.0, 0.7778, 0.0, 0.9943, 0.0, 0.5152, 0.0, 0.0, 0.0, 0.0132, 0.0, 0.9737, 1.0, 0.0, 0.9929, 0.5862, 1.0, 0.9864, 0.958, 0.8596, 0.9108, 0.839, 0.959, 0.9967, 0.979, 0.9961, 0.4, 0.9466, 0.6667, 0.9938, 0.9733, 0.8235, 0.9955, 0.96, 0.9829, 0.0526, 0.9457, 0.7857, 1.0, 0.9909, 0.2308, 0.0208, 0.9843, 0.9533, 0.993, 0.0, 0.5], 'micro': 0.8716, 'macro': 0.6369, 'weighted': 0.8524}
2024-08-04 09:37:54 - [34m[1mLOGS   [0m - Best checkpoint with score 0.64 saved at /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_best.pt
2024-08-04 09:37:54 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_score_0.4841.pt
2024-08-04 09:37:54 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.5377.pt', 'checkpoint_score_0.5723.pt', 'checkpoint_score_0.5985.pt', 'checkpoint_score_0.6211.pt', 'checkpoint_score_0.6369.pt']
2024-08-04 09:37:55 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_avg.pt
2024-08-04 09:37:56 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_last.pt
2024-08-04 09:37:56 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_last.pt
2024-08-04 09:37:56 - [34m[1mLOGS   [0m - Training checkpoint for epoch 8/iteration 1101 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/training_checkpoint_epoch_8_iter_1101.pt
2024-08-04 09:37:56 - [34m[1mLOGS   [0m - Model state for epoch 8/iteration 1101 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_noc/ingredient_172/train/checkpoint_epoch_8_iter_1101.pt
[31m===========================================================================[0m
2024-08-04 09:37:58 - [32m[1mINFO   [0m - Training epoch 9
2024-08-04 09:38:00 - [34m[1mLOGS   [0m - Epoch:   9 [    1102/10000000], loss: {'classification': 3.905, 'neural_augmentation': 0.3173, 'total_loss': 4.2223}, LR: [4.1e-05, 4.1e-05], Avg. batch load time: 1.517, Elapsed time:  1.80
2024-08-04 09:38:20 - [34m[1mLOGS   [0m - *** Training summary for epoch 9
	 loss={'classification': 3.6865, 'neural_augmentation': 0.2991, 'total_loss': 3.9857}
Terminated
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1608 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
