nohup: ignoring input
2024-08-04 06:38:51 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
../projects/catlip/multi_label_image_classification/food101/foodv_small.yaml
{'taskname': '+ ViT-B/16 [COCO-CLS]', 'common': {'run_label': 'train', 'log_freq': 500, 'auto_resume': True, 'mixed_precision': True, 'mixed_precision_dtype': 'bfloat16', 'grad_clip': 1.0, 'save_all_checkpoints': True}, 'dataset': {'root': '/ML-A100/team/mm/models/food101/food101', 'train_batch_size0': 128, 'val_batch_size0': 50, 'eval_batch_size0': 50, 'workers': 64, 'persistent_workers': True, 'pin_memory': True, 'name': 'food172_ingredient', 'category': 'classification'}, 'image_augmentation': {'random_resized_crop': {'enable': True, 'interpolation': 'bilinear'}, 'random_horizontal_flip': {'enable': True}, 'resize': {'enable': True, 'size': 232, 'interpolation': 'bilinear'}, 'center_crop': {'enable': True, 'size': 224}}, 'sampler': {'name': 'variable_batch_sampler', 'vbs': {'crop_size_width': 224, 'crop_size_height': 224, 'max_n_scales': 25, 'min_crop_size_width': 128, 'max_crop_size_width': 320, 'min_crop_size_height': 128, 'max_crop_size_height': 320, 'check_scale': 16}}, 'loss': {'category': 'composite_loss', 'composite_loss': [{'loss_category': 'classification', 'loss_weight': 1.0, 'classification': {'name': 'binary_cross_entropy', 'binary_cross_entropy': {'reduction': 'batch_mean'}}}, {'loss_category': 'neural_augmentation', 'loss_weight': 1.0, 'neural_augmentation': {'perceptual_metric': 'psnr', 'target_value': [40, 20], 'curriculum_method': 'cosine'}}]}, 'optim': {'name': 'adamw', 'weight_decay': 0.05, 'no_decay_bn_filter_bias': True, 'adamw': {'beta1': 0.9, 'beta2': 0.999}}, 'scheduler': {'name': 'cosine', 'max_epochs': 30, 'warmup_iterations': 500, 'warmup_init_lr': 1e-06, 'cosine': {'max_lr': 5e-05, 'min_lr': 5e-06}}, 'model': {'activation_checkpointing': True, 'resume_exclude_scopes': ['classifier'], 'classification': {'name': 'foodv', 'n_classes': 174, 'pretrained': '/ML-A100/team/mm/models/catlip_data/results_small_dci/train/checkpoint_epoch_9_iter_79046.pt', 'foodv': {'mode': 'small', 'norm_layer': 'layer_norm_fp32', 'use_flash_attention': True}}, 'learn_augmentation': {'brightness': True, 'contrast': True, 'noise': True, 'mode': 'distribution'}, 'activation': {'name': 'gelu'}, 'layer': {'conv_init': 'kaiming_normal', 'linear_init': 'trunc_normal', 'linear_init_std_dev': 0.02}}, 'ema': {'enable': False, 'momentum': 0.0005}, 'stats': {'train': ['loss'], 'val': ['loss', 'multiclass_classification_pr(pred=logits)'], 'checkpoint_metric': 'multiclass_classification_pr(pred=logits).macro', 'checkpoint_metric_max': True, 'metrics': {'multiclass_classification_pr': {'suppress_warnings': True}}}}
small
dci
2024-08-04 06:38:52 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/catlip_data/results_small_dci/train/checkpoint_epoch_9_iter_79046.pt
2024-08-04 06:38:52 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-08-04 06:38:52 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=174, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =   25.744 M
Total trainable parameters =   25.744 M

2024-08-04 06:38:52 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-08-04 06:38:52 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 25.744M                | 3.385G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  0.116G    |
|   pool.proj                          |   1.18M                |   0.116G   |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.502M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  51.38M    |
|   mlp.0                              |   0.263M               |   25.69M   |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   25.69M   |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  89.262K               |  89.088K   |
|   classifier.weight                  |   (174, 512)           |            |
|   classifier.bias                    |   (174,)               |            |
2024-08-04 06:38:52 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-08-04 06:38:52 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks.6.attn.q_norm', 'blocks1.0.attn.q_norm', 'neural_augmentor.noise', 'blocks1.0.attn.attn_drop', 'blocks.4.drop_path2', 'blocks1.3.attn.q_norm', 'blocks.4.drop_path1', 'blocks.0.drop_path1', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks.6.attn.attn_drop', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks1.5.drop_path1', 'blocks.0.attn.k_norm', 'patch_embed.backbone.stages.0.0.down', 'blocks.3.drop_path1', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks.5.attn.q_norm', 'blocks1.2.attn.q_norm', 'blocks.0.ls1', 'blocks1.3.attn.attn_drop', 'blocks1.3.ls1', 'blocks1.0.drop_path2', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'norm', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks1.4.attn.q_norm', 'patch_embed.backbone.stages.1.3.down', 'blocks1.3.drop_path2', 'blocks.5.ls2', 'blocks1.4.attn.k_norm', 'blocks.4.ls1', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks.4.ls2', 'blocks.5.drop_path2', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks.4.attn.q_norm', 'blocks.1.drop_path1', 'blocks.6.ls1', 'blocks.1.drop_path2', 'patch_embed.backbone.stages.1.0.down', 'blocks.2.attn.attn_drop', 'blocks1.0.ls2', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks.5.attn.attn_drop', 'blocks1.0.drop_path1', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'patch_embed.backbone.stages.1.1.down', 'blocks.1.attn.k_norm', 'blocks1.0.attn.k_norm', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks.6.drop_path1', 'blocks1.2.attn.attn_drop', 'blocks.2.ls1', 'blocks1.3.ls2', 'blocks1.6.drop_path2', 'neural_augmentor.brightness.min_fn', 'blocks1.6.ls1', 'patch_embed.backbone.stages.1.2.shortcut', 'patch_embed.proj', 'blocks1.1.ls2', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.2.ls2', 'blocks1.5.ls2', 'blocks.0.ls2', 'patch_drop', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks1.6.attn.k_norm', 'blocks.5.drop_path1', 'blocks1.6.attn.q_norm', 'blocks.2.ls2', 'blocks.3.attn.q_norm', 'blocks1.1.drop_path1', 'blocks.6.attn.k_norm', 'neural_augmentor.brightness', 'patch_embed.backbone.stages.0.1.drop_path', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks1.5.drop_path2', 'blocks1.3.drop_path1', 'blocks1.3.attn.k_norm', 'blocks1.4.ls1', 'blocks.2.attn.q_norm', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks1.5.ls1', 'blocks.2.drop_path1', 'blocks1.1.drop_path2', 'blocks.4.attn.attn_drop', 'blocks.5.attn.k_norm', 'neural_augmentor.noise.max_fn', 'blocks1.1.attn.attn_drop', 'blocks.6.drop_path2', 'neural_augmentor.contrast.max_fn', 'blocks1.4.attn.attn_drop', 'blocks.0.attn.attn_drop', 'blocks.3.attn.k_norm', 'neural_augmentor.noise.min_fn', 'blocks.2.attn.k_norm', 'blocks1.5.attn.q_norm', 'blocks1.0.ls1', 'neural_augmentor.contrast', 'blocks1.4.drop_path1', 'blocks.1.attn.attn_drop', 'blocks.2.drop_path2', 'blocks.4.attn.k_norm', 'patch_embed.backbone.stages.1.2.down', 'blocks1.1.ls1', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks.3.ls1', 'blocks.1.ls2', 'neural_augmentor', 'blocks1.4.ls2', 'blocks.3.attn.attn_drop', 'blocks.5.ls1', 'blocks1.2.ls1', 'blocks1.5.attn.k_norm', 'neural_augmentor.brightness.max_fn', 'blocks1.1.attn.k_norm', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks.0.attn.q_norm', 'blocks1.6.attn.attn_drop', 'blocks1.2.drop_path1', 'patch_embed.backbone.stem.norm1.drop', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks.1.ls1', 'blocks1.6.ls2', 'blocks.6.ls2', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks.3.ls2', 'blocks1.2.attn.k_norm', 'blocks.1.attn.q_norm', 'blocks.3.drop_path2', 'blocks1.1.attn.q_norm', 'patch_embed.backbone.stages.0.1.shortcut', 'patch_embed.backbone.stages.0.1.down', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks1.5.attn.attn_drop', 'norm_pre', 'blocks1.6.drop_path1', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks1.2.drop_path2', 'blocks1.4.drop_path2', 'blocks.0.drop_path2', 'neural_augmentor.contrast.min_fn'}
2024-08-04 06:38:52 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::add_': 14, 'aten::avg_pool2d': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-08-04 06:38:52 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-08-04 06:38:52 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-08-04 06:38:53 - [34m[1mLOGS   [0m - Available GPUs: 4
2024-08-04 06:38:53 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-08-04 06:38:53 - [34m[1mLOGS   [0m - Setting --ddp.world-size the same as the number of available gpus.
2024-08-04 06:38:53 - [34m[1mLOGS   [0m - Directory created at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train
2024-08-04 06:39:00 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:30001
/ML-A100/team/mm/models/food101/food101
/ML-A100/team/mm/models/food101/food101
small
dci
2024-08-04 06:39:02 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:30001
/ML-A100/team/mm/models/food101/food101
/ML-A100/team/mm/models/food101/food101
small
dci
2024-08-04 06:38:58 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:30001
/ML-A100/team/mm/models/food101/food101
/ML-A100/team/mm/models/food101/food101
small
dci
2024-08-04 06:38:55 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:30001
/ML-A100/team/mm/models/food101/food101
2024-08-04 06:39:04 - [34m[1mLOGS   [0m - Training dataset details are given below
food172ingredient_lassification(
	root=/ML-A100/team/mm/models/food101/food101 
	is_training=True 
	num_samples=75750
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
)
/ML-A100/team/mm/models/food101/food101
2024-08-04 06:39:04 - [34m[1mLOGS   [0m - Validation dataset details are given below
food172ingredient_lassification(
	root=/ML-A100/team/mm/models/food101/food101 
	is_training=False 
	num_samples=25250
	transforms=Compose(
			Resize(size=232, interpolation=bilinear, maintain_aspect_ratio=True), 
			CenterCrop(size=(h=224, w=224)), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
)
2024-08-04 06:39:04 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=128
	 scales=[(128, 128, 392), (144, 144, 309), (160, 160, 250), (176, 176, 207), (192, 192, 174), (208, 208, 148), (224, 224, 128), (240, 240, 111), (256, 256, 98), (272, 272, 86), (288, 288, 77), (304, 304, 69), (320, 320, 62)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-08-04 06:39:04 - [34m[1mLOGS   [0m - Validation sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=50
	 scales=[(224, 224, 50)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-08-04 06:39:04 - [34m[1mLOGS   [0m - Number of data workers: 64
small
dci
2024-08-04 06:39:08 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/catlip_data/results_small_dci/train/checkpoint_epoch_9_iter_79046.pt
2024-08-04 06:39:08 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-08-04 06:39:08 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=174, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =   25.744 M
Total trainable parameters =   25.744 M

2024-08-04 06:39:08 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-08-04 06:39:08 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 25.744M                | 3.385G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  0.116G    |
|   pool.proj                          |   1.18M                |   0.116G   |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.502M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  51.38M    |
|   mlp.0                              |   0.263M               |   25.69M   |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   25.69M   |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  89.262K               |  89.088K   |
|   classifier.weight                  |   (174, 512)           |            |
|   classifier.bias                    |   (174,)               |            |
2024-08-04 06:39:08 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-08-04 06:39:08 - [33m[1mWARNING[0m - Uncalled Modules:
{'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks.5.ls2', 'blocks.4.ls1', 'blocks1.5.drop_path1', 'blocks1.6.drop_path1', 'blocks.1.ls1', 'blocks1.5.attn.k_norm', 'blocks1.3.ls2', 'patch_embed.backbone.stages.1.1.shortcut', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks1.0.drop_path1', 'blocks1.6.drop_path2', 'neural_augmentor.contrast.max_fn', 'blocks.6.drop_path1', 'blocks1.2.drop_path1', 'blocks1.4.attn.q_norm', 'patch_embed.backbone.stages.1.2.shortcut', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks1.6.attn.q_norm', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks.5.attn.q_norm', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks.3.ls2', 'blocks1.2.ls1', 'blocks1.4.drop_path1', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks.0.ls2', 'blocks1.2.ls2', 'blocks.1.attn.attn_drop', 'blocks1.1.ls1', 'blocks1.6.attn.attn_drop', 'blocks.0.drop_path2', 'blocks1.4.drop_path2', 'blocks1.4.attn.k_norm', 'blocks.6.ls1', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'patch_embed.backbone.stages.1.3.down', 'patch_embed.proj', 'blocks1.1.attn.attn_drop', 'patch_embed.backbone.stages.1.0.drop_path', 'neural_augmentor.noise.min_fn', 'blocks.5.drop_path2', 'blocks1.2.attn.attn_drop', 'neural_augmentor.brightness', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks1.5.drop_path2', 'blocks1.3.attn.q_norm', 'blocks.0.attn.attn_drop', 'blocks1.3.attn.k_norm', 'blocks.4.attn.k_norm', 'blocks.1.attn.k_norm', 'blocks1.5.attn.q_norm', 'blocks.2.attn.q_norm', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'patch_embed.backbone.stages.1.1.down', 'blocks1.3.attn.attn_drop', 'blocks1.0.drop_path2', 'blocks1.4.ls1', 'blocks.4.attn.attn_drop', 'patch_embed.backbone.stages.0.0.down', 'blocks1.0.attn.q_norm', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'neural_augmentor.contrast.min_fn', 'blocks1.3.drop_path2', 'patch_embed.backbone.stages.0.0.drop_path', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks.6.ls2', 'neural_augmentor.noise', 'blocks.1.drop_path2', 'blocks1.6.ls1', 'blocks.3.ls1', 'neural_augmentor.brightness.min_fn', 'patch_embed.backbone.stages.1.0.down', 'blocks.2.drop_path2', 'blocks1.0.ls1', 'neural_augmentor.contrast', 'blocks.5.attn.attn_drop', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks.3.attn.attn_drop', 'blocks1.5.attn.attn_drop', 'blocks1.1.attn.k_norm', 'patch_embed.backbone.stem.norm1.drop', 'blocks.0.drop_path1', 'blocks.3.drop_path2', 'blocks.0.ls1', 'blocks1.2.drop_path2', 'blocks.4.drop_path1', 'patch_embed.backbone.stages.1.3.shortcut', 'patch_drop', 'blocks1.6.attn.k_norm', 'blocks.5.attn.k_norm', 'blocks1.4.ls2', 'blocks1.1.ls2', 'neural_augmentor', 'blocks.4.ls2', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks.5.ls1', 'blocks1.2.attn.k_norm', 'norm_pre', 'neural_augmentor.brightness.max_fn', 'blocks1.1.drop_path2', 'patch_embed.backbone.stages.1.2.down', 'blocks.4.drop_path2', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks.2.ls1', 'blocks.0.attn.q_norm', 'blocks1.3.ls1', 'blocks.3.drop_path1', 'blocks.5.drop_path1', 'blocks.2.attn.attn_drop', 'blocks1.1.drop_path1', 'neural_augmentor.noise.max_fn', 'blocks1.5.ls1', 'blocks.1.ls2', 'blocks.6.attn.k_norm', 'blocks1.3.drop_path1', 'blocks1.6.ls2', 'blocks.2.ls2', 'blocks.4.attn.q_norm', 'blocks.0.attn.k_norm', 'blocks.2.attn.k_norm', 'blocks.1.drop_path1', 'blocks1.2.attn.q_norm', 'blocks1.0.ls2', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks.6.attn.q_norm', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks1.0.attn.attn_drop', 'blocks.6.attn.attn_drop', 'norm', 'blocks1.5.ls2', 'blocks.1.attn.q_norm', 'blocks.2.drop_path1', 'blocks.3.attn.q_norm', 'blocks1.0.attn.k_norm', 'blocks.6.drop_path2', 'patch_embed.backbone.stages.0.1.down', 'blocks1.4.attn.attn_drop', 'blocks1.1.attn.q_norm', 'blocks.3.attn.k_norm'}
2024-08-04 06:39:08 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::add_': 14, 'aten::avg_pool2d': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-08-04 06:39:08 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-08-04 06:39:08 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	BinaryCrossEntropy(  reduction=batch_mean loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-08-04 06:39:08 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-08-04 06:39:08 - [34m[1mLOGS   [0m - Max. epochs for training: 30
2024-08-04 06:39:08 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=5e-06
 	 max_lr=5e-05
 	 period=30
 	 warmup_init_lr=1e-06
 	 warmup_iters=500
 )
2024-08-04 06:39:08 - [34m[1mLOGS   [0m - No checkpoint found at '/ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt'
2024-08-04 06:39:08 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/config.yaml[0m
[31m===========================================================================[0m
2024-08-04 06:39:10 - [32m[1mINFO   [0m - Training epoch 0
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [128, 512, 1, 1], strides() = [512, 1, 512, 512]
bucket_view.sizes() = [128, 512, 1, 1], strides() = [512, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-08-04 06:42:54 - [34m[1mLOGS   [0m - Epoch:   0 [       1/10000000], loss: {'classification': 238.235, 'neural_augmentation': 0.3245, 'total_loss': 238.5595}, LR: [1e-06, 1e-06], Avg. batch load time: 214.923, Elapsed time: 223.96
2024-08-04 06:43:18 - [34m[1mLOGS   [0m - *** Training summary for epoch 0
	 loss={'classification': 134.5234, 'neural_augmentation': 0.3206, 'total_loss': 134.8439}
2024-08-04 06:47:46 - [34m[1mLOGS   [0m - *** Validation summary for epoch 0
	 loss={'classification': 22.345, 'neural_augmentation': 0.0, 'total_loss': 22.345} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.1366, 0.0245, 0.1343, 0.0376, 0.0196, 0.0304, 0.1355, 0.0197, 0.1191, 0.2677, 0.0456, 0.2358, 0.0725, 0.0387, 0.0502, 0.0197, 0.1297, 0.0425, 0.0238, 0.0462, 0.0372, 0.061, 0.0249, 0.1089, 0.0584, 0.1016, 0.0448, 0.0449, 0.3663, 0.1011, 0.0196, 0.0197, 0.0422, 0.0328, 0.146, 0.0216, 0.0828, 0.4443, 0.096, 0.0208, 0.0842, 0.0711, 0.0195, 0.0205, 0.0195, 0.0564, 0.0575, 0.0337, 0.0644, 0.0195, 0.0229, 0.1418, 0.051, 0.1531, 0.1192, 0.1413, 0.0684, 0.024, 0.0251, 0.0269, 0.0195, 0.0198, 0.0229, 0.0326, 0.1123, 0.0416, 0.0255, 0.0601, 0.0697, 0.0198, 0.0213, 0.0424, 0.0196, 0.0941, 0.028, 0.068, 0.0195, 0.1138, 0.0755, 0.0295, 0.1484, 0.0427, 0.3475, 0.023, 0.0583, 0.0647, 0.383, 0.0311, 0.1025, 0.0203, 0.0737, 0.0195, 0.0298, 0.1515, 0.0205, 0.1966, 0.0269, 0.0201, 0.1149, 0.0199, 0.1486, 0.102, 0.0436, 0.1226, 0.0327, 0.2114, 0.0428, 0.0202, 0.0938, 0.0195, 0.0313, 0.0379, 0.4494, 0.0391, 0.0196, 0.1184, 0.0289, 0.1259, 0.0268, 0.0275, 0.1277, 0.1125, 0.0661, 0.0196, 0.213, 0.0284, 0.0357, 0.0741, 0.05, 0.0432, 0.0252, 0.0241, 0.0269, 0.02, 0.1392, 0.0611, 0.0597, 0.0358, 0.2304, 0.0257, 0.0216, 0.0195, 0.0399, 0.0813, 0.1022, 0.0195, 0.5485, 0.0843, 0.0204, 0.0386, 0.0414, 0.0421, 0.0574, 0.0302, 0.0592, 0.0245, 0.0199, 0.1005, 0.0195, 0.0206, 0.1351, 0.0198, 0.0224, 0.2999, 0.0211, 0.0195, 0.1126, 0.0416, 0.0485, 0.0204, 0.0223, 0.0196, 0.5024, 0.0268], 'AP': [0.0781, 0.011, 0.0737, 0.0147, 0.0082, 0.0131, 0.0765, 0.0089, 0.0648, 0.1775, 0.0232, 0.1547, 0.0291, 0.0153, 0.0246, 0.0076, 0.0717, 0.0198, 0.0111, 0.0164, 0.016, 0.0272, 0.012, 0.0599, 0.0269, 0.0555, 0.0204, 0.0187, 0.2243, 0.0551, 0.0082, 0.0084, 0.0168, 0.015, 0.0697, 0.01, 0.0322, 0.298, 0.05, 0.0102, 0.0342, 0.0271, 0.0079, 0.0094, 0.0078, 0.0185, 0.0219, 0.0148, 0.032, 0.0083, 0.0113, 0.0779, 0.0282, 0.0878, 0.0559, 0.0965, 0.0325, 0.0112, 0.0115, 0.0113, 0.0077, 0.0088, 0.0098, 0.0161, 0.0641, 0.021, 0.0118, 0.0304, 0.0362, 0.0083, 0.0104, 0.0203, 0.0071, 0.0519, 0.0129, 0.0318, 0.0066, 0.0566, 0.0386, 0.0135, 0.0824, 0.0167, 0.2568, 0.0113, 0.0293, 0.0318, 0.2768, 0.0141, 0.0554, 0.0093, 0.0332, 0.0081, 0.0134, 0.0817, 0.0096, 0.114, 0.0118, 0.0085, 0.0597, 0.0086, 0.0856, 0.0492, 0.0178, 0.0604, 0.014, 0.1127, 0.0213, 0.0088, 0.0379, 0.0083, 0.0145, 0.0168, 0.3759, 0.0186, 0.0091, 0.0618, 0.0127, 0.0675, 0.0119, 0.0126, 0.061, 0.0598, 0.027, 0.0065, 0.1263, 0.0132, 0.0135, 0.0412, 0.0197, 0.0196, 0.0121, 0.0107, 0.0128, 0.009, 0.0792, 0.0261, 0.0275, 0.0157, 0.1488, 0.0121, 0.01, 0.0076, 0.0189, 0.0399, 0.053, 0.0066, 0.4407, 0.0417, 0.0092, 0.0195, 0.0192, 0.0202, 0.025, 0.014, 0.0305, 0.0108, 0.0083, 0.0574, 0.0068, 0.0083, 0.0714, 0.0085, 0.0105, 0.23, 0.0101, 0.0072, 0.0607, 0.0192, 0.0207, 0.0079, 0.0102, 0.0077, 0.382, 0.0126], 'Recall@P=50': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003, 0.0, 0.0032, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.006, 0.0067, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004, 0.0, 0.0, 0.0, 0.0004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0699, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0003, 0.0, 0.0, 0.0013, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0409, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0001, 0.0], 'micro': 0.1934, 'macro': 0.0437, 'weighted': 0.1505}
2024-08-04 06:47:52 - [34m[1mLOGS   [0m - Best checkpoint with score 0.04 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:47:53 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:47:53 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:47:54 - [34m[1mLOGS   [0m - Training checkpoint for epoch 0/iteration 116 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_0_iter_116.pt
2024-08-04 06:47:54 - [34m[1mLOGS   [0m - Model state for epoch 0/iteration 116 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_0_iter_116.pt
[31m===========================================================================[0m
2024-08-04 06:47:56 - [32m[1mINFO   [0m - Training epoch 1
2024-08-04 06:48:00 - [34m[1mLOGS   [0m - Epoch:   1 [     117/10000000], loss: {'classification': 22.7807, 'neural_augmentation': 0.293, 'total_loss': 23.0737}, LR: [1.2e-05, 1.2e-05], Avg. batch load time: 3.984, Elapsed time:  4.38
2024-08-04 06:48:24 - [34m[1mLOGS   [0m - *** Training summary for epoch 1
	 loss={'classification': 21.1901, 'neural_augmentation': 0.3188, 'total_loss': 21.5089}
2024-08-04 06:48:49 - [34m[1mLOGS   [0m - *** Validation summary for epoch 1
	 loss={'classification': 18.9498, 'neural_augmentation': 0.0, 'total_loss': 18.9498} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.2838, 0.0767, 0.2572, 0.0775, 0.024, 0.5458, 0.2803, 0.1697, 0.4162, 0.3821, 0.0497, 0.4548, 0.1031, 0.2516, 0.1878, 0.1696, 0.3344, 0.2815, 0.032, 0.3067, 0.0386, 0.144, 0.0729, 0.323, 0.3286, 0.2423, 0.3409, 0.2136, 0.5023, 0.1835, 0.0273, 0.0837, 0.0732, 0.1628, 0.2727, 0.021, 0.2683, 0.5369, 0.2013, 0.0268, 0.5228, 0.0729, 0.1168, 0.6381, 0.1244, 0.5562, 0.168, 0.0577, 0.1569, 0.0695, 0.028, 0.1816, 0.2375, 0.2826, 0.3235, 0.5374, 0.4144, 0.0356, 0.2633, 0.0372, 0.1089, 0.061, 0.1835, 0.3268, 0.2583, 0.16, 0.0755, 0.4653, 0.2349, 0.0336, 0.0567, 0.0943, 0.1251, 0.3258, 0.1041, 0.2927, 0.1069, 0.351, 0.3714, 0.249, 0.3185, 0.1476, 0.4771, 0.1082, 0.359, 0.4163, 0.4499, 0.0407, 0.2078, 0.1713, 0.452, 0.0519, 0.1642, 0.2866, 0.0279, 0.3932, 0.0614, 0.0196, 0.3854, 0.2361, 0.3015, 0.1576, 0.2669, 0.2587, 0.6863, 0.3068, 0.2906, 0.0266, 0.2762, 0.0464, 0.1609, 0.2591, 0.5951, 0.087, 0.0279, 0.2906, 0.0434, 0.5668, 0.0516, 0.3072, 0.2034, 0.2046, 0.0748, 0.4742, 0.5181, 0.0416, 0.1627, 0.3559, 0.0363, 0.0771, 0.0356, 0.0451, 0.0617, 0.0756, 0.368, 0.6551, 0.1226, 0.6318, 0.3297, 0.0809, 0.0617, 0.1508, 0.2773, 0.3717, 0.1996, 0.2637, 0.6014, 0.14, 0.0876, 0.4692, 0.1182, 0.0605, 0.2127, 0.6277, 0.147, 0.1816, 0.1117, 0.2703, 0.1361, 0.0562, 0.4781, 0.0571, 0.1071, 0.3739, 0.0392, 0.1472, 0.3856, 0.2066, 0.4427, 0.0291, 0.0262, 0.1498, 0.5896, 0.0845], 'AP': [0.2134, 0.0295, 0.1735, 0.0329, 0.011, 0.5407, 0.2009, 0.0773, 0.3582, 0.3716, 0.0238, 0.4419, 0.0483, 0.1542, 0.1084, 0.0842, 0.3241, 0.1772, 0.0151, 0.2775, 0.0177, 0.0735, 0.0319, 0.2726, 0.2661, 0.1749, 0.2834, 0.1667, 0.4973, 0.1241, 0.0119, 0.0333, 0.0306, 0.073, 0.1973, 0.0099, 0.1435, 0.5201, 0.1273, 0.0127, 0.5385, 0.0294, 0.0528, 0.675, 0.0687, 0.5792, 0.0934, 0.0278, 0.0859, 0.027, 0.0116, 0.1078, 0.182, 0.2102, 0.2222, 0.5325, 0.3795, 0.0141, 0.1794, 0.016, 0.0458, 0.0274, 0.1129, 0.2569, 0.1775, 0.0913, 0.0314, 0.3841, 0.1379, 0.0141, 0.0256, 0.0434, 0.0579, 0.2461, 0.0388, 0.203, 0.0483, 0.2638, 0.3175, 0.1629, 0.2316, 0.0733, 0.4446, 0.0501, 0.2351, 0.3797, 0.4426, 0.018, 0.181, 0.0748, 0.4545, 0.0207, 0.0932, 0.2031, 0.0125, 0.3175, 0.0259, 0.0082, 0.3681, 0.1533, 0.2799, 0.087, 0.1941, 0.1682, 0.7007, 0.228, 0.1721, 0.0124, 0.2069, 0.0198, 0.0893, 0.1641, 0.6049, 0.0419, 0.0126, 0.2829, 0.019, 0.5574, 0.0222, 0.2416, 0.119, 0.1227, 0.0303, 0.4562, 0.5039, 0.0183, 0.0894, 0.3247, 0.0164, 0.0374, 0.0137, 0.0199, 0.0254, 0.0298, 0.2718, 0.6943, 0.0654, 0.6416, 0.2512, 0.0285, 0.0269, 0.0679, 0.1737, 0.2968, 0.1158, 0.1363, 0.5798, 0.0704, 0.0372, 0.3927, 0.0563, 0.0285, 0.1252, 0.624, 0.0811, 0.0934, 0.0469, 0.1899, 0.0698, 0.0244, 0.4799, 0.0234, 0.0513, 0.3401, 0.0175, 0.067, 0.364, 0.1074, 0.366, 0.0124, 0.012, 0.0746, 0.5672, 0.0368], 'Recall@P=50': [0.0007, 0.0, 0.0011, 0.0, 0.0, 0.004, 0.0008, 0.0, 0.2232, 0.1877, 0.0, 0.3484, 0.0, 0.0, 0.0, 0.0, 0.2408, 0.008, 0.0, 0.148, 0.0, 0.004, 0.0, 0.0008, 0.2413, 0.0008, 0.176, 0.092, 0.4501, 0.0016, 0.0, 0.0, 0.0, 0.008, 0.0005, 0.0, 0.004, 0.4232, 0.004, 0.0, 0.54, 0.0, 0.0, 0.74, 0.016, 0.576, 0.0013, 0.0, 0.0013, 0.0, 0.0, 0.0, 0.05, 0.0189, 0.186, 0.55, 0.274, 0.0, 0.02, 0.0, 0.0, 0.0, 0.036, 0.124, 0.004, 0.002, 0.0, 0.0227, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0368, 0.0, 0.004, 0.0, 0.0, 0.0013, 0.004, 0.0005, 0.0, 0.346, 0.0, 0.0, 0.256, 0.3062, 0.0, 0.1096, 0.0, 0.002, 0.0, 0.012, 0.0032, 0.0, 0.0015, 0.0, 0.0, 0.2813, 0.004, 0.1566, 0.0, 0.092, 0.004, 0.768, 0.0, 0.016, 0.0, 0.0248, 0.0, 0.016, 0.0, 0.7236, 0.002, 0.0, 0.1587, 0.0, 0.0007, 0.0, 0.096, 0.0, 0.0, 0.0, 0.008, 0.463, 0.0, 0.0, 0.224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022, 0.776, 0.0, 0.744, 0.0023, 0.0, 0.0, 0.0, 0.004, 0.194, 0.0, 0.0, 0.0001, 0.0, 0.0, 0.394, 0.0, 0.0, 0.0187, 0.708, 0.002, 0.004, 0.0, 0.011, 0.0, 0.0, 0.452, 0.0, 0.0, 0.1383, 0.0, 0.0, 0.284, 0.0, 0.252, 0.0, 0.0, 0.0, 0.0004, 0.0], 'micro': 0.337, 'macro': 0.1774, 'weighted': 0.3221}
2024-08-04 06:49:03 - [34m[1mLOGS   [0m - Best checkpoint with score 0.18 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:49:03 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:49:03 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:49:04 - [34m[1mLOGS   [0m - Training checkpoint for epoch 1/iteration 234 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_1_iter_234.pt
2024-08-04 06:49:04 - [34m[1mLOGS   [0m - Model state for epoch 1/iteration 234 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_1_iter_234.pt
[31m===========================================================================[0m
2024-08-04 06:49:06 - [32m[1mINFO   [0m - Training epoch 2
2024-08-04 06:49:07 - [34m[1mLOGS   [0m - Epoch:   2 [     235/10000000], loss: {'classification': 19.7828, 'neural_augmentation': 0.3068, 'total_loss': 20.0897}, LR: [2.4e-05, 2.4e-05], Avg. batch load time: 0.883, Elapsed time:  1.29
2024-08-04 06:49:36 - [34m[1mLOGS   [0m - *** Training summary for epoch 2
	 loss={'classification': 16.8826, 'neural_augmentation': 0.3147, 'total_loss': 17.1973}
2024-08-04 06:49:57 - [34m[1mLOGS   [0m - *** Validation summary for epoch 2
	 loss={'classification': 12.0265, 'neural_augmentation': 0.0, 'total_loss': 12.0265} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.5448, 0.5993, 0.527, 0.4222, 0.2731, 0.8529, 0.5865, 0.411, 0.6983, 0.6376, 0.2901, 0.716, 0.2927, 0.6931, 0.5422, 0.7418, 0.8278, 0.8512, 0.5366, 0.7305, 0.3712, 0.3942, 0.5785, 0.7634, 0.5663, 0.576, 0.7511, 0.8426, 0.6812, 0.4889, 0.2774, 0.2219, 0.4762, 0.6901, 0.5861, 0.1927, 0.7676, 0.7138, 0.6207, 0.3459, 0.9551, 0.4804, 0.5606, 0.9008, 0.6928, 0.8722, 0.5668, 0.3254, 0.4818, 0.6478, 0.1962, 0.5397, 0.7565, 0.6388, 0.7393, 0.7782, 0.8651, 0.422, 0.6433, 0.3333, 0.4863, 0.5906, 0.6377, 0.8145, 0.5094, 0.4821, 0.7459, 0.8122, 0.7873, 0.1765, 0.7346, 0.3879, 0.4932, 0.6641, 0.5727, 0.5357, 0.4396, 0.7371, 0.8165, 0.792, 0.6384, 0.7876, 0.6621, 0.4636, 0.8364, 0.763, 0.6405, 0.5148, 0.5333, 0.6975, 0.9001, 0.2422, 0.7101, 0.6273, 0.4988, 0.7041, 0.74, 0.0402, 0.6343, 0.4944, 0.5951, 0.5875, 0.7595, 0.4609, 0.8961, 0.6364, 0.8587, 0.2071, 0.7268, 0.6396, 0.4526, 0.8671, 0.7638, 0.5153, 0.2801, 0.7454, 0.3211, 0.8187, 0.552, 0.9027, 0.7664, 0.5276, 0.5846, 0.8884, 0.7176, 0.3563, 0.7851, 0.626, 0.4859, 0.365, 0.2741, 0.6971, 0.8043, 0.4618, 0.7496, 0.9049, 0.379, 0.9072, 0.5474, 0.498, 0.5703, 0.4615, 0.7736, 0.6631, 0.5728, 0.8515, 0.7361, 0.5395, 0.58, 0.7322, 0.4302, 0.342, 0.5555, 0.9072, 0.6915, 0.5256, 0.4815, 0.6555, 0.7176, 0.5596, 0.5806, 0.5103, 0.4213, 0.6529, 0.5091, 0.4683, 0.6477, 0.6983, 0.7949, 0.71, 0.1664, 0.6762, 0.7347, 0.3282], 'AP': [0.5907, 0.641, 0.5358, 0.3781, 0.1592, 0.9042, 0.6343, 0.3248, 0.7814, 0.6971, 0.2239, 0.7929, 0.1898, 0.7299, 0.5373, 0.7563, 0.8854, 0.9115, 0.5626, 0.7585, 0.3225, 0.3165, 0.5912, 0.8287, 0.6134, 0.6041, 0.8046, 0.8923, 0.7631, 0.5204, 0.1965, 0.1233, 0.4247, 0.7075, 0.6218, 0.1068, 0.7972, 0.7907, 0.6662, 0.294, 0.9767, 0.4745, 0.5483, 0.938, 0.7463, 0.8882, 0.6125, 0.2508, 0.5043, 0.69, 0.1164, 0.5746, 0.8004, 0.6918, 0.7819, 0.8251, 0.9221, 0.3672, 0.6647, 0.2599, 0.4876, 0.6058, 0.6479, 0.8517, 0.5445, 0.426, 0.7837, 0.8714, 0.8458, 0.0788, 0.7664, 0.3395, 0.4986, 0.7221, 0.5999, 0.5368, 0.3842, 0.7872, 0.8698, 0.8567, 0.7085, 0.8372, 0.7261, 0.4529, 0.8913, 0.8444, 0.7028, 0.5107, 0.5528, 0.7013, 0.9455, 0.132, 0.7531, 0.6694, 0.4977, 0.7704, 0.7888, 0.0155, 0.6819, 0.4508, 0.6469, 0.6417, 0.7983, 0.4554, 0.9339, 0.6971, 0.8952, 0.0973, 0.8045, 0.6521, 0.3961, 0.9045, 0.8368, 0.4983, 0.1819, 0.8016, 0.2475, 0.8636, 0.5407, 0.9415, 0.8223, 0.547, 0.5698, 0.9209, 0.7968, 0.3141, 0.8239, 0.6729, 0.4305, 0.3038, 0.1921, 0.7246, 0.8554, 0.392, 0.8138, 0.9486, 0.332, 0.9468, 0.5857, 0.4491, 0.6083, 0.4324, 0.8173, 0.7343, 0.6115, 0.8875, 0.8204, 0.5298, 0.5968, 0.7516, 0.3997, 0.2524, 0.5917, 0.9467, 0.7525, 0.5103, 0.469, 0.7095, 0.7569, 0.5881, 0.6317, 0.4726, 0.3656, 0.7292, 0.4759, 0.4187, 0.6749, 0.7611, 0.8501, 0.7593, 0.0878, 0.7278, 0.8125, 0.2401], 'Recall@P=50': [0.5907, 0.668, 0.544, 0.348, 0.012, 0.916, 0.6808, 0.008, 0.868, 0.7769, 0.176, 0.8548, 0.004, 0.844, 0.55, 0.844, 0.9304, 0.92, 0.564, 0.8, 0.272, 0.01, 0.608, 0.8744, 0.608, 0.6088, 0.832, 0.912, 0.8593, 0.4728, 0.064, 0.0, 0.404, 0.724, 0.668, 0.004, 0.86, 0.9218, 0.676, 0.208, 0.988, 0.452, 0.628, 0.948, 0.824, 0.892, 0.592, 0.004, 0.4547, 0.732, 0.004, 0.5737, 0.83, 0.7337, 0.828, 0.8413, 0.94, 0.356, 0.692, 0.176, 0.444, 0.624, 0.708, 0.888, 0.5127, 0.002, 0.84, 0.9053, 0.884, 0.0, 0.856, 0.29, 0.468, 0.756, 0.604, 0.568, 0.288, 0.852, 0.8907, 0.896, 0.7695, 0.86, 0.8256, 0.416, 0.946, 0.898, 0.8067, 0.504, 0.5226, 0.736, 0.97, 0.0, 0.768, 0.6712, 0.492, 0.8404, 0.828, 0.0, 0.686, 0.292, 0.6851, 0.634, 0.828, 0.36, 0.944, 0.74, 0.908, 0.0, 0.856, 0.66, 0.36, 0.92, 0.9523, 0.508, 0.024, 0.824, 0.16, 0.8767, 0.588, 0.972, 0.892, 0.5493, 0.596, 0.928, 0.913, 0.24, 0.852, 0.692, 0.448, 0.174, 0.036, 0.752, 0.868, 0.384, 0.866, 0.968, 0.248, 0.968, 0.5993, 0.432, 0.616, 0.412, 0.872, 0.768, 0.6632, 0.904, 0.9578, 0.5813, 0.584, 0.764, 0.324, 0.002, 0.5933, 0.972, 0.808, 0.528, 0.44, 0.726, 0.82, 0.604, 0.685, 0.508, 0.296, 0.779, 0.004, 0.428, 0.736, 0.786, 0.868, 0.816, 0.0, 0.776, 0.9525, 0.016], 'micro': 0.6953, 'macro': 0.6138, 'weighted': 0.6951}
2024-08-04 06:50:08 - [34m[1mLOGS   [0m - Best checkpoint with score 0.61 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:50:12 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:50:12 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:50:13 - [34m[1mLOGS   [0m - Training checkpoint for epoch 2/iteration 363 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_2_iter_363.pt
2024-08-04 06:50:13 - [34m[1mLOGS   [0m - Model state for epoch 2/iteration 363 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_2_iter_363.pt
[31m===========================================================================[0m
2024-08-04 06:50:15 - [32m[1mINFO   [0m - Training epoch 3
2024-08-04 06:50:18 - [34m[1mLOGS   [0m - Epoch:   3 [     364/10000000], loss: {'classification': 13.3735, 'neural_augmentation': 0.2803, 'total_loss': 13.6539}, LR: [3.7e-05, 3.7e-05], Avg. batch load time: 2.521, Elapsed time:  2.68
2024-08-04 06:50:37 - [34m[1mLOGS   [0m - *** Training summary for epoch 3
	 loss={'classification': 11.6902, 'neural_augmentation': 0.3093, 'total_loss': 11.9995}
2024-08-04 06:51:00 - [34m[1mLOGS   [0m - *** Validation summary for epoch 3
	 loss={'classification': 7.4106, 'neural_augmentation': 0.0, 'total_loss': 7.4106} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.8292, 0.7651, 0.7609, 0.8731, 0.7354, 0.9317, 0.8118, 0.7364, 0.8449, 0.7567, 0.6392, 0.8257, 0.6147, 0.8599, 0.7798, 0.8268, 0.9284, 0.9417, 0.8311, 0.8703, 0.7982, 0.632, 0.8309, 0.8716, 0.7463, 0.8363, 0.8537, 0.9189, 0.8092, 0.7757, 0.646, 0.6116, 0.7121, 0.8684, 0.7604, 0.75, 0.8889, 0.8446, 0.8388, 0.79, 0.9879, 0.7212, 0.8549, 0.9376, 0.8629, 0.9331, 0.7099, 0.7413, 0.6407, 0.8529, 0.6368, 0.7646, 0.9087, 0.7959, 0.8953, 0.8674, 0.9351, 0.7661, 0.8113, 0.6712, 0.6941, 0.8635, 0.8303, 0.8963, 0.7456, 0.6985, 0.9256, 0.9157, 0.8866, 0.3492, 0.863, 0.6277, 0.8559, 0.832, 0.786, 0.7799, 0.7412, 0.8761, 0.8799, 0.9212, 0.7729, 0.8958, 0.8085, 0.6652, 0.9197, 0.919, 0.8154, 0.7947, 0.7083, 0.8638, 0.9396, 0.364, 0.8535, 0.7753, 0.806, 0.8215, 0.9202, 0.3981, 0.7795, 0.7758, 0.8, 0.8354, 0.8694, 0.7236, 0.932, 0.8192, 0.9262, 0.7298, 0.8641, 0.8509, 0.7545, 0.9231, 0.8634, 0.7705, 0.4745, 0.8535, 0.6957, 0.9038, 0.7598, 0.9417, 0.8812, 0.6988, 0.88, 0.9314, 0.8348, 0.7084, 0.8992, 0.8644, 0.7752, 0.6995, 0.4373, 0.8812, 0.9072, 0.7976, 0.8706, 0.9388, 0.7004, 0.9426, 0.7754, 0.7301, 0.7857, 0.6947, 0.8884, 0.7967, 0.8252, 0.9303, 0.8371, 0.7743, 0.8659, 0.8093, 0.6489, 0.6341, 0.7602, 0.9393, 0.888, 0.7665, 0.7706, 0.8162, 0.8436, 0.7846, 0.7832, 0.7836, 0.6582, 0.8134, 0.7773, 0.7441, 0.784, 0.8764, 0.8911, 0.8664, 0.697, 0.872, 0.8371, 0.7302], 'AP': [0.8993, 0.8208, 0.8352, 0.9209, 0.7492, 0.9451, 0.8745, 0.7856, 0.9162, 0.836, 0.6767, 0.9056, 0.6341, 0.9218, 0.8352, 0.891, 0.9634, 0.964, 0.8534, 0.9107, 0.8214, 0.6769, 0.8929, 0.9339, 0.8162, 0.8954, 0.9046, 0.9488, 0.8943, 0.8351, 0.6751, 0.5817, 0.7529, 0.9076, 0.8339, 0.789, 0.9122, 0.9188, 0.8965, 0.8497, 0.9873, 0.7401, 0.907, 0.9637, 0.9006, 0.9485, 0.7828, 0.7874, 0.6791, 0.8912, 0.6644, 0.8373, 0.9517, 0.8587, 0.9352, 0.9277, 0.9712, 0.8363, 0.8547, 0.7059, 0.7482, 0.902, 0.8748, 0.9334, 0.8217, 0.7458, 0.9404, 0.9577, 0.9295, 0.3047, 0.8902, 0.6605, 0.9008, 0.8809, 0.8303, 0.8352, 0.7837, 0.9322, 0.9232, 0.9465, 0.8506, 0.9331, 0.8937, 0.7035, 0.9647, 0.9538, 0.8869, 0.8631, 0.7788, 0.9149, 0.9703, 0.3162, 0.8989, 0.8475, 0.8163, 0.8929, 0.9553, 0.3455, 0.8555, 0.83, 0.8687, 0.8778, 0.8844, 0.7952, 0.9545, 0.894, 0.9483, 0.7757, 0.9257, 0.8997, 0.7966, 0.9478, 0.9325, 0.8399, 0.4176, 0.9181, 0.73, 0.9507, 0.8151, 0.9584, 0.9371, 0.7713, 0.9108, 0.9571, 0.9125, 0.7446, 0.9491, 0.912, 0.8452, 0.7471, 0.3789, 0.9202, 0.9388, 0.8415, 0.9197, 0.9624, 0.7376, 0.9654, 0.8514, 0.7854, 0.8405, 0.7322, 0.9296, 0.876, 0.897, 0.9562, 0.9208, 0.8387, 0.9237, 0.867, 0.6763, 0.681, 0.8225, 0.9643, 0.9434, 0.7835, 0.8059, 0.8721, 0.8869, 0.8454, 0.8586, 0.8046, 0.6993, 0.8982, 0.8329, 0.7708, 0.8407, 0.9297, 0.9386, 0.9199, 0.7171, 0.9153, 0.917, 0.7712], 'Recall@P=50': [0.9393, 0.852, 0.8891, 0.948, 0.74, 0.948, 0.9288, 0.82, 0.9696, 0.9148, 0.718, 0.9536, 0.628, 0.958, 0.874, 0.948, 0.9712, 0.96, 0.86, 0.928, 0.828, 0.738, 0.936, 0.9616, 0.8733, 0.9264, 0.932, 0.968, 0.9685, 0.8776, 0.72, 0.64, 0.808, 0.924, 0.8815, 0.844, 0.924, 0.9881, 0.932, 0.912, 0.988, 0.764, 0.936, 0.976, 0.928, 0.944, 0.8013, 0.792, 0.7293, 0.908, 0.712, 0.8821, 0.97, 0.9069, 0.952, 0.956, 0.98, 0.908, 0.884, 0.744, 0.796, 0.928, 0.9, 0.948, 0.876, 0.784, 0.952, 0.9747, 0.948, 0.164, 0.948, 0.7, 0.908, 0.9136, 0.856, 0.89, 0.816, 0.9573, 0.948, 0.96, 0.9137, 0.94, 0.9678, 0.748, 0.978, 0.968, 0.9625, 0.912, 0.8061, 0.944, 0.98, 0.212, 0.924, 0.8784, 0.828, 0.9502, 0.968, 0.32, 0.9027, 0.888, 0.9257, 0.892, 0.876, 0.8773, 0.956, 0.935, 0.958, 0.804, 0.9576, 0.924, 0.844, 0.956, 0.986, 0.882, 0.324, 0.9573, 0.76, 0.9727, 0.896, 0.972, 0.964, 0.832, 0.92, 0.964, 0.9813, 0.76, 0.984, 0.94, 0.928, 0.808, 0.3, 0.924, 0.944, 0.924, 0.9407, 0.972, 0.776, 0.972, 0.9107, 0.808, 0.924, 0.784, 0.94, 0.926, 0.9376, 0.956, 0.9902, 0.9147, 0.94, 0.898, 0.732, 0.728, 0.8667, 0.968, 0.968, 0.812, 0.852, 0.907, 0.912, 0.892, 0.928, 0.828, 0.74, 0.9535, 0.904, 0.832, 0.876, 0.958, 0.96, 0.948, 0.748, 0.932, 0.9892, 0.804], 'micro': 0.8793, 'macro': 0.8461, 'weighted': 0.8763}
2024-08-04 06:51:06 - [34m[1mLOGS   [0m - Best checkpoint with score 0.85 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:51:08 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:51:08 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:51:08 - [34m[1mLOGS   [0m - Training checkpoint for epoch 3/iteration 471 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_3_iter_471.pt
2024-08-04 06:51:09 - [34m[1mLOGS   [0m - Model state for epoch 3/iteration 471 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_3_iter_471.pt
[31m===========================================================================[0m
2024-08-04 06:51:11 - [32m[1mINFO   [0m - Training epoch 4
2024-08-04 06:51:11 - [34m[1mLOGS   [0m - Epoch:   4 [     472/10000000], loss: {'classification': 9.1812, 'neural_augmentation': 0.3055, 'total_loss': 9.4867}, LR: [4.7e-05, 4.7e-05], Avg. batch load time: 0.597, Elapsed time:  0.76
2024-08-04 06:51:44 - [34m[1mLOGS   [0m - *** Training summary for epoch 4
	 loss={'classification': 8.9436, 'neural_augmentation': 0.3043, 'total_loss': 9.2478}
2024-08-04 06:52:13 - [34m[1mLOGS   [0m - *** Validation summary for epoch 4
	 loss={'classification': 5.3134, 'neural_augmentation': 0.0, 'total_loss': 5.3134} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.8926, 0.8139, 0.8404, 0.9095, 0.8412, 0.9522, 0.8455, 0.8301, 0.9091, 0.8212, 0.7771, 0.8902, 0.7335, 0.8988, 0.8649, 0.8826, 0.9434, 0.9576, 0.8816, 0.9045, 0.8743, 0.7487, 0.8848, 0.9253, 0.8604, 0.9007, 0.9102, 0.9452, 0.8751, 0.8614, 0.8017, 0.7093, 0.8347, 0.9114, 0.8419, 0.7992, 0.9287, 0.8937, 0.8791, 0.8506, 0.9879, 0.7849, 0.9091, 0.9522, 0.9151, 0.937, 0.7399, 0.8921, 0.7222, 0.9184, 0.8061, 0.8384, 0.9342, 0.8823, 0.9136, 0.9069, 0.9646, 0.8661, 0.8608, 0.7632, 0.8155, 0.9062, 0.8889, 0.9281, 0.8366, 0.7676, 0.9407, 0.9409, 0.9299, 0.6273, 0.8898, 0.8098, 0.9113, 0.879, 0.8117, 0.856, 0.812, 0.9293, 0.9213, 0.938, 0.8453, 0.944, 0.8718, 0.834, 0.9291, 0.9389, 0.8823, 0.8617, 0.7948, 0.9148, 0.9525, 0.4954, 0.9146, 0.8217, 0.8728, 0.8748, 0.9524, 0.6911, 0.8457, 0.8462, 0.8623, 0.8792, 0.9046, 0.8084, 0.9478, 0.8848, 0.9585, 0.8148, 0.9058, 0.9184, 0.8392, 0.9374, 0.9046, 0.8808, 0.538, 0.9057, 0.793, 0.9267, 0.8515, 0.9474, 0.9028, 0.8061, 0.9289, 0.9419, 0.8854, 0.7925, 0.9303, 0.9243, 0.8807, 0.8281, 0.517, 0.9167, 0.9482, 0.9155, 0.9057, 0.9516, 0.7956, 0.9443, 0.8604, 0.8337, 0.8723, 0.7773, 0.9095, 0.8848, 0.9125, 0.9431, 0.8922, 0.8733, 0.9189, 0.8952, 0.762, 0.7691, 0.85, 0.9491, 0.9273, 0.8069, 0.8401, 0.8734, 0.9016, 0.8419, 0.8544, 0.8645, 0.7947, 0.8788, 0.8834, 0.818, 0.864, 0.901, 0.9135, 0.9047, 0.8423, 0.9167, 0.883, 0.8102], 'AP': [0.9501, 0.8793, 0.907, 0.9509, 0.8817, 0.9739, 0.9062, 0.8829, 0.9575, 0.9017, 0.8418, 0.9511, 0.7647, 0.9517, 0.9119, 0.9447, 0.9752, 0.9766, 0.8917, 0.9554, 0.9218, 0.8215, 0.9355, 0.9641, 0.9127, 0.9469, 0.9469, 0.9706, 0.9462, 0.9207, 0.8439, 0.766, 0.8891, 0.949, 0.9072, 0.858, 0.9427, 0.9578, 0.9385, 0.899, 0.9921, 0.8152, 0.9427, 0.9671, 0.9461, 0.9342, 0.8363, 0.9235, 0.7842, 0.9453, 0.8388, 0.8999, 0.9758, 0.9372, 0.9558, 0.9577, 0.9832, 0.9344, 0.9239, 0.7984, 0.8584, 0.9533, 0.912, 0.9663, 0.9012, 0.8445, 0.9685, 0.9752, 0.9518, 0.6247, 0.9134, 0.8457, 0.9509, 0.9197, 0.8789, 0.9019, 0.8664, 0.9645, 0.9593, 0.968, 0.9143, 0.9757, 0.9426, 0.8612, 0.9679, 0.9721, 0.9487, 0.9233, 0.8553, 0.9491, 0.9659, 0.4801, 0.9349, 0.8947, 0.8931, 0.9403, 0.9677, 0.7181, 0.9077, 0.9182, 0.9276, 0.9213, 0.9461, 0.8836, 0.9663, 0.9422, 0.97, 0.859, 0.9519, 0.95, 0.885, 0.9763, 0.9616, 0.9345, 0.5203, 0.9536, 0.8386, 0.9656, 0.9127, 0.9689, 0.9558, 0.8764, 0.9567, 0.9491, 0.9477, 0.8075, 0.9649, 0.9567, 0.9222, 0.8719, 0.469, 0.949, 0.9725, 0.9524, 0.9534, 0.9723, 0.8463, 0.9679, 0.9269, 0.8647, 0.9346, 0.8319, 0.9674, 0.9389, 0.9587, 0.9781, 0.9588, 0.9276, 0.9629, 0.932, 0.8302, 0.8185, 0.9105, 0.9692, 0.9691, 0.8425, 0.8677, 0.9286, 0.9284, 0.9047, 0.9201, 0.9135, 0.8296, 0.9481, 0.9237, 0.845, 0.9187, 0.95, 0.9621, 0.9519, 0.8692, 0.9405, 0.9498, 0.8593], 'Recall@P=50': [0.9767, 0.932, 0.952, 0.964, 0.892, 0.98, 0.956, 0.92, 0.9856, 0.952, 0.882, 0.9796, 0.752, 0.978, 0.934, 0.968, 0.9848, 0.98, 0.892, 0.98, 0.936, 0.862, 0.952, 0.9768, 0.9453, 0.9728, 0.968, 0.988, 0.9873, 0.9544, 0.876, 0.832, 0.936, 0.96, 0.9425, 0.892, 0.952, 0.992, 0.97, 0.96, 0.992, 0.832, 0.964, 0.968, 0.96, 0.94, 0.8973, 0.932, 0.844, 0.948, 0.868, 0.93, 0.988, 0.9691, 0.976, 0.9767, 0.988, 0.98, 0.96, 0.816, 0.888, 0.992, 0.932, 0.984, 0.9433, 0.886, 0.98, 0.9867, 0.96, 0.664, 0.952, 0.89, 0.96, 0.956, 0.924, 0.002, 0.904, 0.9773, 0.972, 0.972, 0.9632, 0.984, 0.9866, 0.888, 0.982, 0.98, 0.9856, 0.956, 0.8739, 0.964, 0.964, 0.44, 0.94, 0.9288, 0.9, 0.9749, 0.968, 0.812, 0.94, 0.94, 0.9589, 0.942, 0.968, 0.9467, 0.964, 0.969, 0.976, 0.904, 0.9752, 0.956, 0.932, 0.988, 0.9929, 0.97, 0.58, 0.9767, 0.884, 0.982, 0.948, 0.972, 0.978, 0.94, 0.964, 0.948, 0.9907, 0.816, 0.972, 0.9667, 0.956, 0.906, 0.004, 0.968, 0.976, 0.96, 0.968, 0.976, 0.8787, 0.976, 0.9677, 0.896, 0.956, 0.868, 0.992, 0.965, 0.9744, 0.984, 0.9963, 0.9547, 0.972, 0.95, 0.896, 0.864, 0.9507, 0.972, 0.988, 0.872, 0.92, 0.954, 0.952, 0.96, 0.961, 0.936, 0.86, 0.9805, 0.956, 0.888, 0.9507, 0.962, 0.984, 0.976, 0.88, 0.952, 0.9965, 0.892], 'micro': 0.9304, 'macro': 0.9104, 'weighted': 0.9304}
2024-08-04 06:52:19 - [34m[1mLOGS   [0m - Best checkpoint with score 0.91 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:52:20 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:52:21 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:52:21 - [34m[1mLOGS   [0m - Training checkpoint for epoch 4/iteration 575 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_4_iter_575.pt
2024-08-04 06:52:21 - [34m[1mLOGS   [0m - Model state for epoch 4/iteration 575 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_4_iter_575.pt
[31m===========================================================================[0m
2024-08-04 06:52:23 - [32m[1mINFO   [0m - Training epoch 5
2024-08-04 06:52:27 - [34m[1mLOGS   [0m - Epoch:   5 [     576/10000000], loss: {'classification': 7.9906, 'neural_augmentation': 0.3006, 'total_loss': 8.2911}, LR: [4.7e-05, 4.7e-05], Avg. batch load time: 3.644, Elapsed time:  3.80
2024-08-04 06:52:47 - [34m[1mLOGS   [0m - *** Training summary for epoch 5
	 loss={'classification': 7.2148, 'neural_augmentation': 0.2966, 'total_loss': 7.5114}
2024-08-04 06:53:13 - [34m[1mLOGS   [0m - *** Validation summary for epoch 5
	 loss={'classification': 4.1495, 'neural_augmentation': 0.0, 'total_loss': 4.1495} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.9185, 0.8337, 0.8746, 0.9412, 0.8935, 0.9613, 0.8912, 0.8712, 0.929, 0.8598, 0.8397, 0.9152, 0.7935, 0.9355, 0.896, 0.8987, 0.9645, 0.9738, 0.905, 0.939, 0.9121, 0.7966, 0.9095, 0.943, 0.8973, 0.9154, 0.9455, 0.9555, 0.9016, 0.8964, 0.8443, 0.779, 0.8768, 0.9487, 0.871, 0.8523, 0.9508, 0.919, 0.8971, 0.8792, 0.992, 0.8195, 0.9317, 0.9547, 0.9059, 0.9545, 0.7847, 0.9165, 0.7839, 0.8953, 0.8596, 0.8701, 0.9541, 0.9125, 0.9426, 0.9289, 0.9787, 0.9041, 0.9121, 0.8358, 0.8193, 0.9212, 0.9284, 0.9426, 0.879, 0.8384, 0.953, 0.9432, 0.927, 0.7607, 0.9076, 0.8454, 0.9333, 0.906, 0.8358, 0.8827, 0.8548, 0.929, 0.9421, 0.9544, 0.8732, 0.9576, 0.8972, 0.8692, 0.9572, 0.9517, 0.9097, 0.895, 0.8394, 0.9301, 0.9677, 0.608, 0.94, 0.8647, 0.9193, 0.9022, 0.9756, 0.7692, 0.8721, 0.8925, 0.8955, 0.9003, 0.9083, 0.8482, 0.9526, 0.908, 0.9593, 0.8554, 0.9284, 0.9446, 0.8701, 0.9549, 0.9264, 0.9141, 0.6352, 0.9139, 0.8264, 0.945, 0.8921, 0.9533, 0.9374, 0.8735, 0.9456, 0.9485, 0.9034, 0.8664, 0.9449, 0.9411, 0.9024, 0.8598, 0.5905, 0.9469, 0.9535, 0.9315, 0.9156, 0.9555, 0.8248, 0.9558, 0.896, 0.8714, 0.9047, 0.8294, 0.9331, 0.9083, 0.9274, 0.9526, 0.9204, 0.8965, 0.9465, 0.9072, 0.8444, 0.8395, 0.885, 0.953, 0.9452, 0.8529, 0.8664, 0.8988, 0.9014, 0.8797, 0.8675, 0.8975, 0.8306, 0.91, 0.9289, 0.8373, 0.9096, 0.9473, 0.9328, 0.9195, 0.9002, 0.9322, 0.9143, 0.8293], 'AP': [0.9666, 0.902, 0.9347, 0.9751, 0.9353, 0.9773, 0.9389, 0.9042, 0.9738, 0.9344, 0.8954, 0.9664, 0.8252, 0.971, 0.9433, 0.9608, 0.9846, 0.9827, 0.9279, 0.9702, 0.942, 0.8656, 0.955, 0.9777, 0.9486, 0.9598, 0.9623, 0.977, 0.9636, 0.9447, 0.8935, 0.8556, 0.9225, 0.9625, 0.9365, 0.908, 0.9698, 0.9747, 0.9499, 0.9132, 0.9952, 0.8569, 0.9607, 0.9728, 0.9492, 0.9683, 0.8714, 0.9475, 0.8431, 0.9491, 0.9189, 0.9305, 0.9851, 0.9568, 0.978, 0.9733, 0.9864, 0.9581, 0.9531, 0.8826, 0.8595, 0.9603, 0.9536, 0.9695, 0.9442, 0.8979, 0.97, 0.9756, 0.9587, 0.8039, 0.9275, 0.9064, 0.9628, 0.9498, 0.8963, 0.9275, 0.8914, 0.9683, 0.9708, 0.965, 0.941, 0.9799, 0.9616, 0.9167, 0.9843, 0.982, 0.9655, 0.9488, 0.9018, 0.9682, 0.9875, 0.6519, 0.9659, 0.9317, 0.9325, 0.9572, 0.9887, 0.8061, 0.9331, 0.9573, 0.9475, 0.9204, 0.9309, 0.9199, 0.9662, 0.9596, 0.9736, 0.8966, 0.9665, 0.9633, 0.9046, 0.9575, 0.9756, 0.9549, 0.6445, 0.9617, 0.8933, 0.9745, 0.959, 0.9668, 0.9707, 0.9335, 0.9672, 0.9713, 0.9626, 0.9063, 0.9754, 0.9656, 0.9429, 0.9049, 0.567, 0.9696, 0.9797, 0.9513, 0.9601, 0.9732, 0.8863, 0.9742, 0.9549, 0.9153, 0.9586, 0.8818, 0.9663, 0.9519, 0.9693, 0.9651, 0.9755, 0.9465, 0.9794, 0.9426, 0.9006, 0.8867, 0.9343, 0.9686, 0.9715, 0.88, 0.9001, 0.9478, 0.9383, 0.9195, 0.9389, 0.9476, 0.8816, 0.966, 0.9562, 0.8772, 0.9562, 0.9789, 0.966, 0.9652, 0.9311, 0.9558, 0.9708, 0.8976], 'Recall@P=50': [0.9827, 0.94, 0.964, 0.992, 0.96, 0.984, 0.9672, 0.936, 0.9928, 0.9742, 0.934, 0.9896, 0.84, 0.984, 0.97, 0.984, 0.988, 0.98, 0.936, 0.988, 0.952, 0.898, 0.976, 0.9896, 0.9693, 0.9768, 0.968, 0.984, 0.992, 0.9656, 0.928, 0.924, 0.944, 0.964, 0.966, 0.94, 0.98, 0.9968, 0.973, 0.948, 0.996, 0.892, 0.988, 0.972, 0.972, 0.972, 0.9253, 0.956, 0.8973, 0.984, 0.94, 0.9595, 0.994, 0.9794, 0.984, 0.9853, 0.984, 0.98, 0.964, 0.916, 0.908, 0.972, 0.968, 0.98, 0.9713, 0.944, 0.968, 0.9853, 0.97, 0.844, 0.976, 0.942, 0.972, 0.9704, 0.924, 0.958, 0.932, 0.984, 0.9773, 0.964, 0.9742, 0.984, 0.992, 0.94, 0.992, 0.994, 0.9924, 0.96, 0.9296, 0.976, 0.99, 0.696, 0.972, 0.9632, 0.928, 0.9851, 0.992, 0.864, 0.9633, 0.988, 0.9709, 0.934, 0.944, 0.956, 0.968, 0.9793, 0.978, 0.94, 0.98, 0.96, 0.94, 0.956, 0.9965, 0.972, 0.72, 0.9793, 0.952, 0.9867, 0.98, 0.968, 0.986, 0.9687, 0.972, 0.972, 0.9937, 0.912, 0.992, 0.9773, 0.972, 0.944, 0.692, 0.98, 0.988, 0.956, 0.972, 0.976, 0.9267, 0.976, 0.9817, 0.956, 0.976, 0.912, 0.98, 0.966, 0.984, 0.968, 0.9981, 0.9747, 0.984, 0.956, 0.94, 0.934, 0.9533, 0.968, 0.984, 0.912, 0.936, 0.962, 0.972, 0.948, 0.976, 0.956, 0.912, 0.9902, 0.976, 0.904, 0.9753, 0.986, 0.988, 0.984, 0.948, 0.964, 0.9976, 0.944], 'micro': 0.9529, 'macro': 0.9371, 'weighted': 0.9528}
2024-08-04 06:53:21 - [34m[1mLOGS   [0m - Best checkpoint with score 0.94 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:53:21 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_score_0.0437.pt
2024-08-04 06:53:21 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.1774.pt', 'checkpoint_score_0.6138.pt', 'checkpoint_score_0.8461.pt', 'checkpoint_score_0.9104.pt', 'checkpoint_score_0.9371.pt']
2024-08-04 06:53:23 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_avg.pt
2024-08-04 06:53:24 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:53:24 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:53:25 - [34m[1mLOGS   [0m - Training checkpoint for epoch 5/iteration 695 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_5_iter_695.pt
2024-08-04 06:53:25 - [34m[1mLOGS   [0m - Model state for epoch 5/iteration 695 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_5_iter_695.pt
[31m===========================================================================[0m
2024-08-04 06:53:27 - [32m[1mINFO   [0m - Training epoch 6
2024-08-04 06:53:33 - [34m[1mLOGS   [0m - Epoch:   6 [     696/10000000], loss: {'classification': 6.6238, 'neural_augmentation': 0.2849, 'total_loss': 6.9087}, LR: [4.6e-05, 4.6e-05], Avg. batch load time: 5.668, Elapsed time:  5.83
2024-08-04 06:53:50 - [34m[1mLOGS   [0m - *** Training summary for epoch 6
	 loss={'classification': 6.1413, 'neural_augmentation': 0.291, 'total_loss': 6.4323}
2024-08-04 06:54:27 - [34m[1mLOGS   [0m - *** Validation summary for epoch 6
	 loss={'classification': 3.355, 'neural_augmentation': 0.0, 'total_loss': 3.355} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.9342, 0.8578, 0.9088, 0.9528, 0.9186, 0.9673, 0.9088, 0.8747, 0.9365, 0.8845, 0.8676, 0.9226, 0.8035, 0.9391, 0.9064, 0.9258, 0.9689, 0.9837, 0.9352, 0.943, 0.9215, 0.842, 0.9138, 0.9467, 0.915, 0.9298, 0.9393, 0.9579, 0.9239, 0.9135, 0.8512, 0.8, 0.9084, 0.9588, 0.8963, 0.8836, 0.9574, 0.9355, 0.9152, 0.9095, 0.996, 0.8645, 0.9495, 0.9673, 0.9363, 0.9652, 0.8238, 0.9395, 0.817, 0.9303, 0.8769, 0.9034, 0.9655, 0.9273, 0.9483, 0.9411, 0.984, 0.9259, 0.9131, 0.8701, 0.8742, 0.9328, 0.9371, 0.9459, 0.9133, 0.8608, 0.9679, 0.9551, 0.9464, 0.8101, 0.9035, 0.8918, 0.9446, 0.9206, 0.8595, 0.8947, 0.8663, 0.9393, 0.946, 0.9651, 0.9009, 0.9662, 0.9162, 0.896, 0.9628, 0.9634, 0.9292, 0.915, 0.8601, 0.9571, 0.971, 0.6792, 0.9446, 0.8856, 0.9409, 0.9249, 0.9756, 0.8126, 0.9009, 0.9095, 0.9187, 0.9127, 0.9376, 0.8718, 0.9654, 0.9267, 0.9711, 0.8679, 0.9466, 0.9631, 0.8732, 0.9633, 0.9398, 0.9307, 0.672, 0.9347, 0.8571, 0.9533, 0.9091, 0.9673, 0.9396, 0.9028, 0.952, 0.9608, 0.9191, 0.8819, 0.9516, 0.9508, 0.9143, 0.8816, 0.641, 0.9579, 0.9633, 0.9421, 0.9303, 0.9693, 0.8604, 0.9698, 0.9142, 0.8867, 0.9185, 0.8832, 0.9391, 0.9296, 0.9415, 0.9651, 0.9311, 0.9155, 0.9579, 0.9283, 0.855, 0.8718, 0.9097, 0.9692, 0.9657, 0.8675, 0.8813, 0.9191, 0.9312, 0.9038, 0.8923, 0.9039, 0.8474, 0.9267, 0.9246, 0.8701, 0.9321, 0.9512, 0.936, 0.9231, 0.9102, 0.9303, 0.9303, 0.8611], 'AP': [0.9735, 0.9135, 0.9588, 0.9852, 0.9503, 0.9878, 0.9586, 0.9242, 0.9792, 0.9515, 0.9271, 0.973, 0.8446, 0.9761, 0.9474, 0.9663, 0.9848, 0.9876, 0.9528, 0.9716, 0.9674, 0.9081, 0.9672, 0.98, 0.9592, 0.9647, 0.959, 0.9824, 0.9762, 0.9596, 0.9013, 0.8845, 0.9404, 0.9734, 0.9493, 0.9323, 0.9732, 0.9815, 0.9582, 0.9393, 0.9976, 0.9002, 0.9667, 0.9825, 0.9654, 0.9763, 0.9029, 0.965, 0.8766, 0.9601, 0.9246, 0.9552, 0.9918, 0.9698, 0.976, 0.9798, 0.9916, 0.9705, 0.957, 0.8959, 0.925, 0.9594, 0.9613, 0.9651, 0.962, 0.9232, 0.9835, 0.9839, 0.9639, 0.8623, 0.9311, 0.9416, 0.9781, 0.9597, 0.9104, 0.9434, 0.919, 0.9772, 0.9806, 0.9866, 0.9615, 0.9865, 0.9716, 0.9232, 0.9885, 0.9825, 0.9778, 0.9661, 0.9314, 0.9847, 0.9902, 0.7504, 0.9708, 0.9421, 0.9552, 0.9721, 0.9851, 0.8553, 0.9522, 0.9699, 0.9658, 0.9492, 0.9681, 0.9444, 0.9732, 0.9697, 0.9838, 0.9232, 0.9757, 0.9768, 0.9167, 0.9848, 0.9833, 0.9718, 0.6874, 0.9713, 0.9137, 0.9843, 0.9674, 0.9741, 0.9735, 0.9552, 0.9786, 0.9856, 0.9704, 0.9176, 0.9742, 0.9716, 0.9562, 0.9237, 0.6142, 0.9719, 0.9886, 0.9536, 0.9709, 0.9807, 0.9205, 0.983, 0.9662, 0.9368, 0.9686, 0.9194, 0.9672, 0.9659, 0.9777, 0.9859, 0.9805, 0.9577, 0.9887, 0.9579, 0.919, 0.9189, 0.9613, 0.9754, 0.9802, 0.91, 0.9146, 0.9656, 0.9495, 0.9411, 0.9561, 0.9522, 0.8935, 0.9747, 0.9643, 0.907, 0.969, 0.9811, 0.9579, 0.978, 0.9429, 0.9683, 0.9795, 0.9098], 'Recall@P=50': [0.9873, 0.948, 0.9794, 0.992, 0.968, 0.996, 0.9808, 0.952, 0.9968, 0.9849, 0.964, 0.9892, 0.86, 0.99, 0.968, 0.984, 0.9904, 0.988, 0.956, 0.98, 0.98, 0.944, 0.988, 0.9904, 0.976, 0.9824, 0.964, 0.988, 0.995, 0.9768, 0.928, 0.94, 0.956, 0.976, 0.9705, 0.964, 0.976, 0.9968, 0.977, 0.968, 1.0, 0.936, 0.004, 0.984, 0.98, 0.984, 0.9493, 0.972, 0.928, 0.984, 0.94, 0.9742, 0.996, 0.9834, 0.98, 0.9887, 0.99, 0.988, 0.968, 0.004, 0.956, 0.972, 0.98, 0.972, 0.9807, 0.964, 0.98, 0.992, 0.972, 0.908, 0.968, 0.968, 0.988, 0.9792, 0.944, 0.976, 0.96, 0.9907, 0.9893, 0.992, 0.9826, 0.992, 0.9954, 0.944, 0.998, 0.994, 0.9951, 0.984, 0.9713, 0.996, 0.994, 0.804, 0.976, 0.9688, 0.956, 0.9924, 0.988, 0.884, 0.9693, 0.996, 0.9829, 0.966, 0.98, 0.9787, 0.976, 0.984, 0.992, 0.952, 0.9856, 0.984, 0.952, 1.0, 0.9966, 0.988, 0.812, 0.9873, 0.948, 0.992, 0.992, 0.976, 0.992, 0.9813, 0.984, 0.992, 0.997, 0.936, 0.988, 0.98, 0.98, 0.958, 0.772, 0.976, 0.996, 0.96, 0.984, 0.98, 0.9573, 0.984, 0.9877, 0.98, 0.984, 0.944, 0.996, 0.973, 0.9872, 0.992, 0.9986, 0.9813, 0.992, 0.966, 0.96, 0.944, 0.984, 0.976, 0.994, 0.94, 0.944, 0.983, 0.976, 0.972, 0.987, 0.968, 0.92, 0.9912, 0.984, 0.944, 0.982, 0.99, 0.004, 1.0, 0.944, 0.98, 0.9987, 0.948], 'micro': 0.9662, 'macro': 0.9519, 'weighted': 0.9651}
2024-08-04 06:54:40 - [34m[1mLOGS   [0m - Best checkpoint with score 0.95 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:54:40 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_score_0.1774.pt
2024-08-04 06:54:40 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.6138.pt', 'checkpoint_score_0.8461.pt', 'checkpoint_score_0.9104.pt', 'checkpoint_score_0.9371.pt', 'checkpoint_score_0.9519.pt']
2024-08-04 06:54:45 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_avg.pt
2024-08-04 06:54:46 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:54:46 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:54:47 - [34m[1mLOGS   [0m - Training checkpoint for epoch 6/iteration 811 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_6_iter_811.pt
2024-08-04 06:54:47 - [34m[1mLOGS   [0m - Model state for epoch 6/iteration 811 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_6_iter_811.pt
[31m===========================================================================[0m
2024-08-04 06:54:49 - [32m[1mINFO   [0m - Training epoch 7
2024-08-04 06:54:56 - [34m[1mLOGS   [0m - Epoch:   7 [     812/10000000], loss: {'classification': 6.6823, 'neural_augmentation': 0.2798, 'total_loss': 6.9621}, LR: [4.4e-05, 4.4e-05], Avg. batch load time: 6.767, Elapsed time:  6.93
2024-08-04 06:55:14 - [34m[1mLOGS   [0m - *** Training summary for epoch 7
	 loss={'classification': 5.4277, 'neural_augmentation': 0.2876, 'total_loss': 5.7153}
2024-08-04 06:55:52 - [34m[1mLOGS   [0m - *** Validation summary for epoch 7
	 loss={'classification': 3.0143, 'neural_augmentation': 0.0, 'total_loss': 3.0143} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.9362, 0.8553, 0.9214, 0.9598, 0.9452, 0.9633, 0.9148, 0.8821, 0.9419, 0.8999, 0.882, 0.932, 0.823, 0.9432, 0.9084, 0.9385, 0.9741, 0.978, 0.9397, 0.9384, 0.9333, 0.8441, 0.9333, 0.9571, 0.9176, 0.9371, 0.9463, 0.9624, 0.9315, 0.9202, 0.8677, 0.8686, 0.9249, 0.9613, 0.9033, 0.8953, 0.9615, 0.9397, 0.9165, 0.9083, 0.996, 0.8696, 0.9493, 0.9615, 0.9395, 0.9657, 0.8206, 0.9467, 0.8416, 0.9339, 0.8981, 0.9098, 0.9741, 0.9331, 0.9582, 0.9451, 0.983, 0.9376, 0.9228, 0.8802, 0.8818, 0.935, 0.9396, 0.9419, 0.9165, 0.8784, 0.968, 0.9531, 0.9504, 0.8316, 0.9183, 0.9097, 0.9482, 0.9333, 0.8547, 0.9114, 0.8808, 0.9445, 0.9531, 0.9695, 0.9047, 0.9697, 0.9176, 0.9102, 0.9628, 0.9636, 0.9351, 0.9315, 0.88, 0.953, 0.9719, 0.7297, 0.9395, 0.9003, 0.937, 0.9226, 0.9798, 0.8488, 0.9, 0.9212, 0.9262, 0.9155, 0.9438, 0.8863, 0.9558, 0.9276, 0.9688, 0.8789, 0.9486, 0.9547, 0.8875, 0.9673, 0.9486, 0.9424, 0.6769, 0.9325, 0.8479, 0.9525, 0.9331, 0.9526, 0.9495, 0.9128, 0.9526, 0.9577, 0.9271, 0.8798, 0.9503, 0.9563, 0.9333, 0.8982, 0.6453, 0.9545, 0.9681, 0.9426, 0.9376, 0.9543, 0.881, 0.956, 0.9242, 0.883, 0.9344, 0.8989, 0.9398, 0.9368, 0.9445, 0.9693, 0.9387, 0.9147, 0.9659, 0.9242, 0.8902, 0.8832, 0.9222, 0.9539, 0.9454, 0.8814, 0.8921, 0.9108, 0.9363, 0.9068, 0.9053, 0.932, 0.8588, 0.9352, 0.9428, 0.8889, 0.9385, 0.9582, 0.9373, 0.9221, 0.9402, 0.9461, 0.9382, 0.878], 'AP': [0.9755, 0.9166, 0.9688, 0.9875, 0.9661, 0.991, 0.9622, 0.9363, 0.9852, 0.9583, 0.9445, 0.9778, 0.8635, 0.9808, 0.9553, 0.9741, 0.9902, 0.9915, 0.9458, 0.9815, 0.9768, 0.91, 0.9761, 0.9836, 0.965, 0.9739, 0.9735, 0.9848, 0.9806, 0.967, 0.9216, 0.9256, 0.9627, 0.9776, 0.9596, 0.9349, 0.982, 0.9834, 0.9627, 0.938, 0.9995, 0.9107, 0.9654, 0.979, 0.9738, 0.9752, 0.9019, 0.9671, 0.8969, 0.9695, 0.9485, 0.9584, 0.9933, 0.9731, 0.984, 0.9803, 0.9918, 0.9764, 0.9707, 0.9232, 0.9341, 0.973, 0.9558, 0.9768, 0.9655, 0.9387, 0.985, 0.9842, 0.9684, 0.8863, 0.9319, 0.9501, 0.9824, 0.9699, 0.9098, 0.9513, 0.9304, 0.9792, 0.983, 0.9943, 0.9617, 0.9844, 0.972, 0.9467, 0.9878, 0.9779, 0.9815, 0.9766, 0.9415, 0.978, 0.9845, 0.7811, 0.9667, 0.9528, 0.941, 0.9702, 0.99, 0.8988, 0.9567, 0.9724, 0.9733, 0.9583, 0.9764, 0.9544, 0.9747, 0.9724, 0.9858, 0.9327, 0.9781, 0.9785, 0.9313, 0.9943, 0.986, 0.9778, 0.6939, 0.9746, 0.9071, 0.9841, 0.9702, 0.9743, 0.9837, 0.9616, 0.9845, 0.9747, 0.9773, 0.9266, 0.9757, 0.976, 0.9675, 0.9398, 0.6235, 0.9778, 0.9851, 0.9624, 0.9745, 0.9758, 0.9364, 0.9805, 0.9743, 0.9303, 0.976, 0.9299, 0.9842, 0.9727, 0.9837, 0.9935, 0.9846, 0.9649, 0.9909, 0.9642, 0.9426, 0.9261, 0.9692, 0.9753, 0.9726, 0.9208, 0.9275, 0.9629, 0.9651, 0.9361, 0.9625, 0.9662, 0.9153, 0.9785, 0.9663, 0.919, 0.9738, 0.984, 0.9687, 0.97, 0.9654, 0.9702, 0.9833, 0.9237], 'Recall@P=50': [0.9887, 0.948, 0.9817, 0.992, 0.976, 0.996, 0.9776, 0.96, 0.9976, 0.9874, 0.974, 0.9924, 0.876, 0.986, 0.976, 0.984, 0.996, 0.996, 0.94, 0.992, 0.988, 0.944, 0.988, 0.9912, 0.98, 0.984, 0.98, 0.992, 0.9954, 0.9856, 0.964, 0.964, 0.984, 0.98, 0.979, 0.96, 0.988, 0.9972, 0.98, 0.976, 1.0, 0.932, 0.992, 0.984, 0.98, 0.984, 0.952, 0.968, 0.9533, 0.984, 0.968, 0.9747, 1.0, 0.9863, 0.988, 0.99, 0.992, 0.988, 0.98, 0.94, 0.956, 0.984, 0.976, 0.992, 0.98, 0.968, 0.988, 0.992, 0.97, 0.916, 0.98, 0.97, 0.988, 0.9864, 0.924, 0.97, 0.964, 0.9853, 0.988, 1.0, 0.9826, 0.984, 0.9964, 0.96, 0.994, 0.996, 0.9973, 0.988, 0.9774, 0.988, 0.994, 0.836, 0.972, 0.9776, 0.936, 0.9909, 0.992, 0.952, 0.9767, 0.992, 0.9886, 0.984, 0.992, 0.9853, 0.976, 0.9883, 0.99, 0.964, 0.9864, 0.98, 0.964, 1.0, 0.998, 0.986, 0.82, 0.9853, 0.956, 0.9933, 0.984, 0.976, 0.994, 0.9853, 0.988, 0.98, 0.998, 0.94, 0.976, 0.984, 0.984, 0.968, 0.008, 0.98, 0.988, 0.972, 0.99, 0.976, 0.9667, 0.984, 0.991, 0.972, 0.992, 0.94, 0.996, 0.982, 0.992, 1.0, 0.9991, 0.9893, 0.992, 0.97, 0.974, 0.956, 0.9893, 0.98, 0.988, 0.948, 0.972, 0.982, 0.972, 0.972, 0.99, 0.98, 0.944, 0.9958, 0.984, 0.944, 0.9867, 0.99, 1.0, 0.976, 0.972, 0.972, 0.9996, 0.956], 'micro': 0.9709, 'macro': 0.9584, 'weighted': 0.97}
2024-08-04 06:56:06 - [34m[1mLOGS   [0m - Best checkpoint with score 0.96 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:56:06 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_score_0.6138.pt
2024-08-04 06:56:06 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.8461.pt', 'checkpoint_score_0.9104.pt', 'checkpoint_score_0.9371.pt', 'checkpoint_score_0.9519.pt', 'checkpoint_score_0.9584.pt']
2024-08-04 06:56:09 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_avg.pt
2024-08-04 06:56:09 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:56:09 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:56:10 - [34m[1mLOGS   [0m - Training checkpoint for epoch 7/iteration 934 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_7_iter_934.pt
2024-08-04 06:56:10 - [34m[1mLOGS   [0m - Model state for epoch 7/iteration 934 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_7_iter_934.pt
[31m===========================================================================[0m
2024-08-04 06:56:12 - [32m[1mINFO   [0m - Training epoch 8
2024-08-04 06:56:14 - [34m[1mLOGS   [0m - Epoch:   8 [     935/10000000], loss: {'classification': 4.1012, 'neural_augmentation': 0.3006, 'total_loss': 4.4019}, LR: [4.3e-05, 4.3e-05], Avg. batch load time: 1.366, Elapsed time:  1.57
2024-08-04 06:56:35 - [34m[1mLOGS   [0m - *** Training summary for epoch 8
	 loss={'classification': 5.0157, 'neural_augmentation': 0.2856, 'total_loss': 5.3014}
2024-08-04 06:57:10 - [34m[1mLOGS   [0m - *** Validation summary for epoch 8
	 loss={'classification': 2.7644, 'neural_augmentation': 0.0, 'total_loss': 2.7644} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.9443, 0.8732, 0.9182, 0.9616, 0.9467, 0.9675, 0.9167, 0.8968, 0.9456, 0.9074, 0.8875, 0.9381, 0.8243, 0.9464, 0.9185, 0.9524, 0.9775, 0.9798, 0.9441, 0.9587, 0.935, 0.8704, 0.924, 0.9577, 0.9278, 0.9427, 0.961, 0.9541, 0.9348, 0.9307, 0.8664, 0.8734, 0.9272, 0.9491, 0.9112, 0.8917, 0.9735, 0.9451, 0.9163, 0.9187, 0.996, 0.8589, 0.963, 0.9699, 0.9253, 0.9639, 0.8356, 0.9419, 0.8579, 0.9256, 0.903, 0.9149, 0.973, 0.9312, 0.9554, 0.9565, 0.98, 0.9426, 0.9328, 0.8835, 0.8631, 0.9306, 0.9463, 0.9608, 0.9248, 0.8837, 0.9695, 0.9609, 0.9487, 0.8442, 0.9307, 0.913, 0.9577, 0.936, 0.8669, 0.9137, 0.8942, 0.9422, 0.958, 0.9777, 0.921, 0.9702, 0.9316, 0.9106, 0.967, 0.9687, 0.9414, 0.932, 0.8904, 0.9655, 0.9729, 0.7609, 0.9431, 0.9034, 0.948, 0.9382, 0.978, 0.8408, 0.9114, 0.9147, 0.9358, 0.9303, 0.9237, 0.8928, 0.9634, 0.9381, 0.9717, 0.8889, 0.9549, 0.9493, 0.8998, 0.9717, 0.9502, 0.9413, 0.6843, 0.9409, 0.8641, 0.9559, 0.9276, 0.968, 0.9455, 0.9151, 0.9616, 0.9549, 0.9355, 0.8807, 0.9598, 0.9545, 0.9317, 0.9084, 0.6516, 0.9518, 0.9699, 0.9499, 0.9347, 0.9652, 0.8966, 0.9736, 0.9319, 0.9051, 0.9301, 0.8893, 0.9361, 0.9429, 0.9496, 0.9719, 0.9456, 0.9174, 0.9677, 0.9407, 0.8987, 0.9016, 0.9182, 0.9677, 0.9698, 0.8935, 0.9028, 0.9215, 0.9246, 0.9153, 0.9161, 0.9293, 0.8641, 0.9386, 0.952, 0.8907, 0.943, 0.9605, 0.9482, 0.9336, 0.94, 0.9516, 0.9387, 0.8819], 'AP': [0.982, 0.9342, 0.9704, 0.9897, 0.9672, 0.9958, 0.9678, 0.937, 0.9887, 0.9651, 0.9406, 0.9793, 0.874, 0.9819, 0.9596, 0.9781, 0.9915, 0.9865, 0.9688, 0.9906, 0.9713, 0.9308, 0.9786, 0.9863, 0.9736, 0.9752, 0.9845, 0.9821, 0.982, 0.9717, 0.9225, 0.938, 0.9706, 0.9754, 0.9638, 0.93, 0.9816, 0.9865, 0.9651, 0.9493, 0.9999, 0.9047, 0.985, 0.9874, 0.9704, 0.9777, 0.9083, 0.9727, 0.9137, 0.9645, 0.9533, 0.9668, 0.9934, 0.9724, 0.9825, 0.9844, 0.9933, 0.978, 0.9728, 0.9316, 0.9254, 0.9751, 0.9456, 0.9873, 0.9736, 0.9404, 0.9884, 0.9877, 0.9742, 0.8994, 0.9499, 0.9542, 0.9853, 0.9719, 0.9314, 0.9561, 0.9316, 0.9809, 0.9875, 0.9877, 0.9741, 0.9873, 0.9797, 0.9434, 0.9923, 0.9871, 0.9835, 0.975, 0.951, 0.9875, 0.9915, 0.8268, 0.9682, 0.9575, 0.972, 0.9799, 0.9892, 0.8844, 0.9617, 0.972, 0.9767, 0.9602, 0.9588, 0.9568, 0.983, 0.9772, 0.9884, 0.9291, 0.9811, 0.977, 0.9375, 0.9865, 0.9875, 0.9762, 0.7093, 0.976, 0.9187, 0.9845, 0.9696, 0.9854, 0.9802, 0.9647, 0.9864, 0.9796, 0.9814, 0.9348, 0.9701, 0.975, 0.9708, 0.9381, 0.6356, 0.978, 0.9888, 0.9633, 0.9757, 0.9846, 0.9472, 0.9877, 0.9763, 0.952, 0.9753, 0.9446, 0.9861, 0.9746, 0.9845, 0.9894, 0.987, 0.9654, 0.9908, 0.9681, 0.9442, 0.9441, 0.9675, 0.9859, 0.9887, 0.9332, 0.9303, 0.9695, 0.9641, 0.9445, 0.9673, 0.9684, 0.9202, 0.9792, 0.974, 0.9229, 0.9773, 0.9867, 0.9799, 0.9805, 0.9678, 0.9792, 0.9845, 0.9271], 'Recall@P=50': [0.9933, 0.968, 0.9874, 0.996, 0.968, 1.0, 0.9848, 0.956, 0.9992, 0.9932, 0.962, 0.9936, 0.896, 0.99, 0.978, 0.988, 0.9936, 0.988, 0.972, 0.996, 0.976, 0.956, 0.996, 0.9952, 0.988, 0.9808, 0.988, 0.992, 0.9977, 0.9856, 0.968, 0.968, 0.988, 0.98, 0.9805, 0.964, 0.988, 0.9979, 0.982, 0.976, 1.0, 0.952, 0.992, 0.992, 0.988, 0.984, 0.9573, 0.98, 0.9587, 0.988, 0.972, 0.9821, 0.998, 0.984, 0.986, 0.9927, 0.994, 0.992, 0.98, 0.952, 0.972, 0.984, 0.976, 0.988, 0.9887, 0.966, 0.992, 0.9907, 0.98, 0.944, 0.98, 0.968, 0.992, 0.9864, 0.964, 0.974, 0.964, 0.9907, 0.9933, 0.988, 0.9911, 0.988, 0.9972, 0.96, 0.996, 0.996, 0.9975, 0.996, 0.9774, 0.992, 0.998, 0.904, 0.972, 0.9792, 0.976, 0.9949, 0.988, 0.928, 0.982, 0.992, 0.9851, 0.978, 0.976, 0.9893, 0.984, 0.987, 0.994, 0.952, 0.9864, 0.98, 0.956, 0.988, 0.998, 0.986, 0.82, 0.9873, 0.96, 0.9947, 0.984, 0.988, 0.992, 0.9867, 0.992, 0.984, 0.9977, 0.948, 0.984, 0.9813, 0.984, 0.968, 0.82, 0.984, 0.992, 0.98, 0.9867, 0.988, 0.9733, 0.988, 0.991, 0.98, 0.992, 0.968, 1.0, 0.982, 0.9928, 0.992, 0.9989, 0.9867, 0.996, 0.982, 0.972, 0.962, 0.9853, 0.988, 0.994, 0.952, 0.956, 0.99, 0.98, 0.964, 0.992, 0.976, 0.956, 0.994, 0.988, 0.948, 0.9867, 0.992, 0.992, 0.996, 0.976, 0.98, 0.9988, 0.968], 'micro': 0.9746, 'macro': 0.9628, 'weighted': 0.9737}
2024-08-04 06:57:23 - [34m[1mLOGS   [0m - Best checkpoint with score 0.96 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:57:23 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_score_0.8461.pt
2024-08-04 06:57:23 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.9104.pt', 'checkpoint_score_0.9371.pt', 'checkpoint_score_0.9519.pt', 'checkpoint_score_0.9584.pt', 'checkpoint_score_0.9628.pt']
2024-08-04 06:57:27 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_avg.pt
2024-08-04 06:57:28 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:57:28 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:57:29 - [34m[1mLOGS   [0m - Training checkpoint for epoch 8/iteration 1049 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_8_iter_1049.pt
2024-08-04 06:57:29 - [34m[1mLOGS   [0m - Model state for epoch 8/iteration 1049 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_8_iter_1049.pt
[31m===========================================================================[0m
2024-08-04 06:57:31 - [32m[1mINFO   [0m - Training epoch 9
2024-08-04 06:57:33 - [34m[1mLOGS   [0m - Epoch:   9 [    1050/10000000], loss: {'classification': 3.8011, 'neural_augmentation': 0.2792, 'total_loss': 4.0803}, LR: [4.1e-05, 4.1e-05], Avg. batch load time: 2.068, Elapsed time:  2.23
2024-08-04 06:57:51 - [34m[1mLOGS   [0m - *** Training summary for epoch 9
	 loss={'classification': 4.6413, 'neural_augmentation': 0.2852, 'total_loss': 4.9265}
2024-08-04 06:58:18 - [34m[1mLOGS   [0m - *** Validation summary for epoch 9
	 loss={'classification': 2.5673, 'neural_augmentation': 0.0, 'total_loss': 2.5673} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.9459, 0.8871, 0.927, 0.9545, 0.9535, 0.9736, 0.9224, 0.8946, 0.9517, 0.9147, 0.9035, 0.9418, 0.8363, 0.9511, 0.9226, 0.9514, 0.9753, 0.9759, 0.9491, 0.9636, 0.9535, 0.8709, 0.9293, 0.9589, 0.9313, 0.9432, 0.9611, 0.9681, 0.9439, 0.936, 0.8758, 0.8637, 0.9357, 0.9616, 0.9141, 0.9008, 0.9737, 0.9488, 0.9254, 0.9224, 0.998, 0.8763, 0.9598, 0.9738, 0.9265, 0.972, 0.8284, 0.9558, 0.8524, 0.9234, 0.9125, 0.923, 0.9675, 0.9371, 0.9567, 0.9521, 0.9809, 0.9341, 0.9331, 0.8907, 0.8768, 0.9409, 0.9518, 0.959, 0.9307, 0.885, 0.9715, 0.9618, 0.9478, 0.8601, 0.9318, 0.9134, 0.9516, 0.9428, 0.8739, 0.9128, 0.8946, 0.9427, 0.9577, 0.9675, 0.9286, 0.9759, 0.9348, 0.9175, 0.9635, 0.9699, 0.9442, 0.9234, 0.8973, 0.9563, 0.9724, 0.7773, 0.9516, 0.9105, 0.9537, 0.9425, 0.9798, 0.8577, 0.9165, 0.9159, 0.942, 0.9324, 0.9353, 0.8892, 0.9676, 0.9422, 0.9709, 0.888, 0.9541, 0.9657, 0.8992, 0.9696, 0.9508, 0.9454, 0.7076, 0.9439, 0.8802, 0.9599, 0.9176, 0.9719, 0.9532, 0.9208, 0.9572, 0.9677, 0.9383, 0.8898, 0.9579, 0.9588, 0.9369, 0.9139, 0.6619, 0.9637, 0.9719, 0.9465, 0.9399, 0.9736, 0.908, 0.9756, 0.9363, 0.9131, 0.9304, 0.9045, 0.9555, 0.9446, 0.9545, 0.9701, 0.9491, 0.9276, 0.9658, 0.9379, 0.8983, 0.9098, 0.9246, 0.9699, 0.9658, 0.902, 0.9091, 0.9323, 0.9256, 0.9158, 0.9129, 0.9331, 0.8673, 0.9396, 0.9583, 0.9, 0.9412, 0.9671, 0.9419, 0.9287, 0.9482, 0.9489, 0.9441, 0.8707], 'AP': [0.9801, 0.9339, 0.9738, 0.9785, 0.9687, 0.9914, 0.9665, 0.9446, 0.9854, 0.9689, 0.9595, 0.9817, 0.889, 0.9829, 0.9606, 0.9823, 0.9917, 0.9857, 0.972, 0.9885, 0.981, 0.9368, 0.9798, 0.985, 0.9744, 0.9748, 0.9841, 0.9869, 0.9853, 0.9741, 0.9192, 0.9282, 0.9668, 0.9781, 0.968, 0.9416, 0.9882, 0.987, 0.9689, 0.9529, 1.0, 0.9127, 0.9746, 0.9849, 0.9629, 0.9735, 0.9149, 0.9761, 0.9193, 0.9614, 0.952, 0.9696, 0.9891, 0.9757, 0.9856, 0.9852, 0.9924, 0.9787, 0.9745, 0.9365, 0.934, 0.972, 0.9643, 0.9859, 0.9735, 0.9427, 0.9892, 0.9861, 0.9757, 0.9169, 0.9516, 0.9566, 0.9822, 0.9764, 0.9309, 0.9559, 0.9379, 0.981, 0.9853, 0.9919, 0.9785, 0.9901, 0.98, 0.9421, 0.9922, 0.9827, 0.985, 0.9774, 0.9565, 0.9855, 0.9921, 0.8399, 0.9754, 0.9609, 0.9636, 0.9826, 0.9941, 0.9182, 0.9645, 0.9745, 0.9769, 0.9613, 0.9599, 0.9585, 0.9821, 0.9798, 0.9853, 0.9351, 0.9795, 0.9824, 0.9409, 0.9827, 0.9885, 0.9819, 0.7445, 0.9774, 0.9272, 0.9876, 0.9728, 0.9831, 0.9794, 0.9658, 0.9845, 0.9887, 0.983, 0.9338, 0.9712, 0.978, 0.9747, 0.9455, 0.6536, 0.9826, 0.9876, 0.9737, 0.9774, 0.9824, 0.9497, 0.9867, 0.9801, 0.9503, 0.9757, 0.9507, 0.986, 0.9757, 0.9868, 0.9878, 0.9886, 0.9707, 0.9812, 0.9715, 0.9515, 0.9519, 0.9682, 0.9811, 0.9829, 0.9323, 0.9326, 0.9757, 0.9544, 0.9538, 0.9682, 0.9706, 0.9158, 0.9811, 0.9709, 0.9303, 0.9758, 0.9875, 0.9727, 0.9792, 0.9741, 0.9808, 0.9857, 0.9228], 'Recall@P=50': [0.9907, 0.956, 0.988, 0.996, 0.968, 0.996, 0.984, 0.968, 0.9992, 0.9926, 0.986, 0.9932, 0.904, 0.988, 0.974, 0.992, 0.996, 0.984, 0.976, 0.996, 0.992, 0.972, 0.992, 0.9904, 0.9893, 0.9904, 0.984, 0.992, 0.9973, 0.9904, 0.952, 0.96, 0.984, 0.98, 0.9865, 0.968, 0.992, 0.9977, 0.988, 0.98, 1.0, 0.964, 0.992, 0.988, 0.98, 0.984, 0.9627, 0.984, 0.968, 0.984, 0.968, 0.9837, 0.996, 0.9886, 0.992, 0.9913, 0.992, 0.992, 0.988, 0.96, 0.98, 0.98, 0.976, 0.992, 0.986, 0.972, 0.992, 0.9933, 0.98, 0.944, 0.98, 0.974, 0.988, 0.9888, 0.956, 0.974, 0.964, 0.9893, 0.9893, 0.996, 0.9916, 0.992, 0.9966, 0.948, 0.996, 0.998, 0.9978, 0.992, 0.9843, 0.996, 0.996, 0.896, 0.984, 0.98, 0.96, 0.996, 0.996, 0.956, 0.982, 0.996, 0.9869, 0.974, 0.972, 0.9907, 0.984, 0.99, 0.992, 0.976, 0.9856, 0.984, 0.972, 0.984, 0.999, 0.992, 0.844, 0.9887, 0.956, 0.994, 0.988, 0.984, 0.992, 0.9853, 0.992, 0.992, 0.9983, 0.968, 0.988, 0.9867, 0.988, 0.976, 0.012, 0.988, 0.992, 0.984, 0.9887, 0.984, 0.972, 0.992, 0.994, 0.972, 0.996, 0.968, 1.0, 0.984, 0.9936, 0.992, 0.9994, 0.988, 0.992, 0.984, 0.976, 0.98, 0.9827, 0.98, 0.994, 0.948, 0.96, 0.991, 0.972, 0.976, 0.99, 0.984, 0.952, 0.996, 0.984, 0.952, 0.9887, 0.99, 0.996, 0.996, 0.984, 0.988, 0.9996, 0.948], 'micro': 0.9767, 'macro': 0.9648, 'weighted': 0.9755}
2024-08-04 06:58:23 - [34m[1mLOGS   [0m - Best checkpoint with score 0.96 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:58:24 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_score_0.9104.pt
2024-08-04 06:58:24 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.9371.pt', 'checkpoint_score_0.9519.pt', 'checkpoint_score_0.9584.pt', 'checkpoint_score_0.9628.pt', 'checkpoint_score_0.9648.pt']
2024-08-04 06:58:47 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_avg.pt
2024-08-04 06:58:48 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:58:48 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:58:48 - [34m[1mLOGS   [0m - Training checkpoint for epoch 9/iteration 1159 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_9_iter_1159.pt
2024-08-04 06:58:48 - [34m[1mLOGS   [0m - Model state for epoch 9/iteration 1159 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_9_iter_1159.pt
[31m===========================================================================[0m
2024-08-04 06:58:50 - [32m[1mINFO   [0m - Training epoch 10
2024-08-04 06:58:53 - [34m[1mLOGS   [0m - Epoch:  10 [    1160/10000000], loss: {'classification': 3.8773, 'neural_augmentation': 0.2795, 'total_loss': 4.1568}, LR: [3.9e-05, 3.9e-05], Avg. batch load time: 2.594, Elapsed time:  2.75
2024-08-04 06:59:13 - [34m[1mLOGS   [0m - *** Training summary for epoch 10
	 loss={'classification': 4.2586, 'neural_augmentation': 0.2911, 'total_loss': 4.5497}
2024-08-04 06:59:37 - [34m[1mLOGS   [0m - *** Validation summary for epoch 10
	 loss={'classification': 2.3975, 'neural_augmentation': 0.0, 'total_loss': 2.3975} || multiclass_classification_pr(pred=logits)={'ODS-F1': [0.9523, 0.8917, 0.9344, 0.9654, 0.9553, 0.9776, 0.9296, 0.8967, 0.9536, 0.9213, 0.9113, 0.9429, 0.8395, 0.9488, 0.9201, 0.9428, 0.9775, 0.9778, 0.9446, 0.9581, 0.9616, 0.8754, 0.9363, 0.965, 0.9315, 0.9452, 0.953, 0.9598, 0.9477, 0.9399, 0.8802, 0.8834, 0.945, 0.9576, 0.9201, 0.9095, 0.9701, 0.9502, 0.9288, 0.9306, 0.996, 0.8708, 0.9661, 0.9738, 0.9384, 0.9679, 0.8411, 0.9631, 0.8641, 0.9461, 0.9065, 0.9298, 0.9737, 0.9427, 0.9613, 0.9587, 0.9839, 0.9395, 0.944, 0.8998, 0.8841, 0.9472, 0.9516, 0.9539, 0.9376, 0.8996, 0.9799, 0.9664, 0.9563, 0.8727, 0.9357, 0.9228, 0.9556, 0.9457, 0.8916, 0.9201, 0.8939, 0.9481, 0.9642, 0.9741, 0.9376, 0.9799, 0.9389, 0.9076, 0.9718, 0.9722, 0.947, 0.9407, 0.9022, 0.9658, 0.9756, 0.8084, 0.9512, 0.9099, 0.9397, 0.9468, 0.9817, 0.8723, 0.9196, 0.926, 0.9479, 0.9389, 0.9317, 0.9002, 0.9717, 0.9468, 0.9748, 0.8926, 0.956, 0.9597, 0.8984, 0.9757, 0.9569, 0.9543, 0.7284, 0.9502, 0.8838, 0.9605, 0.9289, 0.9735, 0.9506, 0.922, 0.9576, 0.9655, 0.9393, 0.8956, 0.9603, 0.9603, 0.9405, 0.9153, 0.6496, 0.9574, 0.9817, 0.953, 0.9468, 0.9719, 0.9102, 0.9738, 0.9383, 0.9102, 0.9368, 0.9174, 0.9499, 0.9498, 0.9562, 0.9781, 0.9528, 0.928, 0.9655, 0.94, 0.9154, 0.9136, 0.9274, 0.9736, 0.9718, 0.9, 0.9155, 0.9355, 0.9363, 0.9315, 0.9163, 0.9369, 0.8742, 0.9468, 0.9616, 0.9084, 0.9504, 0.9661, 0.9493, 0.9347, 0.9539, 0.9499, 0.9504, 0.877], 'AP': [0.9828, 0.9382, 0.9759, 0.9852, 0.9727, 0.9927, 0.9706, 0.9483, 0.9905, 0.9709, 0.9626, 0.9819, 0.8905, 0.9858, 0.9558, 0.9753, 0.9935, 0.9927, 0.9605, 0.9906, 0.9804, 0.9343, 0.9799, 0.9862, 0.9745, 0.9794, 0.9883, 0.987, 0.9864, 0.9771, 0.9126, 0.9432, 0.9814, 0.9787, 0.9704, 0.9525, 0.9892, 0.9883, 0.9711, 0.9624, 0.9999, 0.9189, 0.9748, 0.988, 0.976, 0.9734, 0.92, 0.982, 0.9269, 0.9737, 0.9504, 0.9711, 0.9924, 0.9788, 0.99, 0.9868, 0.9956, 0.9691, 0.975, 0.9426, 0.932, 0.9685, 0.9567, 0.9859, 0.9767, 0.9544, 0.9931, 0.9914, 0.9785, 0.9178, 0.9621, 0.9619, 0.9796, 0.9807, 0.9418, 0.9598, 0.9429, 0.9846, 0.9866, 0.9921, 0.9779, 0.9913, 0.983, 0.9438, 0.9915, 0.9853, 0.9866, 0.9765, 0.9553, 0.9865, 0.9908, 0.8664, 0.972, 0.9588, 0.952, 0.983, 0.9937, 0.9175, 0.9638, 0.9829, 0.9803, 0.9675, 0.9632, 0.9623, 0.9812, 0.9816, 0.9903, 0.946, 0.9821, 0.9834, 0.9448, 0.9893, 0.9901, 0.98, 0.7596, 0.9821, 0.9275, 0.9887, 0.978, 0.9845, 0.9847, 0.9666, 0.9833, 0.9851, 0.9833, 0.9439, 0.9763, 0.9816, 0.9817, 0.944, 0.6617, 0.9833, 0.9917, 0.9731, 0.9801, 0.984, 0.9555, 0.9872, 0.9795, 0.9575, 0.9766, 0.9544, 0.9897, 0.9803, 0.988, 0.9906, 0.9893, 0.9718, 0.9874, 0.9716, 0.9551, 0.9527, 0.968, 0.9841, 0.9882, 0.9373, 0.9395, 0.9748, 0.9718, 0.9548, 0.9679, 0.9728, 0.9116, 0.984, 0.9725, 0.9327, 0.9793, 0.9883, 0.9801, 0.9812, 0.9768, 0.9807, 0.9885, 0.9208], 'Recall@P=50': [0.992, 0.956, 0.9897, 0.992, 0.98, 0.996, 0.9832, 0.976, 0.9992, 0.9935, 0.98, 0.9924, 0.916, 0.996, 0.974, 0.984, 0.9968, 0.996, 0.96, 1.0, 0.988, 0.956, 0.996, 0.9928, 0.9867, 0.9912, 0.996, 0.992, 0.9975, 0.9864, 0.948, 0.976, 0.988, 0.984, 0.987, 0.98, 0.996, 0.9979, 0.99, 0.972, 1.0, 0.952, 0.992, 0.992, 0.984, 0.984, 0.9613, 0.988, 0.968, 0.984, 0.964, 0.9837, 0.994, 0.9903, 0.998, 0.9933, 0.996, 0.988, 0.984, 0.972, 0.968, 0.976, 0.972, 0.996, 0.9893, 0.98, 0.996, 0.996, 0.984, 0.952, 0.98, 0.988, 0.984, 0.9928, 0.976, 0.976, 0.968, 0.992, 0.9907, 0.992, 0.9916, 0.996, 0.9976, 0.956, 0.994, 0.994, 0.9982, 0.992, 0.98, 0.996, 0.996, 0.912, 0.984, 0.9792, 0.96, 0.996, 0.996, 0.952, 0.9813, 1.0, 0.9891, 0.982, 0.972, 0.988, 0.98, 0.992, 0.998, 0.972, 0.9896, 0.992, 0.976, 0.988, 0.9992, 0.984, 0.872, 0.9947, 0.976, 0.9947, 0.988, 0.984, 0.998, 0.9867, 0.996, 0.988, 0.9983, 0.96, 0.984, 0.9907, 0.988, 0.984, 0.008, 0.992, 0.996, 0.992, 0.9907, 0.984, 0.9813, 0.988, 0.9917, 0.98, 0.992, 0.972, 0.996, 0.988, 0.9952, 0.992, 0.9995, 0.992, 0.988, 0.984, 0.982, 0.982, 0.988, 0.988, 0.992, 0.952, 0.948, 0.99, 0.984, 0.976, 0.993, 0.988, 0.952, 0.9965, 0.988, 0.952, 0.992, 0.992, 1.0, 0.992, 0.988, 0.98, 0.9995, 0.96], 'micro': 0.9788, 'macro': 0.9675, 'weighted': 0.9776}
2024-08-04 06:59:50 - [34m[1mLOGS   [0m - Best checkpoint with score 0.97 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_best.pt
2024-08-04 06:59:50 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_score_0.9371.pt
2024-08-04 06:59:50 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_0.9519.pt', 'checkpoint_score_0.9584.pt', 'checkpoint_score_0.9628.pt', 'checkpoint_score_0.9648.pt', 'checkpoint_score_0.9675.pt']
2024-08-04 06:59:52 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_avg.pt
2024-08-04 06:59:53 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_last.pt
2024-08-04 06:59:53 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_last.pt
2024-08-04 06:59:53 - [34m[1mLOGS   [0m - Training checkpoint for epoch 10/iteration 1275 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/training_checkpoint_epoch_10_iter_1275.pt
2024-08-04 06:59:54 - [34m[1mLOGS   [0m - Model state for epoch 10/iteration 1275 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/ingredient_101/train/checkpoint_epoch_10_iter_1275.pt
[31m===========================================================================[0m
2024-08-04 06:59:56 - [32m[1mINFO   [0m - Training epoch 11
2024-08-04 06:59:57 - [34m[1mLOGS   [0m - Epoch:  11 [    1276/10000000], loss: {'classification': 3.5586, 'neural_augmentation': 0.2825, 'total_loss': 3.8411}, LR: [3.7e-05, 3.7e-05], Avg. batch load time: 1.546, Elapsed time:  1.72
