nohup: ignoring input
2024-07-27 16:23:28 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
2024-07-27 16:23:30 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/vit_base.pt
2024-07-27 16:23:30 - [32m[1mINFO   [0m - Trainable parameters: ['neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_emb.0.block.conv.weight', 'patch_emb.0.block.norm.weight', 'patch_emb.0.block.norm.bias', 'patch_emb.1.block.conv.weight', 'patch_emb.1.block.norm.weight', 'patch_emb.1.block.norm.bias', 'patch_emb.2.block.conv.weight', 'patch_emb.2.block.conv.bias', 'post_transformer_norm.weight', 'post_transformer_norm.bias', 'transformer.0.pre_norm_mha.0.weight', 'transformer.0.pre_norm_mha.0.bias', 'transformer.0.pre_norm_mha.1.qkv_proj.weight', 'transformer.0.pre_norm_mha.1.qkv_proj.bias', 'transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'transformer.0.pre_norm_ffn.0.weight', 'transformer.0.pre_norm_ffn.0.bias', 'transformer.0.pre_norm_ffn.1.weight', 'transformer.0.pre_norm_ffn.1.bias', 'transformer.0.pre_norm_ffn.4.weight', 'transformer.0.pre_norm_ffn.4.bias', 'transformer.1.pre_norm_mha.0.weight', 'transformer.1.pre_norm_mha.0.bias', 'transformer.1.pre_norm_mha.1.qkv_proj.weight', 'transformer.1.pre_norm_mha.1.qkv_proj.bias', 'transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'transformer.1.pre_norm_ffn.0.weight', 'transformer.1.pre_norm_ffn.0.bias', 'transformer.1.pre_norm_ffn.1.weight', 'transformer.1.pre_norm_ffn.1.bias', 'transformer.1.pre_norm_ffn.4.weight', 'transformer.1.pre_norm_ffn.4.bias', 'transformer.2.pre_norm_mha.0.weight', 'transformer.2.pre_norm_mha.0.bias', 'transformer.2.pre_norm_mha.1.qkv_proj.weight', 'transformer.2.pre_norm_mha.1.qkv_proj.bias', 'transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'transformer.2.pre_norm_ffn.0.weight', 'transformer.2.pre_norm_ffn.0.bias', 'transformer.2.pre_norm_ffn.1.weight', 'transformer.2.pre_norm_ffn.1.bias', 'transformer.2.pre_norm_ffn.4.weight', 'transformer.2.pre_norm_ffn.4.bias', 'transformer.3.pre_norm_mha.0.weight', 'transformer.3.pre_norm_mha.0.bias', 'transformer.3.pre_norm_mha.1.qkv_proj.weight', 'transformer.3.pre_norm_mha.1.qkv_proj.bias', 'transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'transformer.3.pre_norm_ffn.0.weight', 'transformer.3.pre_norm_ffn.0.bias', 'transformer.3.pre_norm_ffn.1.weight', 'transformer.3.pre_norm_ffn.1.bias', 'transformer.3.pre_norm_ffn.4.weight', 'transformer.3.pre_norm_ffn.4.bias', 'transformer.4.pre_norm_mha.0.weight', 'transformer.4.pre_norm_mha.0.bias', 'transformer.4.pre_norm_mha.1.qkv_proj.weight', 'transformer.4.pre_norm_mha.1.qkv_proj.bias', 'transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'transformer.4.pre_norm_ffn.0.weight', 'transformer.4.pre_norm_ffn.0.bias', 'transformer.4.pre_norm_ffn.1.weight', 'transformer.4.pre_norm_ffn.1.bias', 'transformer.4.pre_norm_ffn.4.weight', 'transformer.4.pre_norm_ffn.4.bias', 'transformer.5.pre_norm_mha.0.weight', 'transformer.5.pre_norm_mha.0.bias', 'transformer.5.pre_norm_mha.1.qkv_proj.weight', 'transformer.5.pre_norm_mha.1.qkv_proj.bias', 'transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'transformer.5.pre_norm_ffn.0.weight', 'transformer.5.pre_norm_ffn.0.bias', 'transformer.5.pre_norm_ffn.1.weight', 'transformer.5.pre_norm_ffn.1.bias', 'transformer.5.pre_norm_ffn.4.weight', 'transformer.5.pre_norm_ffn.4.bias', 'transformer.6.pre_norm_mha.0.weight', 'transformer.6.pre_norm_mha.0.bias', 'transformer.6.pre_norm_mha.1.qkv_proj.weight', 'transformer.6.pre_norm_mha.1.qkv_proj.bias', 'transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'transformer.6.pre_norm_ffn.0.weight', 'transformer.6.pre_norm_ffn.0.bias', 'transformer.6.pre_norm_ffn.1.weight', 'transformer.6.pre_norm_ffn.1.bias', 'transformer.6.pre_norm_ffn.4.weight', 'transformer.6.pre_norm_ffn.4.bias', 'transformer.7.pre_norm_mha.0.weight', 'transformer.7.pre_norm_mha.0.bias', 'transformer.7.pre_norm_mha.1.qkv_proj.weight', 'transformer.7.pre_norm_mha.1.qkv_proj.bias', 'transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'transformer.7.pre_norm_ffn.0.weight', 'transformer.7.pre_norm_ffn.0.bias', 'transformer.7.pre_norm_ffn.1.weight', 'transformer.7.pre_norm_ffn.1.bias', 'transformer.7.pre_norm_ffn.4.weight', 'transformer.7.pre_norm_ffn.4.bias', 'transformer.8.pre_norm_mha.0.weight', 'transformer.8.pre_norm_mha.0.bias', 'transformer.8.pre_norm_mha.1.qkv_proj.weight', 'transformer.8.pre_norm_mha.1.qkv_proj.bias', 'transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'transformer.8.pre_norm_ffn.0.weight', 'transformer.8.pre_norm_ffn.0.bias', 'transformer.8.pre_norm_ffn.1.weight', 'transformer.8.pre_norm_ffn.1.bias', 'transformer.8.pre_norm_ffn.4.weight', 'transformer.8.pre_norm_ffn.4.bias', 'transformer.9.pre_norm_mha.0.weight', 'transformer.9.pre_norm_mha.0.bias', 'transformer.9.pre_norm_mha.1.qkv_proj.weight', 'transformer.9.pre_norm_mha.1.qkv_proj.bias', 'transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'transformer.9.pre_norm_ffn.0.weight', 'transformer.9.pre_norm_ffn.0.bias', 'transformer.9.pre_norm_ffn.1.weight', 'transformer.9.pre_norm_ffn.1.bias', 'transformer.9.pre_norm_ffn.4.weight', 'transformer.9.pre_norm_ffn.4.bias', 'transformer.10.pre_norm_mha.0.weight', 'transformer.10.pre_norm_mha.0.bias', 'transformer.10.pre_norm_mha.1.qkv_proj.weight', 'transformer.10.pre_norm_mha.1.qkv_proj.bias', 'transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'transformer.10.pre_norm_ffn.0.weight', 'transformer.10.pre_norm_ffn.0.bias', 'transformer.10.pre_norm_ffn.1.weight', 'transformer.10.pre_norm_ffn.1.bias', 'transformer.10.pre_norm_ffn.4.weight', 'transformer.10.pre_norm_ffn.4.bias', 'transformer.11.pre_norm_mha.0.weight', 'transformer.11.pre_norm_mha.0.bias', 'transformer.11.pre_norm_mha.1.qkv_proj.weight', 'transformer.11.pre_norm_mha.1.qkv_proj.bias', 'transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'transformer.11.pre_norm_ffn.0.weight', 'transformer.11.pre_norm_ffn.0.bias', 'transformer.11.pre_norm_ffn.1.weight', 'transformer.11.pre_norm_ffn.1.bias', 'transformer.11.pre_norm_ffn.4.weight', 'transformer.11.pre_norm_ffn.4.bias', 'classifier.weight', 'classifier.bias', 'pos_embed.pos_embed.pos_embed']
2024-07-27 16:23:30 - [32m[1mINFO   [0m - Trainable parameters: ['aspp.aspp_layer.convs.0.block.conv.weight', 'aspp.aspp_layer.convs.0.block.norm.weight', 'aspp.aspp_layer.convs.0.block.norm.bias', 'aspp.aspp_layer.convs.1.block.conv.weight', 'aspp.aspp_layer.convs.1.block.norm.weight', 'aspp.aspp_layer.convs.1.block.norm.bias', 'aspp.aspp_layer.convs.2.block.conv.weight', 'aspp.aspp_layer.convs.2.block.norm.weight', 'aspp.aspp_layer.convs.2.block.norm.bias', 'aspp.aspp_layer.convs.3.block.conv.weight', 'aspp.aspp_layer.convs.3.block.norm.weight', 'aspp.aspp_layer.convs.3.block.norm.bias', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.conv.weight', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.weight', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.bias', 'aspp.aspp_layer.project.block.conv.weight', 'aspp.aspp_layer.project.block.norm.weight', 'aspp.aspp_layer.project.block.norm.bias', 'classifier.block.conv.weight', 'classifier.block.conv.bias']
2024-07-27 16:23:30 - [32m[1mINFO   [0m - Trainable parameters: ['encoder.neural_augmentor.brightness._low', 'encoder.neural_augmentor.brightness._high', 'encoder.neural_augmentor.contrast._low', 'encoder.neural_augmentor.contrast._high', 'encoder.neural_augmentor.noise._low', 'encoder.neural_augmentor.noise._high', 'encoder.patch_emb.0.block.conv.weight', 'encoder.patch_emb.0.block.norm.weight', 'encoder.patch_emb.0.block.norm.bias', 'encoder.patch_emb.1.block.conv.weight', 'encoder.patch_emb.1.block.norm.weight', 'encoder.patch_emb.1.block.norm.bias', 'encoder.patch_emb.2.block.conv.weight', 'encoder.patch_emb.2.block.conv.bias', 'encoder.post_transformer_norm.weight', 'encoder.post_transformer_norm.bias', 'encoder.transformer.0.pre_norm_mha.0.weight', 'encoder.transformer.0.pre_norm_mha.0.bias', 'encoder.transformer.0.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.0.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.0.pre_norm_ffn.0.weight', 'encoder.transformer.0.pre_norm_ffn.0.bias', 'encoder.transformer.0.pre_norm_ffn.1.weight', 'encoder.transformer.0.pre_norm_ffn.1.bias', 'encoder.transformer.0.pre_norm_ffn.4.weight', 'encoder.transformer.0.pre_norm_ffn.4.bias', 'encoder.transformer.1.pre_norm_mha.0.weight', 'encoder.transformer.1.pre_norm_mha.0.bias', 'encoder.transformer.1.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.1.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.1.pre_norm_ffn.0.weight', 'encoder.transformer.1.pre_norm_ffn.0.bias', 'encoder.transformer.1.pre_norm_ffn.1.weight', 'encoder.transformer.1.pre_norm_ffn.1.bias', 'encoder.transformer.1.pre_norm_ffn.4.weight', 'encoder.transformer.1.pre_norm_ffn.4.bias', 'encoder.transformer.2.pre_norm_mha.0.weight', 'encoder.transformer.2.pre_norm_mha.0.bias', 'encoder.transformer.2.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.2.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.2.pre_norm_ffn.0.weight', 'encoder.transformer.2.pre_norm_ffn.0.bias', 'encoder.transformer.2.pre_norm_ffn.1.weight', 'encoder.transformer.2.pre_norm_ffn.1.bias', 'encoder.transformer.2.pre_norm_ffn.4.weight', 'encoder.transformer.2.pre_norm_ffn.4.bias', 'encoder.transformer.3.pre_norm_mha.0.weight', 'encoder.transformer.3.pre_norm_mha.0.bias', 'encoder.transformer.3.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.3.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.3.pre_norm_ffn.0.weight', 'encoder.transformer.3.pre_norm_ffn.0.bias', 'encoder.transformer.3.pre_norm_ffn.1.weight', 'encoder.transformer.3.pre_norm_ffn.1.bias', 'encoder.transformer.3.pre_norm_ffn.4.weight', 'encoder.transformer.3.pre_norm_ffn.4.bias', 'encoder.transformer.4.pre_norm_mha.0.weight', 'encoder.transformer.4.pre_norm_mha.0.bias', 'encoder.transformer.4.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.4.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.4.pre_norm_ffn.0.weight', 'encoder.transformer.4.pre_norm_ffn.0.bias', 'encoder.transformer.4.pre_norm_ffn.1.weight', 'encoder.transformer.4.pre_norm_ffn.1.bias', 'encoder.transformer.4.pre_norm_ffn.4.weight', 'encoder.transformer.4.pre_norm_ffn.4.bias', 'encoder.transformer.5.pre_norm_mha.0.weight', 'encoder.transformer.5.pre_norm_mha.0.bias', 'encoder.transformer.5.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.5.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.5.pre_norm_ffn.0.weight', 'encoder.transformer.5.pre_norm_ffn.0.bias', 'encoder.transformer.5.pre_norm_ffn.1.weight', 'encoder.transformer.5.pre_norm_ffn.1.bias', 'encoder.transformer.5.pre_norm_ffn.4.weight', 'encoder.transformer.5.pre_norm_ffn.4.bias', 'encoder.transformer.6.pre_norm_mha.0.weight', 'encoder.transformer.6.pre_norm_mha.0.bias', 'encoder.transformer.6.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.6.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.6.pre_norm_ffn.0.weight', 'encoder.transformer.6.pre_norm_ffn.0.bias', 'encoder.transformer.6.pre_norm_ffn.1.weight', 'encoder.transformer.6.pre_norm_ffn.1.bias', 'encoder.transformer.6.pre_norm_ffn.4.weight', 'encoder.transformer.6.pre_norm_ffn.4.bias', 'encoder.transformer.7.pre_norm_mha.0.weight', 'encoder.transformer.7.pre_norm_mha.0.bias', 'encoder.transformer.7.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.7.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.7.pre_norm_ffn.0.weight', 'encoder.transformer.7.pre_norm_ffn.0.bias', 'encoder.transformer.7.pre_norm_ffn.1.weight', 'encoder.transformer.7.pre_norm_ffn.1.bias', 'encoder.transformer.7.pre_norm_ffn.4.weight', 'encoder.transformer.7.pre_norm_ffn.4.bias', 'encoder.transformer.8.pre_norm_mha.0.weight', 'encoder.transformer.8.pre_norm_mha.0.bias', 'encoder.transformer.8.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.8.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.8.pre_norm_ffn.0.weight', 'encoder.transformer.8.pre_norm_ffn.0.bias', 'encoder.transformer.8.pre_norm_ffn.1.weight', 'encoder.transformer.8.pre_norm_ffn.1.bias', 'encoder.transformer.8.pre_norm_ffn.4.weight', 'encoder.transformer.8.pre_norm_ffn.4.bias', 'encoder.transformer.9.pre_norm_mha.0.weight', 'encoder.transformer.9.pre_norm_mha.0.bias', 'encoder.transformer.9.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.9.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.9.pre_norm_ffn.0.weight', 'encoder.transformer.9.pre_norm_ffn.0.bias', 'encoder.transformer.9.pre_norm_ffn.1.weight', 'encoder.transformer.9.pre_norm_ffn.1.bias', 'encoder.transformer.9.pre_norm_ffn.4.weight', 'encoder.transformer.9.pre_norm_ffn.4.bias', 'encoder.transformer.10.pre_norm_mha.0.weight', 'encoder.transformer.10.pre_norm_mha.0.bias', 'encoder.transformer.10.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.10.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.10.pre_norm_ffn.0.weight', 'encoder.transformer.10.pre_norm_ffn.0.bias', 'encoder.transformer.10.pre_norm_ffn.1.weight', 'encoder.transformer.10.pre_norm_ffn.1.bias', 'encoder.transformer.10.pre_norm_ffn.4.weight', 'encoder.transformer.10.pre_norm_ffn.4.bias', 'encoder.transformer.11.pre_norm_mha.0.weight', 'encoder.transformer.11.pre_norm_mha.0.bias', 'encoder.transformer.11.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.11.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.11.pre_norm_ffn.0.weight', 'encoder.transformer.11.pre_norm_ffn.0.bias', 'encoder.transformer.11.pre_norm_ffn.1.weight', 'encoder.transformer.11.pre_norm_ffn.1.bias', 'encoder.transformer.11.pre_norm_ffn.4.weight', 'encoder.transformer.11.pre_norm_ffn.4.bias', 'encoder.pos_embed.pos_embed.pos_embed', 'seg_head.aspp.aspp_layer.convs.0.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.0.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.0.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.1.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.1.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.1.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.2.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.2.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.2.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.3.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.3.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.3.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.bias', 'seg_head.aspp.aspp_layer.project.block.conv.weight', 'seg_head.aspp.aspp_layer.project.block.norm.weight', 'seg_head.aspp.aspp_layer.project.block.norm.bias', 'seg_head.classifier.block.conv.weight', 'seg_head.classifier.block.conv.bias']
2024-07-27 16:23:30 - [34m[1mLOGS   [0m - [36mModel[0m
SegEncoderDecoder(
  (encoder): VisionTransformer(
    (neural_augmentor): DistributionNeuralAugmentor(
    	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
    	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
    	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
    (patch_emb): Sequential(
      (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=GELU)
      (1): Conv2d(192, 192, kernel_size=(2, 2), stride=(2, 2), bias=False, normalization=BatchNorm2d, activation=GELU)
      (2): Conv2d(192, 768, kernel_size=(2, 2), stride=(2, 2))
    )
    (post_transformer_norm): LayerNormFP32((768,), eps=1e-06, elementwise_affine=True)
    (transformer): Sequential(
      (0): FlashTransformerEncoder
      (1): FlashTransformerEncoder
      (2): FlashTransformerEncoder
      (3): FlashTransformerEncoder
      (4): FlashTransformerEncoder
      (5): FlashTransformerEncoder
      (6): FlashTransformerEncoder
      (7): FlashTransformerEncoder
      (8): FlashTransformerEncoder
      (9): FlashTransformerEncoder
      (10): FlashTransformerEncoder
      (11): FlashTransformerEncoder
    )
    (classifier): None
    (pos_embed): LearnablePositionalEmbedding(num_embeddings=196, embedding_dim=768, padding_idx=None, sequence_first=False)
    (emb_dropout): Dropout(p=0.0, inplace=False)
  )
  (seg_head): DeeplabV3(
    (upsample_seg_out): UpSample(scale_factor=8.0, mode='bilinear')
    (aspp): Sequential(
      (aspp_layer): ASPP(in_channels=768, out_channels=512, atrous_rates=[12, 24, 36], is_aspp_sep=False, dropout=0.1)
    )
    (classifier): Conv2d(512, 103, kernel_size=(1, 1), stride=(1, 1))
  )
)
[31m=================================================================[0m
                  SegEncoderDecoder Summary
[31m=================================================================[0m
Total parameters     =   98.728 M
Total trainable parameters =   98.728 M

2024-07-27 16:23:30 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-27 16:23:30 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                                    | #parameters or shape   | #flops    |
|:------------------------------------------|:-----------------------|:----------|
| model                                     | 98.728M                | 77.441G   |
|  encoder                                  |  85.955M               |  67.713G  |
|   encoder.neural_augmentor                |   6                    |           |
|    encoder.neural_augmentor.brightness    |    2                   |           |
|    encoder.neural_augmentor.contrast      |    2                   |           |
|    encoder.neural_augmentor.noise         |    2                   |           |
|   encoder.patch_emb                       |   0.748M               |   1.046G  |
|    encoder.patch_emb.0.block              |    9.6K                |    0.12G  |
|    encoder.patch_emb.1.block              |    0.148M              |    0.464G |
|    encoder.patch_emb.2.block.conv         |    0.591M              |    0.462G |
|   encoder.post_transformer_norm           |   1.536K               |   3.011M  |
|    encoder.post_transformer_norm.weight   |    (768,)              |           |
|    encoder.post_transformer_norm.bias     |    (768,)              |           |
|   encoder.transformer                     |   85.054M              |   66.661G |
|    encoder.transformer.0                  |    7.088M              |    5.555G |
|    encoder.transformer.1                  |    7.088M              |    5.555G |
|    encoder.transformer.2                  |    7.088M              |    5.555G |
|    encoder.transformer.3                  |    7.088M              |    5.555G |
|    encoder.transformer.4                  |    7.088M              |    5.555G |
|    encoder.transformer.5                  |    7.088M              |    5.555G |
|    encoder.transformer.6                  |    7.088M              |    5.555G |
|    encoder.transformer.7                  |    7.088M              |    5.555G |
|    encoder.transformer.8                  |    7.088M              |    5.555G |
|    encoder.transformer.9                  |    7.088M              |    5.555G |
|    encoder.transformer.10                 |    7.088M              |    5.555G |
|    encoder.transformer.11                 |    7.088M              |    5.555G |
|   encoder.pos_embed.pos_embed             |   0.151M               |   2.408M  |
|    encoder.pos_embed.pos_embed.pos_embed  |    (1, 1, 196, 768)    |           |
|  seg_head                                 |  12.773M               |  9.728G   |
|   seg_head.aspp.aspp_layer                |   12.72M               |   9.666G  |
|    seg_head.aspp.aspp_layer.convs         |    11.408M             |    8.638G |
|    seg_head.aspp.aspp_layer.project.block |    1.312M              |    1.028G |
|   seg_head.classifier.block.conv          |   52.839K              |   41.345M |
|    seg_head.classifier.block.conv.weight  |    (103, 512, 1, 1)    |           |
|    seg_head.classifier.block.conv.bias    |    (103,)              |           |
|   seg_head.upsample_seg_out               |                        |   20.673M |
2024-07-27 16:23:30 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-27 16:23:30 - [33m[1mWARNING[0m - Uncalled Modules:
{'encoder.neural_augmentor', 'encoder.transformer.7.drop_path', 'encoder.neural_augmentor.brightness', 'encoder.transformer.10.drop_path', 'encoder.transformer.8.drop_path', 'encoder.transformer.4.drop_path', 'encoder.transformer.1.drop_path', 'encoder.transformer.9.drop_path', 'encoder.neural_augmentor.contrast.max_fn', 'encoder.transformer.5.drop_path', 'encoder.neural_augmentor.brightness.min_fn', 'encoder.transformer.3.drop_path', 'encoder.neural_augmentor.contrast', 'encoder.transformer.11.drop_path', 'encoder.neural_augmentor.noise.min_fn', 'encoder.neural_augmentor.contrast.min_fn', 'encoder.transformer.2.drop_path', 'encoder.transformer.0.drop_path', 'encoder.neural_augmentor.noise.max_fn', 'encoder.transformer.6.drop_path', 'encoder.neural_augmentor.brightness.max_fn', 'encoder.neural_augmentor.noise'}
2024-07-27 16:23:30 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 25, 'aten::gelu': 20, 'aten::scaled_dot_product_attention': 12, 'aten::feature_dropout': 1})
[31m=================================================================[0m
2024-07-27 16:23:31 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-27 16:23:31 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-27 16:23:31 - [34m[1mLOGS   [0m - Available GPUs: 4
2024-07-27 16:23:31 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-27 16:23:31 - [34m[1mLOGS   [0m - Setting --ddp.world-size the same as the number of available gpus.
2024-07-27 16:23:31 - [34m[1mLOGS   [0m - Directory exists at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train
2024-07-27 16:23:35 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:40010
2024-07-27 16:23:38 - [34m[1mLOGS   [0m - Training dataset details are given below
FoodsegDataset(
	root=/ML-A100/team/mm/models/UECFOODPIXCOMPLETE/data 
	is_training=True 
	num_samples=9000
	transforms=Compose(
			RandomShortSizeResize(short_side_min=256, short_side_max=768, interpolation=bicubic), 
			RandomHorizontalFlip(p=0.5), 
			RandomCrop(size=(h=512, w=512), seg_class_max_ratio=0.75, seg_fill=0), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
)
2024-07-27 16:23:38 - [34m[1mLOGS   [0m - Validation dataset details are given below
FoodsegDataset(
	root=/ML-A100/team/mm/models/UECFOODPIXCOMPLETE/data 
	is_training=False 
	num_samples=1000
	transforms=Compose(
			Resize(size=[512, 512], interpolation=bicubic, maintain_aspect_ratio=False), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
)
2024-07-27 16:23:38 - [34m[1mLOGS   [0m - Training sampler details: BatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	base_im_size=(h=512, w=512)
	base_batch_size=32
)
2024-07-27 16:23:38 - [34m[1mLOGS   [0m - Validation sampler details: BatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	base_im_size=(h=512, w=512)
	base_batch_size=8
)
2024-07-27 16:23:38 - [34m[1mLOGS   [0m - Number of data workers: 64
2024-07-27 16:23:40 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/vit_base.pt
2024-07-27 16:23:40 - [32m[1mINFO   [0m - Trainable parameters: ['neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_emb.0.block.conv.weight', 'patch_emb.0.block.norm.weight', 'patch_emb.0.block.norm.bias', 'patch_emb.1.block.conv.weight', 'patch_emb.1.block.norm.weight', 'patch_emb.1.block.norm.bias', 'patch_emb.2.block.conv.weight', 'patch_emb.2.block.conv.bias', 'post_transformer_norm.weight', 'post_transformer_norm.bias', 'transformer.0.pre_norm_mha.0.weight', 'transformer.0.pre_norm_mha.0.bias', 'transformer.0.pre_norm_mha.1.qkv_proj.weight', 'transformer.0.pre_norm_mha.1.qkv_proj.bias', 'transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'transformer.0.pre_norm_ffn.0.weight', 'transformer.0.pre_norm_ffn.0.bias', 'transformer.0.pre_norm_ffn.1.weight', 'transformer.0.pre_norm_ffn.1.bias', 'transformer.0.pre_norm_ffn.4.weight', 'transformer.0.pre_norm_ffn.4.bias', 'transformer.1.pre_norm_mha.0.weight', 'transformer.1.pre_norm_mha.0.bias', 'transformer.1.pre_norm_mha.1.qkv_proj.weight', 'transformer.1.pre_norm_mha.1.qkv_proj.bias', 'transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'transformer.1.pre_norm_ffn.0.weight', 'transformer.1.pre_norm_ffn.0.bias', 'transformer.1.pre_norm_ffn.1.weight', 'transformer.1.pre_norm_ffn.1.bias', 'transformer.1.pre_norm_ffn.4.weight', 'transformer.1.pre_norm_ffn.4.bias', 'transformer.2.pre_norm_mha.0.weight', 'transformer.2.pre_norm_mha.0.bias', 'transformer.2.pre_norm_mha.1.qkv_proj.weight', 'transformer.2.pre_norm_mha.1.qkv_proj.bias', 'transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'transformer.2.pre_norm_ffn.0.weight', 'transformer.2.pre_norm_ffn.0.bias', 'transformer.2.pre_norm_ffn.1.weight', 'transformer.2.pre_norm_ffn.1.bias', 'transformer.2.pre_norm_ffn.4.weight', 'transformer.2.pre_norm_ffn.4.bias', 'transformer.3.pre_norm_mha.0.weight', 'transformer.3.pre_norm_mha.0.bias', 'transformer.3.pre_norm_mha.1.qkv_proj.weight', 'transformer.3.pre_norm_mha.1.qkv_proj.bias', 'transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'transformer.3.pre_norm_ffn.0.weight', 'transformer.3.pre_norm_ffn.0.bias', 'transformer.3.pre_norm_ffn.1.weight', 'transformer.3.pre_norm_ffn.1.bias', 'transformer.3.pre_norm_ffn.4.weight', 'transformer.3.pre_norm_ffn.4.bias', 'transformer.4.pre_norm_mha.0.weight', 'transformer.4.pre_norm_mha.0.bias', 'transformer.4.pre_norm_mha.1.qkv_proj.weight', 'transformer.4.pre_norm_mha.1.qkv_proj.bias', 'transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'transformer.4.pre_norm_ffn.0.weight', 'transformer.4.pre_norm_ffn.0.bias', 'transformer.4.pre_norm_ffn.1.weight', 'transformer.4.pre_norm_ffn.1.bias', 'transformer.4.pre_norm_ffn.4.weight', 'transformer.4.pre_norm_ffn.4.bias', 'transformer.5.pre_norm_mha.0.weight', 'transformer.5.pre_norm_mha.0.bias', 'transformer.5.pre_norm_mha.1.qkv_proj.weight', 'transformer.5.pre_norm_mha.1.qkv_proj.bias', 'transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'transformer.5.pre_norm_ffn.0.weight', 'transformer.5.pre_norm_ffn.0.bias', 'transformer.5.pre_norm_ffn.1.weight', 'transformer.5.pre_norm_ffn.1.bias', 'transformer.5.pre_norm_ffn.4.weight', 'transformer.5.pre_norm_ffn.4.bias', 'transformer.6.pre_norm_mha.0.weight', 'transformer.6.pre_norm_mha.0.bias', 'transformer.6.pre_norm_mha.1.qkv_proj.weight', 'transformer.6.pre_norm_mha.1.qkv_proj.bias', 'transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'transformer.6.pre_norm_ffn.0.weight', 'transformer.6.pre_norm_ffn.0.bias', 'transformer.6.pre_norm_ffn.1.weight', 'transformer.6.pre_norm_ffn.1.bias', 'transformer.6.pre_norm_ffn.4.weight', 'transformer.6.pre_norm_ffn.4.bias', 'transformer.7.pre_norm_mha.0.weight', 'transformer.7.pre_norm_mha.0.bias', 'transformer.7.pre_norm_mha.1.qkv_proj.weight', 'transformer.7.pre_norm_mha.1.qkv_proj.bias', 'transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'transformer.7.pre_norm_ffn.0.weight', 'transformer.7.pre_norm_ffn.0.bias', 'transformer.7.pre_norm_ffn.1.weight', 'transformer.7.pre_norm_ffn.1.bias', 'transformer.7.pre_norm_ffn.4.weight', 'transformer.7.pre_norm_ffn.4.bias', 'transformer.8.pre_norm_mha.0.weight', 'transformer.8.pre_norm_mha.0.bias', 'transformer.8.pre_norm_mha.1.qkv_proj.weight', 'transformer.8.pre_norm_mha.1.qkv_proj.bias', 'transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'transformer.8.pre_norm_ffn.0.weight', 'transformer.8.pre_norm_ffn.0.bias', 'transformer.8.pre_norm_ffn.1.weight', 'transformer.8.pre_norm_ffn.1.bias', 'transformer.8.pre_norm_ffn.4.weight', 'transformer.8.pre_norm_ffn.4.bias', 'transformer.9.pre_norm_mha.0.weight', 'transformer.9.pre_norm_mha.0.bias', 'transformer.9.pre_norm_mha.1.qkv_proj.weight', 'transformer.9.pre_norm_mha.1.qkv_proj.bias', 'transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'transformer.9.pre_norm_ffn.0.weight', 'transformer.9.pre_norm_ffn.0.bias', 'transformer.9.pre_norm_ffn.1.weight', 'transformer.9.pre_norm_ffn.1.bias', 'transformer.9.pre_norm_ffn.4.weight', 'transformer.9.pre_norm_ffn.4.bias', 'transformer.10.pre_norm_mha.0.weight', 'transformer.10.pre_norm_mha.0.bias', 'transformer.10.pre_norm_mha.1.qkv_proj.weight', 'transformer.10.pre_norm_mha.1.qkv_proj.bias', 'transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'transformer.10.pre_norm_ffn.0.weight', 'transformer.10.pre_norm_ffn.0.bias', 'transformer.10.pre_norm_ffn.1.weight', 'transformer.10.pre_norm_ffn.1.bias', 'transformer.10.pre_norm_ffn.4.weight', 'transformer.10.pre_norm_ffn.4.bias', 'transformer.11.pre_norm_mha.0.weight', 'transformer.11.pre_norm_mha.0.bias', 'transformer.11.pre_norm_mha.1.qkv_proj.weight', 'transformer.11.pre_norm_mha.1.qkv_proj.bias', 'transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'transformer.11.pre_norm_ffn.0.weight', 'transformer.11.pre_norm_ffn.0.bias', 'transformer.11.pre_norm_ffn.1.weight', 'transformer.11.pre_norm_ffn.1.bias', 'transformer.11.pre_norm_ffn.4.weight', 'transformer.11.pre_norm_ffn.4.bias', 'classifier.weight', 'classifier.bias', 'pos_embed.pos_embed.pos_embed']
2024-07-27 16:23:40 - [32m[1mINFO   [0m - Trainable parameters: ['aspp.aspp_layer.convs.0.block.conv.weight', 'aspp.aspp_layer.convs.0.block.norm.weight', 'aspp.aspp_layer.convs.0.block.norm.bias', 'aspp.aspp_layer.convs.1.block.conv.weight', 'aspp.aspp_layer.convs.1.block.norm.weight', 'aspp.aspp_layer.convs.1.block.norm.bias', 'aspp.aspp_layer.convs.2.block.conv.weight', 'aspp.aspp_layer.convs.2.block.norm.weight', 'aspp.aspp_layer.convs.2.block.norm.bias', 'aspp.aspp_layer.convs.3.block.conv.weight', 'aspp.aspp_layer.convs.3.block.norm.weight', 'aspp.aspp_layer.convs.3.block.norm.bias', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.conv.weight', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.weight', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.bias', 'aspp.aspp_layer.project.block.conv.weight', 'aspp.aspp_layer.project.block.norm.weight', 'aspp.aspp_layer.project.block.norm.bias', 'classifier.block.conv.weight', 'classifier.block.conv.bias']
2024-07-27 16:23:40 - [32m[1mINFO   [0m - Trainable parameters: ['encoder.neural_augmentor.brightness._low', 'encoder.neural_augmentor.brightness._high', 'encoder.neural_augmentor.contrast._low', 'encoder.neural_augmentor.contrast._high', 'encoder.neural_augmentor.noise._low', 'encoder.neural_augmentor.noise._high', 'encoder.patch_emb.0.block.conv.weight', 'encoder.patch_emb.0.block.norm.weight', 'encoder.patch_emb.0.block.norm.bias', 'encoder.patch_emb.1.block.conv.weight', 'encoder.patch_emb.1.block.norm.weight', 'encoder.patch_emb.1.block.norm.bias', 'encoder.patch_emb.2.block.conv.weight', 'encoder.patch_emb.2.block.conv.bias', 'encoder.post_transformer_norm.weight', 'encoder.post_transformer_norm.bias', 'encoder.transformer.0.pre_norm_mha.0.weight', 'encoder.transformer.0.pre_norm_mha.0.bias', 'encoder.transformer.0.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.0.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.0.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.0.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.0.pre_norm_ffn.0.weight', 'encoder.transformer.0.pre_norm_ffn.0.bias', 'encoder.transformer.0.pre_norm_ffn.1.weight', 'encoder.transformer.0.pre_norm_ffn.1.bias', 'encoder.transformer.0.pre_norm_ffn.4.weight', 'encoder.transformer.0.pre_norm_ffn.4.bias', 'encoder.transformer.1.pre_norm_mha.0.weight', 'encoder.transformer.1.pre_norm_mha.0.bias', 'encoder.transformer.1.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.1.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.1.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.1.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.1.pre_norm_ffn.0.weight', 'encoder.transformer.1.pre_norm_ffn.0.bias', 'encoder.transformer.1.pre_norm_ffn.1.weight', 'encoder.transformer.1.pre_norm_ffn.1.bias', 'encoder.transformer.1.pre_norm_ffn.4.weight', 'encoder.transformer.1.pre_norm_ffn.4.bias', 'encoder.transformer.2.pre_norm_mha.0.weight', 'encoder.transformer.2.pre_norm_mha.0.bias', 'encoder.transformer.2.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.2.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.2.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.2.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.2.pre_norm_ffn.0.weight', 'encoder.transformer.2.pre_norm_ffn.0.bias', 'encoder.transformer.2.pre_norm_ffn.1.weight', 'encoder.transformer.2.pre_norm_ffn.1.bias', 'encoder.transformer.2.pre_norm_ffn.4.weight', 'encoder.transformer.2.pre_norm_ffn.4.bias', 'encoder.transformer.3.pre_norm_mha.0.weight', 'encoder.transformer.3.pre_norm_mha.0.bias', 'encoder.transformer.3.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.3.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.3.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.3.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.3.pre_norm_ffn.0.weight', 'encoder.transformer.3.pre_norm_ffn.0.bias', 'encoder.transformer.3.pre_norm_ffn.1.weight', 'encoder.transformer.3.pre_norm_ffn.1.bias', 'encoder.transformer.3.pre_norm_ffn.4.weight', 'encoder.transformer.3.pre_norm_ffn.4.bias', 'encoder.transformer.4.pre_norm_mha.0.weight', 'encoder.transformer.4.pre_norm_mha.0.bias', 'encoder.transformer.4.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.4.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.4.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.4.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.4.pre_norm_ffn.0.weight', 'encoder.transformer.4.pre_norm_ffn.0.bias', 'encoder.transformer.4.pre_norm_ffn.1.weight', 'encoder.transformer.4.pre_norm_ffn.1.bias', 'encoder.transformer.4.pre_norm_ffn.4.weight', 'encoder.transformer.4.pre_norm_ffn.4.bias', 'encoder.transformer.5.pre_norm_mha.0.weight', 'encoder.transformer.5.pre_norm_mha.0.bias', 'encoder.transformer.5.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.5.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.5.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.5.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.5.pre_norm_ffn.0.weight', 'encoder.transformer.5.pre_norm_ffn.0.bias', 'encoder.transformer.5.pre_norm_ffn.1.weight', 'encoder.transformer.5.pre_norm_ffn.1.bias', 'encoder.transformer.5.pre_norm_ffn.4.weight', 'encoder.transformer.5.pre_norm_ffn.4.bias', 'encoder.transformer.6.pre_norm_mha.0.weight', 'encoder.transformer.6.pre_norm_mha.0.bias', 'encoder.transformer.6.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.6.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.6.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.6.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.6.pre_norm_ffn.0.weight', 'encoder.transformer.6.pre_norm_ffn.0.bias', 'encoder.transformer.6.pre_norm_ffn.1.weight', 'encoder.transformer.6.pre_norm_ffn.1.bias', 'encoder.transformer.6.pre_norm_ffn.4.weight', 'encoder.transformer.6.pre_norm_ffn.4.bias', 'encoder.transformer.7.pre_norm_mha.0.weight', 'encoder.transformer.7.pre_norm_mha.0.bias', 'encoder.transformer.7.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.7.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.7.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.7.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.7.pre_norm_ffn.0.weight', 'encoder.transformer.7.pre_norm_ffn.0.bias', 'encoder.transformer.7.pre_norm_ffn.1.weight', 'encoder.transformer.7.pre_norm_ffn.1.bias', 'encoder.transformer.7.pre_norm_ffn.4.weight', 'encoder.transformer.7.pre_norm_ffn.4.bias', 'encoder.transformer.8.pre_norm_mha.0.weight', 'encoder.transformer.8.pre_norm_mha.0.bias', 'encoder.transformer.8.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.8.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.8.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.8.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.8.pre_norm_ffn.0.weight', 'encoder.transformer.8.pre_norm_ffn.0.bias', 'encoder.transformer.8.pre_norm_ffn.1.weight', 'encoder.transformer.8.pre_norm_ffn.1.bias', 'encoder.transformer.8.pre_norm_ffn.4.weight', 'encoder.transformer.8.pre_norm_ffn.4.bias', 'encoder.transformer.9.pre_norm_mha.0.weight', 'encoder.transformer.9.pre_norm_mha.0.bias', 'encoder.transformer.9.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.9.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.9.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.9.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.9.pre_norm_ffn.0.weight', 'encoder.transformer.9.pre_norm_ffn.0.bias', 'encoder.transformer.9.pre_norm_ffn.1.weight', 'encoder.transformer.9.pre_norm_ffn.1.bias', 'encoder.transformer.9.pre_norm_ffn.4.weight', 'encoder.transformer.9.pre_norm_ffn.4.bias', 'encoder.transformer.10.pre_norm_mha.0.weight', 'encoder.transformer.10.pre_norm_mha.0.bias', 'encoder.transformer.10.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.10.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.10.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.10.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.10.pre_norm_ffn.0.weight', 'encoder.transformer.10.pre_norm_ffn.0.bias', 'encoder.transformer.10.pre_norm_ffn.1.weight', 'encoder.transformer.10.pre_norm_ffn.1.bias', 'encoder.transformer.10.pre_norm_ffn.4.weight', 'encoder.transformer.10.pre_norm_ffn.4.bias', 'encoder.transformer.11.pre_norm_mha.0.weight', 'encoder.transformer.11.pre_norm_mha.0.bias', 'encoder.transformer.11.pre_norm_mha.1.qkv_proj.weight', 'encoder.transformer.11.pre_norm_mha.1.qkv_proj.bias', 'encoder.transformer.11.pre_norm_mha.1.out_proj_attn.weight', 'encoder.transformer.11.pre_norm_mha.1.out_proj_attn.bias', 'encoder.transformer.11.pre_norm_ffn.0.weight', 'encoder.transformer.11.pre_norm_ffn.0.bias', 'encoder.transformer.11.pre_norm_ffn.1.weight', 'encoder.transformer.11.pre_norm_ffn.1.bias', 'encoder.transformer.11.pre_norm_ffn.4.weight', 'encoder.transformer.11.pre_norm_ffn.4.bias', 'encoder.pos_embed.pos_embed.pos_embed', 'seg_head.aspp.aspp_layer.convs.0.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.0.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.0.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.1.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.1.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.1.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.2.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.2.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.2.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.3.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.3.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.3.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.bias', 'seg_head.aspp.aspp_layer.project.block.conv.weight', 'seg_head.aspp.aspp_layer.project.block.norm.weight', 'seg_head.aspp.aspp_layer.project.block.norm.bias', 'seg_head.classifier.block.conv.weight', 'seg_head.classifier.block.conv.bias']
2024-07-27 16:23:40 - [34m[1mLOGS   [0m - [36mModel[0m
SegEncoderDecoder(
  (encoder): VisionTransformer(
    (neural_augmentor): DistributionNeuralAugmentor(
    	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
    	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
    	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
    (patch_emb): Sequential(
      (0): Conv2d(3, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, normalization=BatchNorm2d, activation=GELU)
      (1): Conv2d(192, 192, kernel_size=(2, 2), stride=(2, 2), bias=False, normalization=BatchNorm2d, activation=GELU)
      (2): Conv2d(192, 768, kernel_size=(2, 2), stride=(2, 2))
    )
    (post_transformer_norm): LayerNormFP32((768,), eps=1e-06, elementwise_affine=True)
    (transformer): Sequential(
      (0): FlashTransformerEncoder
      (1): FlashTransformerEncoder
      (2): FlashTransformerEncoder
      (3): FlashTransformerEncoder
      (4): FlashTransformerEncoder
      (5): FlashTransformerEncoder
      (6): FlashTransformerEncoder
      (7): FlashTransformerEncoder
      (8): FlashTransformerEncoder
      (9): FlashTransformerEncoder
      (10): FlashTransformerEncoder
      (11): FlashTransformerEncoder
    )
    (classifier): None
    (pos_embed): LearnablePositionalEmbedding(num_embeddings=196, embedding_dim=768, padding_idx=None, sequence_first=False)
    (emb_dropout): Dropout(p=0.0, inplace=False)
  )
  (seg_head): DeeplabV3(
    (upsample_seg_out): UpSample(scale_factor=8.0, mode='bilinear')
    (aspp): Sequential(
      (aspp_layer): ASPP(in_channels=768, out_channels=512, atrous_rates=[12, 24, 36], is_aspp_sep=False, dropout=0.1)
    )
    (classifier): Conv2d(512, 103, kernel_size=(1, 1), stride=(1, 1))
  )
)
[31m=================================================================[0m
                  SegEncoderDecoder Summary
[31m=================================================================[0m
Total parameters     =   98.728 M
Total trainable parameters =   98.728 M

2024-07-27 16:23:40 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-27 16:23:40 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                                    | #parameters or shape   | #flops    |
|:------------------------------------------|:-----------------------|:----------|
| model                                     | 98.728M                | 77.441G   |
|  encoder                                  |  85.955M               |  67.713G  |
|   encoder.neural_augmentor                |   6                    |           |
|    encoder.neural_augmentor.brightness    |    2                   |           |
|    encoder.neural_augmentor.contrast      |    2                   |           |
|    encoder.neural_augmentor.noise         |    2                   |           |
|   encoder.patch_emb                       |   0.748M               |   1.046G  |
|    encoder.patch_emb.0.block              |    9.6K                |    0.12G  |
|    encoder.patch_emb.1.block              |    0.148M              |    0.464G |
|    encoder.patch_emb.2.block.conv         |    0.591M              |    0.462G |
|   encoder.post_transformer_norm           |   1.536K               |   3.011M  |
|    encoder.post_transformer_norm.weight   |    (768,)              |           |
|    encoder.post_transformer_norm.bias     |    (768,)              |           |
|   encoder.transformer                     |   85.054M              |   66.661G |
|    encoder.transformer.0                  |    7.088M              |    5.555G |
|    encoder.transformer.1                  |    7.088M              |    5.555G |
|    encoder.transformer.2                  |    7.088M              |    5.555G |
|    encoder.transformer.3                  |    7.088M              |    5.555G |
|    encoder.transformer.4                  |    7.088M              |    5.555G |
|    encoder.transformer.5                  |    7.088M              |    5.555G |
|    encoder.transformer.6                  |    7.088M              |    5.555G |
|    encoder.transformer.7                  |    7.088M              |    5.555G |
|    encoder.transformer.8                  |    7.088M              |    5.555G |
|    encoder.transformer.9                  |    7.088M              |    5.555G |
|    encoder.transformer.10                 |    7.088M              |    5.555G |
|    encoder.transformer.11                 |    7.088M              |    5.555G |
|   encoder.pos_embed.pos_embed             |   0.151M               |   2.408M  |
|    encoder.pos_embed.pos_embed.pos_embed  |    (1, 1, 196, 768)    |           |
|  seg_head                                 |  12.773M               |  9.728G   |
|   seg_head.aspp.aspp_layer                |   12.72M               |   9.666G  |
|    seg_head.aspp.aspp_layer.convs         |    11.408M             |    8.638G |
|    seg_head.aspp.aspp_layer.project.block |    1.312M              |    1.028G |
|   seg_head.classifier.block.conv          |   52.839K              |   41.345M |
|    seg_head.classifier.block.conv.weight  |    (103, 512, 1, 1)    |           |
|    seg_head.classifier.block.conv.bias    |    (103,)              |           |
|   seg_head.upsample_seg_out               |                        |   20.673M |
2024-07-27 16:23:40 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-27 16:23:40 - [33m[1mWARNING[0m - Uncalled Modules:
{'encoder.neural_augmentor.brightness.max_fn', 'encoder.transformer.8.drop_path', 'encoder.neural_augmentor.contrast', 'encoder.transformer.3.drop_path', 'encoder.transformer.5.drop_path', 'encoder.neural_augmentor.contrast.max_fn', 'encoder.neural_augmentor.noise', 'encoder.transformer.9.drop_path', 'encoder.transformer.1.drop_path', 'encoder.transformer.4.drop_path', 'encoder.neural_augmentor.noise.min_fn', 'encoder.neural_augmentor.brightness', 'encoder.transformer.2.drop_path', 'encoder.neural_augmentor.contrast.min_fn', 'encoder.transformer.7.drop_path', 'encoder.neural_augmentor.noise.max_fn', 'encoder.transformer.10.drop_path', 'encoder.neural_augmentor.brightness.min_fn', 'encoder.transformer.6.drop_path', 'encoder.transformer.0.drop_path', 'encoder.transformer.11.drop_path', 'encoder.neural_augmentor'}
2024-07-27 16:23:40 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 25, 'aten::gelu': 20, 'aten::scaled_dot_product_attention': 12, 'aten::feature_dropout': 1})
[31m=================================================================[0m
2024-07-27 16:23:40 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-27 16:23:40 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	SegCrossEntropy(  ignore_idx=-1  class_weighting=False  label_smoothing=0.0  aux_weight=0.4 loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-27 16:23:40 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-07-27 16:23:40 - [34m[1mLOGS   [0m - Max. epochs for training: 50
2024-07-27 16:23:40 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=3e-06
 	 max_lr=3e-05
 	 period=50
 	 warmup_init_lr=1e-06
 	 warmup_iters=500
 )
2024-07-27 16:23:42 - [34m[1mLOGS   [0m - Loaded checkpoint from /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_last.pt
2024-07-27 16:23:42 - [34m[1mLOGS   [0m - Resuming training for epoch 37
2024-07-27 16:23:42 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-27 16:23:44 - [32m[1mINFO   [0m - Training epoch 37
2024-07-27 16:23:34 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:40010
2024-07-27 16:23:35 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:40010
2024-07-27 16:23:34 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:40010
2024-07-27 16:26:50 - [34m[1mLOGS   [0m - Epoch:  37 [    2630/10000000], loss: {'segmentation': 0.1202, 'neural_augmentation': 0.6202, 'total_loss': 0.7405}, LR: [7e-06, 7e-06, 7e-06, 7e-06], Avg. batch load time: 173.867, Elapsed time: 185.94
2024-07-27 16:33:58 - [34m[1mLOGS   [0m - *** Training summary for epoch 37
	 loss={'segmentation': 0.1251, 'neural_augmentation': 0.5965, 'total_loss': 0.7217}
2024-07-27 16:37:00 - [34m[1mLOGS   [0m - *** Validation summary for epoch 37
	 loss={'segmentation': 0.4394, 'neural_augmentation': 0.0, 'total_loss': 0.4394} || iou=69.9861
2024-07-27 16:37:01 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_last.pt
2024-07-27 16:37:02 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_last.pt
2024-07-27 16:37:08 - [34m[1mLOGS   [0m - Training checkpoint for epoch 37/iteration 2700 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_epoch_37_iter_2700.pt
2024-07-27 16:37:09 - [34m[1mLOGS   [0m - Model state for epoch 37/iteration 2700 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_epoch_37_iter_2700.pt
[31m===========================================================================[0m
2024-07-27 16:37:11 - [32m[1mINFO   [0m - Training epoch 38
2024-07-27 16:37:20 - [34m[1mLOGS   [0m - Epoch:  38 [    2701/10000000], loss: {'segmentation': 0.1232, 'neural_augmentation': 0.6154, 'total_loss': 0.7385}, LR: [7e-06, 7e-06, 7e-06, 7e-06], Avg. batch load time: 1.791, Elapsed time:  8.61
2024-07-27 16:44:26 - [34m[1mLOGS   [0m - *** Training summary for epoch 38
	 loss={'segmentation': 0.1252, 'neural_augmentation': 0.6149, 'total_loss': 0.7401}
2024-07-27 16:44:40 - [34m[1mLOGS   [0m - *** Validation summary for epoch 38
	 loss={'segmentation': 0.4173, 'neural_augmentation': 0.0, 'total_loss': 0.4173} || iou=71.8174
2024-07-27 16:44:41 - [34m[1mLOGS   [0m - Best checkpoint with score 71.82 saved at /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_best.pt
2024-07-27 16:44:41 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_score_68.6325.pt
2024-07-27 16:44:41 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_68.7792.pt', 'checkpoint_score_69.3142.pt', 'checkpoint_score_69.7809.pt', 'checkpoint_score_71.4278.pt', 'checkpoint_score_71.8174.pt']
2024-07-27 16:44:50 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_avg.pt
2024-07-27 16:44:50 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_last.pt
2024-07-27 16:44:51 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_last.pt
2024-07-27 16:44:52 - [34m[1mLOGS   [0m - Training checkpoint for epoch 38/iteration 2771 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_epoch_38_iter_2771.pt
2024-07-27 16:44:52 - [34m[1mLOGS   [0m - Model state for epoch 38/iteration 2771 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_epoch_38_iter_2771.pt
[31m===========================================================================[0m
2024-07-27 16:44:54 - [32m[1mINFO   [0m - Training epoch 39
2024-07-27 16:45:05 - [34m[1mLOGS   [0m - Epoch:  39 [    2772/10000000], loss: {'segmentation': 0.1344, 'neural_augmentation': 0.6554, 'total_loss': 0.7898}, LR: [6e-06, 6e-06, 6e-06, 6e-06], Avg. batch load time: 4.492, Elapsed time: 10.55
2024-07-27 16:52:12 - [34m[1mLOGS   [0m - *** Training summary for epoch 39
	 loss={'segmentation': 0.119, 'neural_augmentation': 0.6366, 'total_loss': 0.7556}
2024-07-27 16:52:26 - [34m[1mLOGS   [0m - *** Validation summary for epoch 39
	 loss={'segmentation': 0.4382, 'neural_augmentation': 0.0, 'total_loss': 0.4382} || iou=70.9193
2024-07-27 16:52:27 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_last.pt
2024-07-27 16:52:28 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_last.pt
2024-07-27 16:52:33 - [34m[1mLOGS   [0m - Training checkpoint for epoch 39/iteration 2842 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_epoch_39_iter_2842.pt
2024-07-27 16:52:33 - [34m[1mLOGS   [0m - Model state for epoch 39/iteration 2842 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_epoch_39_iter_2842.pt
[31m===========================================================================[0m
2024-07-27 16:52:35 - [32m[1mINFO   [0m - Training epoch 40
2024-07-27 16:52:45 - [34m[1mLOGS   [0m - Epoch:  40 [    2843/10000000], loss: {'segmentation': 0.1234, 'neural_augmentation': 0.6585, 'total_loss': 0.7819}, LR: [6e-06, 6e-06, 6e-06, 6e-06], Avg. batch load time: 3.214, Elapsed time:  9.24
2024-07-27 16:59:52 - [34m[1mLOGS   [0m - *** Training summary for epoch 40
	 loss={'segmentation': 0.1153, 'neural_augmentation': 0.6473, 'total_loss': 0.7626}
2024-07-27 17:00:06 - [34m[1mLOGS   [0m - *** Validation summary for epoch 40
	 loss={'segmentation': 0.4141, 'neural_augmentation': 0.0, 'total_loss': 0.4141} || iou=72.5812
2024-07-27 17:00:07 - [34m[1mLOGS   [0m - Best checkpoint with score 72.58 saved at /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_best.pt
2024-07-27 17:00:07 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_score_68.7792.pt
2024-07-27 17:00:07 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_69.3142.pt', 'checkpoint_score_69.7809.pt', 'checkpoint_score_71.4278.pt', 'checkpoint_score_71.8174.pt', 'checkpoint_score_72.5812.pt']
2024-07-27 17:00:14 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_avg.pt
2024-07-27 17:00:15 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_last.pt
2024-07-27 17:00:16 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_last.pt
2024-07-27 17:00:17 - [34m[1mLOGS   [0m - Training checkpoint for epoch 40/iteration 2913 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_epoch_40_iter_2913.pt
2024-07-27 17:00:17 - [34m[1mLOGS   [0m - Model state for epoch 40/iteration 2913 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_epoch_40_iter_2913.pt
[31m===========================================================================[0m
2024-07-27 17:00:19 - [32m[1mINFO   [0m - Training epoch 41
2024-07-27 17:00:29 - [34m[1mLOGS   [0m - Epoch:  41 [    2914/10000000], loss: {'segmentation': 0.0968, 'neural_augmentation': 0.6594, 'total_loss': 0.7561}, LR: [5e-06, 5e-06, 5e-06, 5e-06], Avg. batch load time: 3.788, Elapsed time:  9.82
2024-07-27 17:07:36 - [34m[1mLOGS   [0m - *** Training summary for epoch 41
	 loss={'segmentation': 0.1116, 'neural_augmentation': 0.6673, 'total_loss': 0.7789}
2024-07-27 17:07:49 - [34m[1mLOGS   [0m - *** Validation summary for epoch 41
	 loss={'segmentation': 0.4189, 'neural_augmentation': 0.0, 'total_loss': 0.4189} || iou=72.1437
2024-07-27 17:07:51 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_last.pt
2024-07-27 17:07:51 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_last.pt
2024-07-27 17:07:54 - [34m[1mLOGS   [0m - Training checkpoint for epoch 41/iteration 2984 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_epoch_41_iter_2984.pt
2024-07-27 17:07:56 - [34m[1mLOGS   [0m - Model state for epoch 41/iteration 2984 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_epoch_41_iter_2984.pt
[31m===========================================================================[0m
2024-07-27 17:07:58 - [32m[1mINFO   [0m - Training epoch 42
2024-07-27 17:08:10 - [34m[1mLOGS   [0m - Epoch:  42 [    2985/10000000], loss: {'segmentation': 0.1297, 'neural_augmentation': 0.6773, 'total_loss': 0.807}, LR: [5e-06, 5e-06, 5e-06, 5e-06], Avg. batch load time: 5.388, Elapsed time: 11.41
2024-07-27 17:15:17 - [34m[1mLOGS   [0m - *** Training summary for epoch 42
	 loss={'segmentation': 0.112, 'neural_augmentation': 0.6813, 'total_loss': 0.7933}
2024-07-27 17:15:30 - [34m[1mLOGS   [0m - *** Validation summary for epoch 42
	 loss={'segmentation': 0.4188, 'neural_augmentation': 0.0, 'total_loss': 0.4188} || iou=72.456
2024-07-27 17:15:32 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_last.pt
2024-07-27 17:15:32 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_last.pt
2024-07-27 17:15:34 - [34m[1mLOGS   [0m - Training checkpoint for epoch 42/iteration 3055 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_epoch_42_iter_3055.pt
2024-07-27 17:15:37 - [34m[1mLOGS   [0m - Model state for epoch 42/iteration 3055 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_epoch_42_iter_3055.pt
[31m===========================================================================[0m
2024-07-27 17:15:39 - [32m[1mINFO   [0m - Training epoch 43
2024-07-27 17:15:49 - [34m[1mLOGS   [0m - Epoch:  43 [    3056/10000000], loss: {'segmentation': 0.1004, 'neural_augmentation': 0.7205, 'total_loss': 0.8209}, LR: [4e-06, 4e-06, 4e-06, 4e-06], Avg. batch load time: 3.498, Elapsed time:  9.52
2024-07-27 17:22:55 - [34m[1mLOGS   [0m - *** Training summary for epoch 43
	 loss={'segmentation': 0.1104, 'neural_augmentation': 0.6936, 'total_loss': 0.804}
2024-07-27 17:23:09 - [34m[1mLOGS   [0m - *** Validation summary for epoch 43
	 loss={'segmentation': 0.4196, 'neural_augmentation': 0.0, 'total_loss': 0.4196} || iou=72.6823
2024-07-27 17:23:10 - [34m[1mLOGS   [0m - Best checkpoint with score 72.68 saved at /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_best.pt
2024-07-27 17:23:10 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_score_69.3142.pt
2024-07-27 17:23:10 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_69.7809.pt', 'checkpoint_score_71.4278.pt', 'checkpoint_score_71.8174.pt', 'checkpoint_score_72.5812.pt', 'checkpoint_score_72.6823.pt']
2024-07-27 17:23:18 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_avg.pt
2024-07-27 17:23:19 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_last.pt
2024-07-27 17:23:19 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_last.pt
2024-07-27 17:23:20 - [34m[1mLOGS   [0m - Training checkpoint for epoch 43/iteration 3126 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/training_checkpoint_epoch_43_iter_3126.pt
2024-07-27 17:23:20 - [34m[1mLOGS   [0m - Model state for epoch 43/iteration 3126 is saved at: /ML-A100/team/mm/models/catlip_data/catlip_vit_base/uec/train/checkpoint_epoch_43_iter_3126.pt
[31m===========================================================================[0m
2024-07-27 17:23:22 - [32m[1mINFO   [0m - Training epoch 44
2024-07-27 17:23:31 - [34m[1mLOGS   [0m - Epoch:  44 [    3127/10000000], loss: {'segmentation': 0.1038, 'neural_augmentation': 0.7124, 'total_loss': 0.8162}, LR: [4e-06, 4e-06, 4e-06, 4e-06], Avg. batch load time: 2.615, Elapsed time:  8.65
Terminated
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1608 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
