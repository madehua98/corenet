nohup: ignoring input
2024-07-28 07:55:24 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
2024-07-28 07:55:25 - [33m[1mWARNING[0m - UnrecognizedYamlConfigEntry("Yaml config key 'model.classification.openvit.mode' was not recognized by argparser. If you think that you have already added argument in corenet/options/opts.py file, then check for typos. If not, then please add it to corenet/options/opts.py."
2024-07-28 07:55:25 - [33m[1mWARNING[0m - UnrecognizedYamlConfigEntry("Yaml config key 'model.classification.openvit.norm_layer' was not recognized by argparser. If you think that you have already added argument in corenet/options/opts.py file, then check for typos. If not, then please add it to corenet/options/opts.py."
2024-07-28 07:55:25 - [33m[1mWARNING[0m - UnrecognizedYamlConfigEntry("Yaml config key 'model.classification.openvit.use_flash_attention' was not recognized by argparser. If you think that you have already added argument in corenet/options/opts.py file, then check for typos. If not, then please add it to corenet/options/opts.py."
102
2024-07-28 07:55:28 - [32m[1mINFO   [0m - Trainable parameters: ['neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'model.embeddings.class_embedding', 'model.embeddings.patch_embedding.weight', 'model.embeddings.position_embedding.weight', 'model.pre_layrnorm.weight', 'model.pre_layrnorm.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.layer_norm1.weight', 'model.encoder.layers.0.layer_norm1.bias', 'model.encoder.layers.0.mlp.fc1.weight', 'model.encoder.layers.0.mlp.fc1.bias', 'model.encoder.layers.0.mlp.fc2.weight', 'model.encoder.layers.0.mlp.fc2.bias', 'model.encoder.layers.0.layer_norm2.weight', 'model.encoder.layers.0.layer_norm2.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.1.layer_norm1.weight', 'model.encoder.layers.1.layer_norm1.bias', 'model.encoder.layers.1.mlp.fc1.weight', 'model.encoder.layers.1.mlp.fc1.bias', 'model.encoder.layers.1.mlp.fc2.weight', 'model.encoder.layers.1.mlp.fc2.bias', 'model.encoder.layers.1.layer_norm2.weight', 'model.encoder.layers.1.layer_norm2.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.layer_norm1.weight', 'model.encoder.layers.2.layer_norm1.bias', 'model.encoder.layers.2.mlp.fc1.weight', 'model.encoder.layers.2.mlp.fc1.bias', 'model.encoder.layers.2.mlp.fc2.weight', 'model.encoder.layers.2.mlp.fc2.bias', 'model.encoder.layers.2.layer_norm2.weight', 'model.encoder.layers.2.layer_norm2.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.3.layer_norm1.weight', 'model.encoder.layers.3.layer_norm1.bias', 'model.encoder.layers.3.mlp.fc1.weight', 'model.encoder.layers.3.mlp.fc1.bias', 'model.encoder.layers.3.mlp.fc2.weight', 'model.encoder.layers.3.mlp.fc2.bias', 'model.encoder.layers.3.layer_norm2.weight', 'model.encoder.layers.3.layer_norm2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.4.layer_norm1.weight', 'model.encoder.layers.4.layer_norm1.bias', 'model.encoder.layers.4.mlp.fc1.weight', 'model.encoder.layers.4.mlp.fc1.bias', 'model.encoder.layers.4.mlp.fc2.weight', 'model.encoder.layers.4.mlp.fc2.bias', 'model.encoder.layers.4.layer_norm2.weight', 'model.encoder.layers.4.layer_norm2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.layer_norm1.weight', 'model.encoder.layers.5.layer_norm1.bias', 'model.encoder.layers.5.mlp.fc1.weight', 'model.encoder.layers.5.mlp.fc1.bias', 'model.encoder.layers.5.mlp.fc2.weight', 'model.encoder.layers.5.mlp.fc2.bias', 'model.encoder.layers.5.layer_norm2.weight', 'model.encoder.layers.5.layer_norm2.bias', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.layer_norm1.weight', 'model.encoder.layers.6.layer_norm1.bias', 'model.encoder.layers.6.mlp.fc1.weight', 'model.encoder.layers.6.mlp.fc1.bias', 'model.encoder.layers.6.mlp.fc2.weight', 'model.encoder.layers.6.mlp.fc2.bias', 'model.encoder.layers.6.layer_norm2.weight', 'model.encoder.layers.6.layer_norm2.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.7.layer_norm1.weight', 'model.encoder.layers.7.layer_norm1.bias', 'model.encoder.layers.7.mlp.fc1.weight', 'model.encoder.layers.7.mlp.fc1.bias', 'model.encoder.layers.7.mlp.fc2.weight', 'model.encoder.layers.7.mlp.fc2.bias', 'model.encoder.layers.7.layer_norm2.weight', 'model.encoder.layers.7.layer_norm2.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.8.layer_norm1.weight', 'model.encoder.layers.8.layer_norm1.bias', 'model.encoder.layers.8.mlp.fc1.weight', 'model.encoder.layers.8.mlp.fc1.bias', 'model.encoder.layers.8.mlp.fc2.weight', 'model.encoder.layers.8.mlp.fc2.bias', 'model.encoder.layers.8.layer_norm2.weight', 'model.encoder.layers.8.layer_norm2.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.9.layer_norm1.weight', 'model.encoder.layers.9.layer_norm1.bias', 'model.encoder.layers.9.mlp.fc1.weight', 'model.encoder.layers.9.mlp.fc1.bias', 'model.encoder.layers.9.mlp.fc2.weight', 'model.encoder.layers.9.mlp.fc2.bias', 'model.encoder.layers.9.layer_norm2.weight', 'model.encoder.layers.9.layer_norm2.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.10.layer_norm1.weight', 'model.encoder.layers.10.layer_norm1.bias', 'model.encoder.layers.10.mlp.fc1.weight', 'model.encoder.layers.10.mlp.fc1.bias', 'model.encoder.layers.10.mlp.fc2.weight', 'model.encoder.layers.10.mlp.fc2.bias', 'model.encoder.layers.10.layer_norm2.weight', 'model.encoder.layers.10.layer_norm2.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.11.layer_norm1.weight', 'model.encoder.layers.11.layer_norm1.bias', 'model.encoder.layers.11.mlp.fc1.weight', 'model.encoder.layers.11.mlp.fc1.bias', 'model.encoder.layers.11.mlp.fc2.weight', 'model.encoder.layers.11.mlp.fc2.bias', 'model.encoder.layers.11.layer_norm2.weight', 'model.encoder.layers.11.layer_norm2.bias', 'model.post_layernorm.weight', 'model.post_layernorm.bias', 'classifier.weight', 'classifier.bias']
2024-07-28 07:55:28 - [32m[1mINFO   [0m - Trainable parameters: ['aspp.aspp_layer.convs.0.block.conv.weight', 'aspp.aspp_layer.convs.0.block.norm.weight', 'aspp.aspp_layer.convs.0.block.norm.bias', 'aspp.aspp_layer.convs.1.block.conv.weight', 'aspp.aspp_layer.convs.1.block.norm.weight', 'aspp.aspp_layer.convs.1.block.norm.bias', 'aspp.aspp_layer.convs.2.block.conv.weight', 'aspp.aspp_layer.convs.2.block.norm.weight', 'aspp.aspp_layer.convs.2.block.norm.bias', 'aspp.aspp_layer.convs.3.block.conv.weight', 'aspp.aspp_layer.convs.3.block.norm.weight', 'aspp.aspp_layer.convs.3.block.norm.bias', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.conv.weight', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.weight', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.bias', 'aspp.aspp_layer.project.block.conv.weight', 'aspp.aspp_layer.project.block.norm.weight', 'aspp.aspp_layer.project.block.norm.bias', 'classifier.block.conv.weight', 'classifier.block.conv.bias']
2024-07-28 07:55:28 - [32m[1mINFO   [0m - Trainable parameters: ['encoder.neural_augmentor.brightness._low', 'encoder.neural_augmentor.brightness._high', 'encoder.neural_augmentor.contrast._low', 'encoder.neural_augmentor.contrast._high', 'encoder.neural_augmentor.noise._low', 'encoder.neural_augmentor.noise._high', 'encoder.model.embeddings.class_embedding', 'encoder.model.embeddings.patch_embedding.weight', 'encoder.model.embeddings.position_embedding.weight', 'encoder.model.pre_layrnorm.weight', 'encoder.model.pre_layrnorm.bias', 'encoder.model.encoder.layers.0.self_attn.k_proj.weight', 'encoder.model.encoder.layers.0.self_attn.k_proj.bias', 'encoder.model.encoder.layers.0.self_attn.v_proj.weight', 'encoder.model.encoder.layers.0.self_attn.v_proj.bias', 'encoder.model.encoder.layers.0.self_attn.q_proj.weight', 'encoder.model.encoder.layers.0.self_attn.q_proj.bias', 'encoder.model.encoder.layers.0.self_attn.out_proj.weight', 'encoder.model.encoder.layers.0.self_attn.out_proj.bias', 'encoder.model.encoder.layers.0.layer_norm1.weight', 'encoder.model.encoder.layers.0.layer_norm1.bias', 'encoder.model.encoder.layers.0.mlp.fc1.weight', 'encoder.model.encoder.layers.0.mlp.fc1.bias', 'encoder.model.encoder.layers.0.mlp.fc2.weight', 'encoder.model.encoder.layers.0.mlp.fc2.bias', 'encoder.model.encoder.layers.0.layer_norm2.weight', 'encoder.model.encoder.layers.0.layer_norm2.bias', 'encoder.model.encoder.layers.1.self_attn.k_proj.weight', 'encoder.model.encoder.layers.1.self_attn.k_proj.bias', 'encoder.model.encoder.layers.1.self_attn.v_proj.weight', 'encoder.model.encoder.layers.1.self_attn.v_proj.bias', 'encoder.model.encoder.layers.1.self_attn.q_proj.weight', 'encoder.model.encoder.layers.1.self_attn.q_proj.bias', 'encoder.model.encoder.layers.1.self_attn.out_proj.weight', 'encoder.model.encoder.layers.1.self_attn.out_proj.bias', 'encoder.model.encoder.layers.1.layer_norm1.weight', 'encoder.model.encoder.layers.1.layer_norm1.bias', 'encoder.model.encoder.layers.1.mlp.fc1.weight', 'encoder.model.encoder.layers.1.mlp.fc1.bias', 'encoder.model.encoder.layers.1.mlp.fc2.weight', 'encoder.model.encoder.layers.1.mlp.fc2.bias', 'encoder.model.encoder.layers.1.layer_norm2.weight', 'encoder.model.encoder.layers.1.layer_norm2.bias', 'encoder.model.encoder.layers.2.self_attn.k_proj.weight', 'encoder.model.encoder.layers.2.self_attn.k_proj.bias', 'encoder.model.encoder.layers.2.self_attn.v_proj.weight', 'encoder.model.encoder.layers.2.self_attn.v_proj.bias', 'encoder.model.encoder.layers.2.self_attn.q_proj.weight', 'encoder.model.encoder.layers.2.self_attn.q_proj.bias', 'encoder.model.encoder.layers.2.self_attn.out_proj.weight', 'encoder.model.encoder.layers.2.self_attn.out_proj.bias', 'encoder.model.encoder.layers.2.layer_norm1.weight', 'encoder.model.encoder.layers.2.layer_norm1.bias', 'encoder.model.encoder.layers.2.mlp.fc1.weight', 'encoder.model.encoder.layers.2.mlp.fc1.bias', 'encoder.model.encoder.layers.2.mlp.fc2.weight', 'encoder.model.encoder.layers.2.mlp.fc2.bias', 'encoder.model.encoder.layers.2.layer_norm2.weight', 'encoder.model.encoder.layers.2.layer_norm2.bias', 'encoder.model.encoder.layers.3.self_attn.k_proj.weight', 'encoder.model.encoder.layers.3.self_attn.k_proj.bias', 'encoder.model.encoder.layers.3.self_attn.v_proj.weight', 'encoder.model.encoder.layers.3.self_attn.v_proj.bias', 'encoder.model.encoder.layers.3.self_attn.q_proj.weight', 'encoder.model.encoder.layers.3.self_attn.q_proj.bias', 'encoder.model.encoder.layers.3.self_attn.out_proj.weight', 'encoder.model.encoder.layers.3.self_attn.out_proj.bias', 'encoder.model.encoder.layers.3.layer_norm1.weight', 'encoder.model.encoder.layers.3.layer_norm1.bias', 'encoder.model.encoder.layers.3.mlp.fc1.weight', 'encoder.model.encoder.layers.3.mlp.fc1.bias', 'encoder.model.encoder.layers.3.mlp.fc2.weight', 'encoder.model.encoder.layers.3.mlp.fc2.bias', 'encoder.model.encoder.layers.3.layer_norm2.weight', 'encoder.model.encoder.layers.3.layer_norm2.bias', 'encoder.model.encoder.layers.4.self_attn.k_proj.weight', 'encoder.model.encoder.layers.4.self_attn.k_proj.bias', 'encoder.model.encoder.layers.4.self_attn.v_proj.weight', 'encoder.model.encoder.layers.4.self_attn.v_proj.bias', 'encoder.model.encoder.layers.4.self_attn.q_proj.weight', 'encoder.model.encoder.layers.4.self_attn.q_proj.bias', 'encoder.model.encoder.layers.4.self_attn.out_proj.weight', 'encoder.model.encoder.layers.4.self_attn.out_proj.bias', 'encoder.model.encoder.layers.4.layer_norm1.weight', 'encoder.model.encoder.layers.4.layer_norm1.bias', 'encoder.model.encoder.layers.4.mlp.fc1.weight', 'encoder.model.encoder.layers.4.mlp.fc1.bias', 'encoder.model.encoder.layers.4.mlp.fc2.weight', 'encoder.model.encoder.layers.4.mlp.fc2.bias', 'encoder.model.encoder.layers.4.layer_norm2.weight', 'encoder.model.encoder.layers.4.layer_norm2.bias', 'encoder.model.encoder.layers.5.self_attn.k_proj.weight', 'encoder.model.encoder.layers.5.self_attn.k_proj.bias', 'encoder.model.encoder.layers.5.self_attn.v_proj.weight', 'encoder.model.encoder.layers.5.self_attn.v_proj.bias', 'encoder.model.encoder.layers.5.self_attn.q_proj.weight', 'encoder.model.encoder.layers.5.self_attn.q_proj.bias', 'encoder.model.encoder.layers.5.self_attn.out_proj.weight', 'encoder.model.encoder.layers.5.self_attn.out_proj.bias', 'encoder.model.encoder.layers.5.layer_norm1.weight', 'encoder.model.encoder.layers.5.layer_norm1.bias', 'encoder.model.encoder.layers.5.mlp.fc1.weight', 'encoder.model.encoder.layers.5.mlp.fc1.bias', 'encoder.model.encoder.layers.5.mlp.fc2.weight', 'encoder.model.encoder.layers.5.mlp.fc2.bias', 'encoder.model.encoder.layers.5.layer_norm2.weight', 'encoder.model.encoder.layers.5.layer_norm2.bias', 'encoder.model.encoder.layers.6.self_attn.k_proj.weight', 'encoder.model.encoder.layers.6.self_attn.k_proj.bias', 'encoder.model.encoder.layers.6.self_attn.v_proj.weight', 'encoder.model.encoder.layers.6.self_attn.v_proj.bias', 'encoder.model.encoder.layers.6.self_attn.q_proj.weight', 'encoder.model.encoder.layers.6.self_attn.q_proj.bias', 'encoder.model.encoder.layers.6.self_attn.out_proj.weight', 'encoder.model.encoder.layers.6.self_attn.out_proj.bias', 'encoder.model.encoder.layers.6.layer_norm1.weight', 'encoder.model.encoder.layers.6.layer_norm1.bias', 'encoder.model.encoder.layers.6.mlp.fc1.weight', 'encoder.model.encoder.layers.6.mlp.fc1.bias', 'encoder.model.encoder.layers.6.mlp.fc2.weight', 'encoder.model.encoder.layers.6.mlp.fc2.bias', 'encoder.model.encoder.layers.6.layer_norm2.weight', 'encoder.model.encoder.layers.6.layer_norm2.bias', 'encoder.model.encoder.layers.7.self_attn.k_proj.weight', 'encoder.model.encoder.layers.7.self_attn.k_proj.bias', 'encoder.model.encoder.layers.7.self_attn.v_proj.weight', 'encoder.model.encoder.layers.7.self_attn.v_proj.bias', 'encoder.model.encoder.layers.7.self_attn.q_proj.weight', 'encoder.model.encoder.layers.7.self_attn.q_proj.bias', 'encoder.model.encoder.layers.7.self_attn.out_proj.weight', 'encoder.model.encoder.layers.7.self_attn.out_proj.bias', 'encoder.model.encoder.layers.7.layer_norm1.weight', 'encoder.model.encoder.layers.7.layer_norm1.bias', 'encoder.model.encoder.layers.7.mlp.fc1.weight', 'encoder.model.encoder.layers.7.mlp.fc1.bias', 'encoder.model.encoder.layers.7.mlp.fc2.weight', 'encoder.model.encoder.layers.7.mlp.fc2.bias', 'encoder.model.encoder.layers.7.layer_norm2.weight', 'encoder.model.encoder.layers.7.layer_norm2.bias', 'encoder.model.encoder.layers.8.self_attn.k_proj.weight', 'encoder.model.encoder.layers.8.self_attn.k_proj.bias', 'encoder.model.encoder.layers.8.self_attn.v_proj.weight', 'encoder.model.encoder.layers.8.self_attn.v_proj.bias', 'encoder.model.encoder.layers.8.self_attn.q_proj.weight', 'encoder.model.encoder.layers.8.self_attn.q_proj.bias', 'encoder.model.encoder.layers.8.self_attn.out_proj.weight', 'encoder.model.encoder.layers.8.self_attn.out_proj.bias', 'encoder.model.encoder.layers.8.layer_norm1.weight', 'encoder.model.encoder.layers.8.layer_norm1.bias', 'encoder.model.encoder.layers.8.mlp.fc1.weight', 'encoder.model.encoder.layers.8.mlp.fc1.bias', 'encoder.model.encoder.layers.8.mlp.fc2.weight', 'encoder.model.encoder.layers.8.mlp.fc2.bias', 'encoder.model.encoder.layers.8.layer_norm2.weight', 'encoder.model.encoder.layers.8.layer_norm2.bias', 'encoder.model.encoder.layers.9.self_attn.k_proj.weight', 'encoder.model.encoder.layers.9.self_attn.k_proj.bias', 'encoder.model.encoder.layers.9.self_attn.v_proj.weight', 'encoder.model.encoder.layers.9.self_attn.v_proj.bias', 'encoder.model.encoder.layers.9.self_attn.q_proj.weight', 'encoder.model.encoder.layers.9.self_attn.q_proj.bias', 'encoder.model.encoder.layers.9.self_attn.out_proj.weight', 'encoder.model.encoder.layers.9.self_attn.out_proj.bias', 'encoder.model.encoder.layers.9.layer_norm1.weight', 'encoder.model.encoder.layers.9.layer_norm1.bias', 'encoder.model.encoder.layers.9.mlp.fc1.weight', 'encoder.model.encoder.layers.9.mlp.fc1.bias', 'encoder.model.encoder.layers.9.mlp.fc2.weight', 'encoder.model.encoder.layers.9.mlp.fc2.bias', 'encoder.model.encoder.layers.9.layer_norm2.weight', 'encoder.model.encoder.layers.9.layer_norm2.bias', 'encoder.model.encoder.layers.10.self_attn.k_proj.weight', 'encoder.model.encoder.layers.10.self_attn.k_proj.bias', 'encoder.model.encoder.layers.10.self_attn.v_proj.weight', 'encoder.model.encoder.layers.10.self_attn.v_proj.bias', 'encoder.model.encoder.layers.10.self_attn.q_proj.weight', 'encoder.model.encoder.layers.10.self_attn.q_proj.bias', 'encoder.model.encoder.layers.10.self_attn.out_proj.weight', 'encoder.model.encoder.layers.10.self_attn.out_proj.bias', 'encoder.model.encoder.layers.10.layer_norm1.weight', 'encoder.model.encoder.layers.10.layer_norm1.bias', 'encoder.model.encoder.layers.10.mlp.fc1.weight', 'encoder.model.encoder.layers.10.mlp.fc1.bias', 'encoder.model.encoder.layers.10.mlp.fc2.weight', 'encoder.model.encoder.layers.10.mlp.fc2.bias', 'encoder.model.encoder.layers.10.layer_norm2.weight', 'encoder.model.encoder.layers.10.layer_norm2.bias', 'encoder.model.encoder.layers.11.self_attn.k_proj.weight', 'encoder.model.encoder.layers.11.self_attn.k_proj.bias', 'encoder.model.encoder.layers.11.self_attn.v_proj.weight', 'encoder.model.encoder.layers.11.self_attn.v_proj.bias', 'encoder.model.encoder.layers.11.self_attn.q_proj.weight', 'encoder.model.encoder.layers.11.self_attn.q_proj.bias', 'encoder.model.encoder.layers.11.self_attn.out_proj.weight', 'encoder.model.encoder.layers.11.self_attn.out_proj.bias', 'encoder.model.encoder.layers.11.layer_norm1.weight', 'encoder.model.encoder.layers.11.layer_norm1.bias', 'encoder.model.encoder.layers.11.mlp.fc1.weight', 'encoder.model.encoder.layers.11.mlp.fc1.bias', 'encoder.model.encoder.layers.11.mlp.fc2.weight', 'encoder.model.encoder.layers.11.mlp.fc2.bias', 'encoder.model.encoder.layers.11.layer_norm2.weight', 'encoder.model.encoder.layers.11.layer_norm2.bias', 'encoder.model.post_layernorm.weight', 'encoder.model.post_layernorm.bias', 'seg_head.aspp.aspp_layer.convs.0.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.0.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.0.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.1.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.1.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.1.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.2.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.2.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.2.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.3.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.3.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.3.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.bias', 'seg_head.aspp.aspp_layer.project.block.conv.weight', 'seg_head.aspp.aspp_layer.project.block.norm.weight', 'seg_head.aspp.aspp_layer.project.block.norm.bias', 'seg_head.classifier.block.conv.weight', 'seg_head.classifier.block.conv.bias']
2024-07-28 07:55:28 - [34m[1mLOGS   [0m - [36mModel[0m
SegEncoderDecoder(
  (encoder): OpenClipViT(
    (neural_augmentor): DistributionNeuralAugmentor(
    	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
    	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
    	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
    (model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (position_embedding): Embedding(197, 768)
      )
      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-11): 12 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (classifier): None
  )
  (seg_head): DeeplabV3(
    (upsample_seg_out): UpSample(scale_factor=8.0, mode='bilinear')
    (aspp): Sequential(
      (aspp_layer): ASPP(in_channels=768, out_channels=224, atrous_rates=[12, 24, 36], is_aspp_sep=False, dropout=0.1)
    )
    (classifier): Conv2d(224, 103, kernel_size=(1, 1), stride=(1, 1))
  )
)
[31m=================================================================[0m
                  SegEncoderDecoder Summary
[31m=================================================================[0m
Total parameters     =   91.065 M
Total trainable parameters =   91.065 M

2024-07-28 07:55:28 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-28 07:55:28 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                                    | #parameters or shape   | #flops     |
|:------------------------------------------|:-----------------------|:-----------|
| model                                     | 91.065M                | 18.586G    |
|  encoder                                  |  85.799M               |  17.582G   |
|   encoder.neural_augmentor                |   6                    |            |
|    encoder.neural_augmentor.brightness    |    2                   |            |
|    encoder.neural_augmentor.contrast      |    2                   |            |
|    encoder.neural_augmentor.noise         |    2                   |            |
|   encoder.model                           |   85.799M              |   17.582G  |
|    encoder.model.embeddings               |    0.742M              |    0.116G  |
|    encoder.model.pre_layrnorm             |    1.536K              |    0.756M  |
|    encoder.model.encoder.layers           |    85.054M             |    17.466G |
|    encoder.model.post_layernorm           |    1.536K              |            |
|  seg_head                                 |  5.266M                |  1.004G    |
|   seg_head.aspp.aspp_layer                |   5.242M               |   0.994G   |
|    seg_head.aspp.aspp_layer.convs         |    4.991M              |    0.945G  |
|    seg_head.aspp.aspp_layer.project.block |    0.251M              |    49.26M  |
|   seg_head.classifier.block.conv          |   23.175K              |   4.522M   |
|    seg_head.classifier.block.conv.weight  |    (103, 224, 1, 1)    |            |
|    seg_head.classifier.block.conv.bias    |    (103,)              |            |
|   seg_head.upsample_seg_out               |                        |   5.168M   |
2024-07-28 07:55:28 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-28 07:55:28 - [33m[1mWARNING[0m - Uncalled Modules:
{'encoder.neural_augmentor.noise.max_fn', 'encoder.neural_augmentor.contrast.min_fn', 'encoder.neural_augmentor.noise.min_fn', 'encoder.neural_augmentor.brightness', 'encoder.model.post_layernorm', 'encoder.neural_augmentor.contrast.max_fn', 'encoder.neural_augmentor', 'encoder.neural_augmentor.brightness.max_fn', 'encoder.neural_augmentor.brightness.min_fn', 'encoder.neural_augmentor.noise', 'encoder.neural_augmentor.contrast'}
2024-07-28 07:55:28 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::mul': 48, 'aten::add': 25, 'aten::softmax': 12, 'aten::sigmoid': 12, 'aten::gelu': 6, 'aten::embedding': 1, 'aten::sub': 1, 'aten::feature_dropout': 1})
[31m=================================================================[0m
2024-07-28 07:55:28 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-28 07:55:28 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-28 07:55:28 - [34m[1mLOGS   [0m - Available GPUs: 4
2024-07-28 07:55:28 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-28 07:55:29 - [34m[1mLOGS   [0m - Setting --ddp.world-size the same as the number of available gpus.
2024-07-28 07:55:29 - [34m[1mLOGS   [0m - Directory exists at: /ML-A100/team/mm/models/catlip_data/open_vit_base/uec_224/train
2024-07-28 07:55:33 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:40010
102
2024-07-28 07:55:33 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:40010
102
2024-07-28 07:55:33 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:40010
102
2024-07-28 07:55:33 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:40010
2024-07-28 07:55:36 - [34m[1mLOGS   [0m - Training dataset details are given below
FoodsegDataset(
	root=/ML-A100/team/mm/models/UECFOODPIXCOMPLETE/data 
	is_training=True 
	num_samples=9000
	transforms=Compose(
			Resize(size=[224, 224], interpolation=bicubic, maintain_aspect_ratio=False), 
			RandomHorizontalFlip(p=0.5), 
			RandomCrop(size=(h=224, w=224), seg_class_max_ratio=0.75, seg_fill=0), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
)
2024-07-28 07:55:36 - [34m[1mLOGS   [0m - Validation dataset details are given below
FoodsegDataset(
	root=/ML-A100/team/mm/models/UECFOODPIXCOMPLETE/data 
	is_training=False 
	num_samples=1000
	transforms=Compose(
			Resize(size=[224, 224], interpolation=bicubic, maintain_aspect_ratio=False), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
)
2024-07-28 07:55:36 - [34m[1mLOGS   [0m - Training sampler details: BatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	base_im_size=(h=224, w=224)
	base_batch_size=8
)
2024-07-28 07:55:36 - [34m[1mLOGS   [0m - Validation sampler details: BatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	base_im_size=(h=224, w=224)
	base_batch_size=4
)
2024-07-28 07:55:36 - [34m[1mLOGS   [0m - Number of data workers: 64
102
2024-07-28 07:55:39 - [32m[1mINFO   [0m - Trainable parameters: ['neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'model.embeddings.class_embedding', 'model.embeddings.patch_embedding.weight', 'model.embeddings.position_embedding.weight', 'model.pre_layrnorm.weight', 'model.pre_layrnorm.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.layer_norm1.weight', 'model.encoder.layers.0.layer_norm1.bias', 'model.encoder.layers.0.mlp.fc1.weight', 'model.encoder.layers.0.mlp.fc1.bias', 'model.encoder.layers.0.mlp.fc2.weight', 'model.encoder.layers.0.mlp.fc2.bias', 'model.encoder.layers.0.layer_norm2.weight', 'model.encoder.layers.0.layer_norm2.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.1.layer_norm1.weight', 'model.encoder.layers.1.layer_norm1.bias', 'model.encoder.layers.1.mlp.fc1.weight', 'model.encoder.layers.1.mlp.fc1.bias', 'model.encoder.layers.1.mlp.fc2.weight', 'model.encoder.layers.1.mlp.fc2.bias', 'model.encoder.layers.1.layer_norm2.weight', 'model.encoder.layers.1.layer_norm2.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.layer_norm1.weight', 'model.encoder.layers.2.layer_norm1.bias', 'model.encoder.layers.2.mlp.fc1.weight', 'model.encoder.layers.2.mlp.fc1.bias', 'model.encoder.layers.2.mlp.fc2.weight', 'model.encoder.layers.2.mlp.fc2.bias', 'model.encoder.layers.2.layer_norm2.weight', 'model.encoder.layers.2.layer_norm2.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.3.layer_norm1.weight', 'model.encoder.layers.3.layer_norm1.bias', 'model.encoder.layers.3.mlp.fc1.weight', 'model.encoder.layers.3.mlp.fc1.bias', 'model.encoder.layers.3.mlp.fc2.weight', 'model.encoder.layers.3.mlp.fc2.bias', 'model.encoder.layers.3.layer_norm2.weight', 'model.encoder.layers.3.layer_norm2.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.4.layer_norm1.weight', 'model.encoder.layers.4.layer_norm1.bias', 'model.encoder.layers.4.mlp.fc1.weight', 'model.encoder.layers.4.mlp.fc1.bias', 'model.encoder.layers.4.mlp.fc2.weight', 'model.encoder.layers.4.mlp.fc2.bias', 'model.encoder.layers.4.layer_norm2.weight', 'model.encoder.layers.4.layer_norm2.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.layer_norm1.weight', 'model.encoder.layers.5.layer_norm1.bias', 'model.encoder.layers.5.mlp.fc1.weight', 'model.encoder.layers.5.mlp.fc1.bias', 'model.encoder.layers.5.mlp.fc2.weight', 'model.encoder.layers.5.mlp.fc2.bias', 'model.encoder.layers.5.layer_norm2.weight', 'model.encoder.layers.5.layer_norm2.bias', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.layer_norm1.weight', 'model.encoder.layers.6.layer_norm1.bias', 'model.encoder.layers.6.mlp.fc1.weight', 'model.encoder.layers.6.mlp.fc1.bias', 'model.encoder.layers.6.mlp.fc2.weight', 'model.encoder.layers.6.mlp.fc2.bias', 'model.encoder.layers.6.layer_norm2.weight', 'model.encoder.layers.6.layer_norm2.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.7.layer_norm1.weight', 'model.encoder.layers.7.layer_norm1.bias', 'model.encoder.layers.7.mlp.fc1.weight', 'model.encoder.layers.7.mlp.fc1.bias', 'model.encoder.layers.7.mlp.fc2.weight', 'model.encoder.layers.7.mlp.fc2.bias', 'model.encoder.layers.7.layer_norm2.weight', 'model.encoder.layers.7.layer_norm2.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.8.layer_norm1.weight', 'model.encoder.layers.8.layer_norm1.bias', 'model.encoder.layers.8.mlp.fc1.weight', 'model.encoder.layers.8.mlp.fc1.bias', 'model.encoder.layers.8.mlp.fc2.weight', 'model.encoder.layers.8.mlp.fc2.bias', 'model.encoder.layers.8.layer_norm2.weight', 'model.encoder.layers.8.layer_norm2.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.9.layer_norm1.weight', 'model.encoder.layers.9.layer_norm1.bias', 'model.encoder.layers.9.mlp.fc1.weight', 'model.encoder.layers.9.mlp.fc1.bias', 'model.encoder.layers.9.mlp.fc2.weight', 'model.encoder.layers.9.mlp.fc2.bias', 'model.encoder.layers.9.layer_norm2.weight', 'model.encoder.layers.9.layer_norm2.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.10.layer_norm1.weight', 'model.encoder.layers.10.layer_norm1.bias', 'model.encoder.layers.10.mlp.fc1.weight', 'model.encoder.layers.10.mlp.fc1.bias', 'model.encoder.layers.10.mlp.fc2.weight', 'model.encoder.layers.10.mlp.fc2.bias', 'model.encoder.layers.10.layer_norm2.weight', 'model.encoder.layers.10.layer_norm2.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.11.layer_norm1.weight', 'model.encoder.layers.11.layer_norm1.bias', 'model.encoder.layers.11.mlp.fc1.weight', 'model.encoder.layers.11.mlp.fc1.bias', 'model.encoder.layers.11.mlp.fc2.weight', 'model.encoder.layers.11.mlp.fc2.bias', 'model.encoder.layers.11.layer_norm2.weight', 'model.encoder.layers.11.layer_norm2.bias', 'model.post_layernorm.weight', 'model.post_layernorm.bias', 'classifier.weight', 'classifier.bias']
2024-07-28 07:55:39 - [32m[1mINFO   [0m - Trainable parameters: ['aspp.aspp_layer.convs.0.block.conv.weight', 'aspp.aspp_layer.convs.0.block.norm.weight', 'aspp.aspp_layer.convs.0.block.norm.bias', 'aspp.aspp_layer.convs.1.block.conv.weight', 'aspp.aspp_layer.convs.1.block.norm.weight', 'aspp.aspp_layer.convs.1.block.norm.bias', 'aspp.aspp_layer.convs.2.block.conv.weight', 'aspp.aspp_layer.convs.2.block.norm.weight', 'aspp.aspp_layer.convs.2.block.norm.bias', 'aspp.aspp_layer.convs.3.block.conv.weight', 'aspp.aspp_layer.convs.3.block.norm.weight', 'aspp.aspp_layer.convs.3.block.norm.bias', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.conv.weight', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.weight', 'aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.bias', 'aspp.aspp_layer.project.block.conv.weight', 'aspp.aspp_layer.project.block.norm.weight', 'aspp.aspp_layer.project.block.norm.bias', 'classifier.block.conv.weight', 'classifier.block.conv.bias']
2024-07-28 07:55:39 - [32m[1mINFO   [0m - Trainable parameters: ['encoder.neural_augmentor.brightness._low', 'encoder.neural_augmentor.brightness._high', 'encoder.neural_augmentor.contrast._low', 'encoder.neural_augmentor.contrast._high', 'encoder.neural_augmentor.noise._low', 'encoder.neural_augmentor.noise._high', 'encoder.model.embeddings.class_embedding', 'encoder.model.embeddings.patch_embedding.weight', 'encoder.model.embeddings.position_embedding.weight', 'encoder.model.pre_layrnorm.weight', 'encoder.model.pre_layrnorm.bias', 'encoder.model.encoder.layers.0.self_attn.k_proj.weight', 'encoder.model.encoder.layers.0.self_attn.k_proj.bias', 'encoder.model.encoder.layers.0.self_attn.v_proj.weight', 'encoder.model.encoder.layers.0.self_attn.v_proj.bias', 'encoder.model.encoder.layers.0.self_attn.q_proj.weight', 'encoder.model.encoder.layers.0.self_attn.q_proj.bias', 'encoder.model.encoder.layers.0.self_attn.out_proj.weight', 'encoder.model.encoder.layers.0.self_attn.out_proj.bias', 'encoder.model.encoder.layers.0.layer_norm1.weight', 'encoder.model.encoder.layers.0.layer_norm1.bias', 'encoder.model.encoder.layers.0.mlp.fc1.weight', 'encoder.model.encoder.layers.0.mlp.fc1.bias', 'encoder.model.encoder.layers.0.mlp.fc2.weight', 'encoder.model.encoder.layers.0.mlp.fc2.bias', 'encoder.model.encoder.layers.0.layer_norm2.weight', 'encoder.model.encoder.layers.0.layer_norm2.bias', 'encoder.model.encoder.layers.1.self_attn.k_proj.weight', 'encoder.model.encoder.layers.1.self_attn.k_proj.bias', 'encoder.model.encoder.layers.1.self_attn.v_proj.weight', 'encoder.model.encoder.layers.1.self_attn.v_proj.bias', 'encoder.model.encoder.layers.1.self_attn.q_proj.weight', 'encoder.model.encoder.layers.1.self_attn.q_proj.bias', 'encoder.model.encoder.layers.1.self_attn.out_proj.weight', 'encoder.model.encoder.layers.1.self_attn.out_proj.bias', 'encoder.model.encoder.layers.1.layer_norm1.weight', 'encoder.model.encoder.layers.1.layer_norm1.bias', 'encoder.model.encoder.layers.1.mlp.fc1.weight', 'encoder.model.encoder.layers.1.mlp.fc1.bias', 'encoder.model.encoder.layers.1.mlp.fc2.weight', 'encoder.model.encoder.layers.1.mlp.fc2.bias', 'encoder.model.encoder.layers.1.layer_norm2.weight', 'encoder.model.encoder.layers.1.layer_norm2.bias', 'encoder.model.encoder.layers.2.self_attn.k_proj.weight', 'encoder.model.encoder.layers.2.self_attn.k_proj.bias', 'encoder.model.encoder.layers.2.self_attn.v_proj.weight', 'encoder.model.encoder.layers.2.self_attn.v_proj.bias', 'encoder.model.encoder.layers.2.self_attn.q_proj.weight', 'encoder.model.encoder.layers.2.self_attn.q_proj.bias', 'encoder.model.encoder.layers.2.self_attn.out_proj.weight', 'encoder.model.encoder.layers.2.self_attn.out_proj.bias', 'encoder.model.encoder.layers.2.layer_norm1.weight', 'encoder.model.encoder.layers.2.layer_norm1.bias', 'encoder.model.encoder.layers.2.mlp.fc1.weight', 'encoder.model.encoder.layers.2.mlp.fc1.bias', 'encoder.model.encoder.layers.2.mlp.fc2.weight', 'encoder.model.encoder.layers.2.mlp.fc2.bias', 'encoder.model.encoder.layers.2.layer_norm2.weight', 'encoder.model.encoder.layers.2.layer_norm2.bias', 'encoder.model.encoder.layers.3.self_attn.k_proj.weight', 'encoder.model.encoder.layers.3.self_attn.k_proj.bias', 'encoder.model.encoder.layers.3.self_attn.v_proj.weight', 'encoder.model.encoder.layers.3.self_attn.v_proj.bias', 'encoder.model.encoder.layers.3.self_attn.q_proj.weight', 'encoder.model.encoder.layers.3.self_attn.q_proj.bias', 'encoder.model.encoder.layers.3.self_attn.out_proj.weight', 'encoder.model.encoder.layers.3.self_attn.out_proj.bias', 'encoder.model.encoder.layers.3.layer_norm1.weight', 'encoder.model.encoder.layers.3.layer_norm1.bias', 'encoder.model.encoder.layers.3.mlp.fc1.weight', 'encoder.model.encoder.layers.3.mlp.fc1.bias', 'encoder.model.encoder.layers.3.mlp.fc2.weight', 'encoder.model.encoder.layers.3.mlp.fc2.bias', 'encoder.model.encoder.layers.3.layer_norm2.weight', 'encoder.model.encoder.layers.3.layer_norm2.bias', 'encoder.model.encoder.layers.4.self_attn.k_proj.weight', 'encoder.model.encoder.layers.4.self_attn.k_proj.bias', 'encoder.model.encoder.layers.4.self_attn.v_proj.weight', 'encoder.model.encoder.layers.4.self_attn.v_proj.bias', 'encoder.model.encoder.layers.4.self_attn.q_proj.weight', 'encoder.model.encoder.layers.4.self_attn.q_proj.bias', 'encoder.model.encoder.layers.4.self_attn.out_proj.weight', 'encoder.model.encoder.layers.4.self_attn.out_proj.bias', 'encoder.model.encoder.layers.4.layer_norm1.weight', 'encoder.model.encoder.layers.4.layer_norm1.bias', 'encoder.model.encoder.layers.4.mlp.fc1.weight', 'encoder.model.encoder.layers.4.mlp.fc1.bias', 'encoder.model.encoder.layers.4.mlp.fc2.weight', 'encoder.model.encoder.layers.4.mlp.fc2.bias', 'encoder.model.encoder.layers.4.layer_norm2.weight', 'encoder.model.encoder.layers.4.layer_norm2.bias', 'encoder.model.encoder.layers.5.self_attn.k_proj.weight', 'encoder.model.encoder.layers.5.self_attn.k_proj.bias', 'encoder.model.encoder.layers.5.self_attn.v_proj.weight', 'encoder.model.encoder.layers.5.self_attn.v_proj.bias', 'encoder.model.encoder.layers.5.self_attn.q_proj.weight', 'encoder.model.encoder.layers.5.self_attn.q_proj.bias', 'encoder.model.encoder.layers.5.self_attn.out_proj.weight', 'encoder.model.encoder.layers.5.self_attn.out_proj.bias', 'encoder.model.encoder.layers.5.layer_norm1.weight', 'encoder.model.encoder.layers.5.layer_norm1.bias', 'encoder.model.encoder.layers.5.mlp.fc1.weight', 'encoder.model.encoder.layers.5.mlp.fc1.bias', 'encoder.model.encoder.layers.5.mlp.fc2.weight', 'encoder.model.encoder.layers.5.mlp.fc2.bias', 'encoder.model.encoder.layers.5.layer_norm2.weight', 'encoder.model.encoder.layers.5.layer_norm2.bias', 'encoder.model.encoder.layers.6.self_attn.k_proj.weight', 'encoder.model.encoder.layers.6.self_attn.k_proj.bias', 'encoder.model.encoder.layers.6.self_attn.v_proj.weight', 'encoder.model.encoder.layers.6.self_attn.v_proj.bias', 'encoder.model.encoder.layers.6.self_attn.q_proj.weight', 'encoder.model.encoder.layers.6.self_attn.q_proj.bias', 'encoder.model.encoder.layers.6.self_attn.out_proj.weight', 'encoder.model.encoder.layers.6.self_attn.out_proj.bias', 'encoder.model.encoder.layers.6.layer_norm1.weight', 'encoder.model.encoder.layers.6.layer_norm1.bias', 'encoder.model.encoder.layers.6.mlp.fc1.weight', 'encoder.model.encoder.layers.6.mlp.fc1.bias', 'encoder.model.encoder.layers.6.mlp.fc2.weight', 'encoder.model.encoder.layers.6.mlp.fc2.bias', 'encoder.model.encoder.layers.6.layer_norm2.weight', 'encoder.model.encoder.layers.6.layer_norm2.bias', 'encoder.model.encoder.layers.7.self_attn.k_proj.weight', 'encoder.model.encoder.layers.7.self_attn.k_proj.bias', 'encoder.model.encoder.layers.7.self_attn.v_proj.weight', 'encoder.model.encoder.layers.7.self_attn.v_proj.bias', 'encoder.model.encoder.layers.7.self_attn.q_proj.weight', 'encoder.model.encoder.layers.7.self_attn.q_proj.bias', 'encoder.model.encoder.layers.7.self_attn.out_proj.weight', 'encoder.model.encoder.layers.7.self_attn.out_proj.bias', 'encoder.model.encoder.layers.7.layer_norm1.weight', 'encoder.model.encoder.layers.7.layer_norm1.bias', 'encoder.model.encoder.layers.7.mlp.fc1.weight', 'encoder.model.encoder.layers.7.mlp.fc1.bias', 'encoder.model.encoder.layers.7.mlp.fc2.weight', 'encoder.model.encoder.layers.7.mlp.fc2.bias', 'encoder.model.encoder.layers.7.layer_norm2.weight', 'encoder.model.encoder.layers.7.layer_norm2.bias', 'encoder.model.encoder.layers.8.self_attn.k_proj.weight', 'encoder.model.encoder.layers.8.self_attn.k_proj.bias', 'encoder.model.encoder.layers.8.self_attn.v_proj.weight', 'encoder.model.encoder.layers.8.self_attn.v_proj.bias', 'encoder.model.encoder.layers.8.self_attn.q_proj.weight', 'encoder.model.encoder.layers.8.self_attn.q_proj.bias', 'encoder.model.encoder.layers.8.self_attn.out_proj.weight', 'encoder.model.encoder.layers.8.self_attn.out_proj.bias', 'encoder.model.encoder.layers.8.layer_norm1.weight', 'encoder.model.encoder.layers.8.layer_norm1.bias', 'encoder.model.encoder.layers.8.mlp.fc1.weight', 'encoder.model.encoder.layers.8.mlp.fc1.bias', 'encoder.model.encoder.layers.8.mlp.fc2.weight', 'encoder.model.encoder.layers.8.mlp.fc2.bias', 'encoder.model.encoder.layers.8.layer_norm2.weight', 'encoder.model.encoder.layers.8.layer_norm2.bias', 'encoder.model.encoder.layers.9.self_attn.k_proj.weight', 'encoder.model.encoder.layers.9.self_attn.k_proj.bias', 'encoder.model.encoder.layers.9.self_attn.v_proj.weight', 'encoder.model.encoder.layers.9.self_attn.v_proj.bias', 'encoder.model.encoder.layers.9.self_attn.q_proj.weight', 'encoder.model.encoder.layers.9.self_attn.q_proj.bias', 'encoder.model.encoder.layers.9.self_attn.out_proj.weight', 'encoder.model.encoder.layers.9.self_attn.out_proj.bias', 'encoder.model.encoder.layers.9.layer_norm1.weight', 'encoder.model.encoder.layers.9.layer_norm1.bias', 'encoder.model.encoder.layers.9.mlp.fc1.weight', 'encoder.model.encoder.layers.9.mlp.fc1.bias', 'encoder.model.encoder.layers.9.mlp.fc2.weight', 'encoder.model.encoder.layers.9.mlp.fc2.bias', 'encoder.model.encoder.layers.9.layer_norm2.weight', 'encoder.model.encoder.layers.9.layer_norm2.bias', 'encoder.model.encoder.layers.10.self_attn.k_proj.weight', 'encoder.model.encoder.layers.10.self_attn.k_proj.bias', 'encoder.model.encoder.layers.10.self_attn.v_proj.weight', 'encoder.model.encoder.layers.10.self_attn.v_proj.bias', 'encoder.model.encoder.layers.10.self_attn.q_proj.weight', 'encoder.model.encoder.layers.10.self_attn.q_proj.bias', 'encoder.model.encoder.layers.10.self_attn.out_proj.weight', 'encoder.model.encoder.layers.10.self_attn.out_proj.bias', 'encoder.model.encoder.layers.10.layer_norm1.weight', 'encoder.model.encoder.layers.10.layer_norm1.bias', 'encoder.model.encoder.layers.10.mlp.fc1.weight', 'encoder.model.encoder.layers.10.mlp.fc1.bias', 'encoder.model.encoder.layers.10.mlp.fc2.weight', 'encoder.model.encoder.layers.10.mlp.fc2.bias', 'encoder.model.encoder.layers.10.layer_norm2.weight', 'encoder.model.encoder.layers.10.layer_norm2.bias', 'encoder.model.encoder.layers.11.self_attn.k_proj.weight', 'encoder.model.encoder.layers.11.self_attn.k_proj.bias', 'encoder.model.encoder.layers.11.self_attn.v_proj.weight', 'encoder.model.encoder.layers.11.self_attn.v_proj.bias', 'encoder.model.encoder.layers.11.self_attn.q_proj.weight', 'encoder.model.encoder.layers.11.self_attn.q_proj.bias', 'encoder.model.encoder.layers.11.self_attn.out_proj.weight', 'encoder.model.encoder.layers.11.self_attn.out_proj.bias', 'encoder.model.encoder.layers.11.layer_norm1.weight', 'encoder.model.encoder.layers.11.layer_norm1.bias', 'encoder.model.encoder.layers.11.mlp.fc1.weight', 'encoder.model.encoder.layers.11.mlp.fc1.bias', 'encoder.model.encoder.layers.11.mlp.fc2.weight', 'encoder.model.encoder.layers.11.mlp.fc2.bias', 'encoder.model.encoder.layers.11.layer_norm2.weight', 'encoder.model.encoder.layers.11.layer_norm2.bias', 'encoder.model.post_layernorm.weight', 'encoder.model.post_layernorm.bias', 'seg_head.aspp.aspp_layer.convs.0.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.0.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.0.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.1.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.1.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.1.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.2.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.2.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.2.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.3.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.3.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.3.block.norm.bias', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.conv.weight', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.weight', 'seg_head.aspp.aspp_layer.convs.4.aspp_pool.conv_1x1.block.norm.bias', 'seg_head.aspp.aspp_layer.project.block.conv.weight', 'seg_head.aspp.aspp_layer.project.block.norm.weight', 'seg_head.aspp.aspp_layer.project.block.norm.bias', 'seg_head.classifier.block.conv.weight', 'seg_head.classifier.block.conv.bias']
2024-07-28 07:55:39 - [34m[1mLOGS   [0m - [36mModel[0m
SegEncoderDecoder(
  (encoder): OpenClipViT(
    (neural_augmentor): DistributionNeuralAugmentor(
    	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
    	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
    	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
    (model): CLIPVisionTransformer(
      (embeddings): CLIPVisionEmbeddings(
        (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
        (position_embedding): Embedding(197, 768)
      )
      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (encoder): CLIPEncoder(
        (layers): ModuleList(
          (0-11): 12 x CLIPEncoderLayer(
            (self_attn): CLIPAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): CLIPMLP(
              (activation_fn): QuickGELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
            )
            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (classifier): None
  )
  (seg_head): DeeplabV3(
    (upsample_seg_out): UpSample(scale_factor=8.0, mode='bilinear')
    (aspp): Sequential(
      (aspp_layer): ASPP(in_channels=768, out_channels=224, atrous_rates=[12, 24, 36], is_aspp_sep=False, dropout=0.1)
    )
    (classifier): Conv2d(224, 103, kernel_size=(1, 1), stride=(1, 1))
  )
)
[31m=================================================================[0m
                  SegEncoderDecoder Summary
[31m=================================================================[0m
Total parameters     =   91.065 M
Total trainable parameters =   91.065 M

2024-07-28 07:55:39 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-28 07:55:39 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                                    | #parameters or shape   | #flops     |
|:------------------------------------------|:-----------------------|:-----------|
| model                                     | 91.065M                | 18.586G    |
|  encoder                                  |  85.799M               |  17.582G   |
|   encoder.neural_augmentor                |   6                    |            |
|    encoder.neural_augmentor.brightness    |    2                   |            |
|    encoder.neural_augmentor.contrast      |    2                   |            |
|    encoder.neural_augmentor.noise         |    2                   |            |
|   encoder.model                           |   85.799M              |   17.582G  |
|    encoder.model.embeddings               |    0.742M              |    0.116G  |
|    encoder.model.pre_layrnorm             |    1.536K              |    0.756M  |
|    encoder.model.encoder.layers           |    85.054M             |    17.466G |
|    encoder.model.post_layernorm           |    1.536K              |            |
|  seg_head                                 |  5.266M                |  1.004G    |
|   seg_head.aspp.aspp_layer                |   5.242M               |   0.994G   |
|    seg_head.aspp.aspp_layer.convs         |    4.991M              |    0.945G  |
|    seg_head.aspp.aspp_layer.project.block |    0.251M              |    49.26M  |
|   seg_head.classifier.block.conv          |   23.175K              |   4.522M   |
|    seg_head.classifier.block.conv.weight  |    (103, 224, 1, 1)    |            |
|    seg_head.classifier.block.conv.bias    |    (103,)              |            |
|   seg_head.upsample_seg_out               |                        |   5.168M   |
2024-07-28 07:55:42 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-28 07:55:42 - [33m[1mWARNING[0m - Uncalled Modules:
{'encoder.neural_augmentor.noise.max_fn', 'encoder.neural_augmentor.contrast.min_fn', 'encoder.neural_augmentor.noise', 'encoder.neural_augmentor.noise.min_fn', 'encoder.model.post_layernorm', 'encoder.neural_augmentor.brightness', 'encoder.neural_augmentor.brightness.max_fn', 'encoder.neural_augmentor.brightness.min_fn', 'encoder.neural_augmentor', 'encoder.neural_augmentor.contrast.max_fn', 'encoder.neural_augmentor.contrast'}
2024-07-28 07:55:42 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::mul': 48, 'aten::add': 25, 'aten::softmax': 12, 'aten::sigmoid': 12, 'aten::gelu': 6, 'aten::embedding': 1, 'aten::sub': 1, 'aten::feature_dropout': 1})
[31m=================================================================[0m
2024-07-28 07:55:43 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-28 07:55:43 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	SegCrossEntropy(  ignore_idx=-1  class_weighting=False  label_smoothing=0.0  aux_weight=0.4 loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-28 07:55:43 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-07-28 07:55:43 - [34m[1mLOGS   [0m - Max. epochs for training: 50
2024-07-28 07:55:43 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=3e-06
 	 max_lr=3e-05
 	 period=50
 	 warmup_init_lr=1e-06
 	 warmup_iters=500
 )
2024-07-28 07:55:43 - [34m[1mLOGS   [0m - No checkpoint found at '/ML-A100/team/mm/models/catlip_data/open_vit_base/uec_224/train/training_checkpoint_last.pt'
2024-07-28 07:55:43 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/open_vit_base/uec_224/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-28 07:55:45 - [32m[1mINFO   [0m - Training epoch 0
Terminated
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 208 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
