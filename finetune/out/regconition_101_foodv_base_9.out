nohup: ignoring input
2024-07-29 17:51:31 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
base
dci
2024-07-29 17:51:34 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_9_iter_79060.pt
2024-07-29 17:51:34 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-29 17:51:34 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (128,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=1024, out_features=101, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =  102.491 M
Total trainable parameters =  102.491 M

2024-07-29 17:51:34 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-29 17:51:34 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.102G                 | 13.398G    |
|  pos_embed                           |  (1, 1, 512)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  3.653M                |  5.52G     |
|   patch_embed.backbone.stem          |   0.151M               |   1.901G   |
|    patch_embed.backbone.stem.conv1   |    3.584K              |    43.352M |
|    patch_embed.backbone.stem.norm1   |    0.256K              |    8.028M  |
|    patch_embed.backbone.stem.conv2   |    0.148M              |    1.85G   |
|   patch_embed.backbone.stages        |   2.321M               |   3.387G   |
|    patch_embed.backbone.stages.0     |    0.274M              |    1.478G  |
|    patch_embed.backbone.stages.1     |    2.047M              |    1.909G  |
|   patch_embed.backbone.pool          |   1.181M               |   0.232G   |
|    patch_embed.backbone.pool.proj    |    1.18M               |    0.231G  |
|    patch_embed.backbone.pool.norm    |    0.512K              |    1.004M  |
|  blocks                              |  18.404M               |  3.607G    |
|   blocks.0                           |   2.629M               |   0.515G   |
|    blocks.0.norm1                    |    1.024K              |    0.502M  |
|    blocks.0.attn                     |    1.051M              |    0.206G  |
|    blocks.0.norm2                    |    1.024K              |    0.502M  |
|    blocks.0.mlp                      |    1.576M              |    0.309G  |
|   blocks.1                           |   2.629M               |   0.515G   |
|    blocks.1.norm1                    |    1.024K              |    0.502M  |
|    blocks.1.attn                     |    1.051M              |    0.206G  |
|    blocks.1.norm2                    |    1.024K              |    0.502M  |
|    blocks.1.mlp                      |    1.576M              |    0.309G  |
|   blocks.2                           |   2.629M               |   0.515G   |
|    blocks.2.norm1                    |    1.024K              |    0.502M  |
|    blocks.2.attn                     |    1.051M              |    0.206G  |
|    blocks.2.norm2                    |    1.024K              |    0.502M  |
|    blocks.2.mlp                      |    1.576M              |    0.309G  |
|   blocks.3                           |   2.629M               |   0.515G   |
|    blocks.3.norm1                    |    1.024K              |    0.502M  |
|    blocks.3.attn                     |    1.051M              |    0.206G  |
|    blocks.3.norm2                    |    1.024K              |    0.502M  |
|    blocks.3.mlp                      |    1.576M              |    0.309G  |
|   blocks.4                           |   2.629M               |   0.515G   |
|    blocks.4.norm1                    |    1.024K              |    0.502M  |
|    blocks.4.attn                     |    1.051M              |    0.206G  |
|    blocks.4.norm2                    |    1.024K              |    0.502M  |
|    blocks.4.mlp                      |    1.576M              |    0.309G  |
|   blocks.5                           |   2.629M               |   0.515G   |
|    blocks.5.norm1                    |    1.024K              |    0.502M  |
|    blocks.5.attn                     |    1.051M              |    0.206G  |
|    blocks.5.norm2                    |    1.024K              |    0.502M  |
|    blocks.5.mlp                      |    1.576M              |    0.309G  |
|   blocks.6                           |   2.629M               |   0.515G   |
|    blocks.6.norm1                    |    1.024K              |    0.502M  |
|    blocks.6.attn                     |    1.051M              |    0.206G  |
|    blocks.6.norm2                    |    1.024K              |    0.502M  |
|    blocks.6.mlp                      |    1.576M              |    0.309G  |
|  pool                                |  4.721M                |  0.463G    |
|   pool.proj                          |   4.72M                |   0.462G   |
|    pool.proj.weight                  |    (1024, 512, 3, 3)   |            |
|    pool.proj.bias                    |    (1024,)             |            |
|   pool.norm                          |   1.024K               |   1.004M   |
|    pool.norm.weight                  |    (512,)              |            |
|    pool.norm.bias                    |    (512,)              |            |
|  blocks1                             |  73.508M               |  3.602G    |
|   blocks1.0                          |   10.501M              |   0.515G   |
|    blocks1.0.norm1                   |    2.048K              |    0.251M  |
|    blocks1.0.attn                    |    4.198M              |    0.206G  |
|    blocks1.0.norm2                   |    2.048K              |    0.251M  |
|    blocks1.0.mlp                     |    6.299M              |    0.309G  |
|   blocks1.1                          |   10.501M              |   0.515G   |
|    blocks1.1.norm1                   |    2.048K              |    0.251M  |
|    blocks1.1.attn                    |    4.198M              |    0.206G  |
|    blocks1.1.norm2                   |    2.048K              |    0.251M  |
|    blocks1.1.mlp                     |    6.299M              |    0.309G  |
|   blocks1.2                          |   10.501M              |   0.515G   |
|    blocks1.2.norm1                   |    2.048K              |    0.251M  |
|    blocks1.2.attn                    |    4.198M              |    0.206G  |
|    blocks1.2.norm2                   |    2.048K              |    0.251M  |
|    blocks1.2.mlp                     |    6.299M              |    0.309G  |
|   blocks1.3                          |   10.501M              |   0.515G   |
|    blocks1.3.norm1                   |    2.048K              |    0.251M  |
|    blocks1.3.attn                    |    4.198M              |    0.206G  |
|    blocks1.3.norm2                   |    2.048K              |    0.251M  |
|    blocks1.3.mlp                     |    6.299M              |    0.309G  |
|   blocks1.4                          |   10.501M              |   0.515G   |
|    blocks1.4.norm1                   |    2.048K              |    0.251M  |
|    blocks1.4.attn                    |    4.198M              |    0.206G  |
|    blocks1.4.norm2                   |    2.048K              |    0.251M  |
|    blocks1.4.mlp                     |    6.299M              |    0.309G  |
|   blocks1.5                          |   10.501M              |   0.515G   |
|    blocks1.5.norm1                   |    2.048K              |    0.251M  |
|    blocks1.5.attn                    |    4.198M              |    0.206G  |
|    blocks1.5.norm2                   |    2.048K              |    0.251M  |
|    blocks1.5.mlp                     |    6.299M              |    0.309G  |
|   blocks1.6                          |   10.501M              |   0.515G   |
|    blocks1.6.norm1                   |    2.048K              |    0.251M  |
|    blocks1.6.attn                    |    4.198M              |    0.206G  |
|    blocks1.6.norm2                   |    2.048K              |    0.251M  |
|    blocks1.6.mlp                     |    6.299M              |    0.309G  |
|  mlp                                 |  2.099M                |  0.206G    |
|   mlp.0                              |   1.05M                |   0.103G   |
|    mlp.0.weight                      |    (1024, 1024)        |            |
|    mlp.0.bias                        |    (1024,)             |            |
|   mlp.2                              |   1.05M                |   0.103G   |
|    mlp.2.weight                      |    (1024, 1024)        |            |
|    mlp.2.bias                        |    (1024,)             |            |
|  fc_norm                             |  2.048K                |  5.12K     |
|   fc_norm.weight                     |   (1024,)              |            |
|   fc_norm.bias                       |   (1024,)              |            |
|  classifier                          |  0.104M                |  0.103M    |
|   classifier.weight                  |   (101, 1024)          |            |
|   classifier.bias                    |   (101,)               |            |
2024-07-29 17:51:34 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-29 17:51:34 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks.6.attn.attn_drop', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks.4.drop_path1', 'blocks.3.attn.k_norm', 'blocks1.3.ls2', 'neural_augmentor.contrast.min_fn', 'patch_drop', 'blocks.0.drop_path1', 'blocks.3.drop_path2', 'blocks1.2.drop_path2', 'blocks.0.attn.attn_drop', 'patch_embed.backbone.stages.0.0.drop_path', 'neural_augmentor.noise.max_fn', 'blocks.3.drop_path1', 'blocks1.5.drop_path1', 'blocks.1.drop_path2', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks1.0.attn.k_norm', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.2.attn.k_norm', 'blocks.5.ls1', 'blocks1.2.attn.attn_drop', 'patch_embed.proj', 'blocks1.3.drop_path1', 'blocks1.0.attn.attn_drop', 'blocks1.4.attn.q_norm', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks1.2.drop_path1', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks1.0.ls2', 'blocks.2.attn.k_norm', 'blocks.4.ls1', 'patch_embed.backbone.stages.1.1.down', 'blocks1.5.drop_path2', 'blocks1.4.attn.k_norm', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks1.6.attn.attn_drop', 'blocks1.1.drop_path1', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks1.6.attn.q_norm', 'neural_augmentor.contrast.max_fn', 'blocks1.3.attn.k_norm', 'blocks.2.drop_path1', 'blocks.5.attn.attn_drop', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks1.2.ls1', 'blocks1.1.ls2', 'blocks.5.drop_path1', 'blocks.1.ls1', 'blocks1.4.drop_path2', 'blocks1.6.attn.k_norm', 'blocks1.5.attn.attn_drop', 'blocks1.1.ls1', 'blocks1.3.attn.q_norm', 'neural_augmentor.contrast', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks.0.attn.q_norm', 'blocks.2.ls1', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks.2.drop_path2', 'blocks.6.drop_path2', 'patch_embed.backbone.stages.1.2.down', 'blocks.0.ls2', 'blocks.3.ls1', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks1.1.attn.q_norm', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks1.0.drop_path1', 'patch_embed.backbone.stages.1.1.drop_path', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'norm', 'blocks.1.attn.k_norm', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'patch_embed.backbone.stages.0.1.drop_path', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks1.6.ls1', 'neural_augmentor.noise.min_fn', 'patch_embed.backbone.stages.0.1.down', 'blocks.0.ls1', 'blocks.3.attn.q_norm', 'blocks.1.drop_path1', 'blocks1.1.attn.attn_drop', 'neural_augmentor.brightness', 'blocks1.5.ls2', 'blocks.4.attn.k_norm', 'blocks1.4.drop_path1', 'blocks.4.attn.q_norm', 'neural_augmentor.brightness.min_fn', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks.1.ls2', 'blocks.3.ls2', 'patch_embed.backbone.stem.norm1.drop', 'blocks.2.attn.attn_drop', 'blocks1.5.ls1', 'blocks.0.attn.k_norm', 'blocks.5.drop_path2', 'blocks1.4.attn.attn_drop', 'norm_pre', 'blocks.6.ls1', 'blocks.1.attn.attn_drop', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks.4.attn.attn_drop', 'blocks.0.drop_path2', 'neural_augmentor.noise', 'blocks1.0.attn.q_norm', 'blocks.6.drop_path1', 'blocks1.6.ls2', 'patch_embed.backbone.stages.1.0.down', 'blocks.2.ls2', 'blocks.5.attn.q_norm', 'blocks.1.attn.q_norm', 'blocks.6.ls2', 'blocks1.1.drop_path2', 'blocks1.5.attn.q_norm', 'blocks1.5.attn.k_norm', 'blocks1.6.drop_path1', 'blocks1.6.drop_path2', 'patch_embed.backbone.stages.1.3.down', 'blocks1.4.ls2', 'blocks1.3.attn.attn_drop', 'blocks1.2.ls2', 'patch_embed.backbone.stages.0.0.down', 'blocks1.1.attn.k_norm', 'blocks1.4.ls1', 'blocks.2.attn.q_norm', 'blocks.4.drop_path2', 'blocks1.3.drop_path2', 'blocks1.3.ls1', 'blocks1.0.drop_path2', 'neural_augmentor', 'blocks.6.attn.k_norm', 'blocks.5.ls2', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'neural_augmentor.brightness.max_fn', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks.4.ls2', 'blocks.5.attn.k_norm', 'blocks.3.attn.attn_drop', 'blocks.6.attn.q_norm', 'blocks1.2.attn.q_norm', 'blocks1.0.ls1'}
2024-07-29 17:51:34 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::add_': 14, 'aten::avg_pool2d': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-29 17:51:34 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-29 17:51:34 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-29 17:51:34 - [34m[1mLOGS   [0m - Available GPUs: 4
2024-07-29 17:51:34 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-29 17:51:34 - [34m[1mLOGS   [0m - Setting --ddp.world-size the same as the number of available gpus.
2024-07-29 17:51:34 - [34m[1mLOGS   [0m - Directory created at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train
2024-07-29 17:51:40 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:30006
base
dci
2024-07-29 17:51:40 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:30006
base
dci
2024-07-29 17:51:39 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:30006
base
dci
2024-07-29 17:51:39 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:30006
2024-07-29 17:51:43 - [34m[1mLOGS   [0m - Number of categories: 101
2024-07-29 17:51:43 - [34m[1mLOGS   [0m - Total number of samples: 75750
2024-07-29 17:51:43 - [34m[1mLOGS   [0m - Using all samples in the dataset.
2024-07-29 17:51:43 - [34m[1mLOGS   [0m - Training dataset details are given below
ImageNetDataset(
	root=/ML-A100/team/mm/models/food101/food101/train_images 
	is_training=True 
	num_samples=75750
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	 num_classes=101
)
2024-07-29 17:51:44 - [34m[1mLOGS   [0m - Number of categories: 101
2024-07-29 17:51:44 - [34m[1mLOGS   [0m - Total number of samples: 25250
2024-07-29 17:51:44 - [34m[1mLOGS   [0m - Using all samples in the dataset.
2024-07-29 17:51:44 - [34m[1mLOGS   [0m - Validation dataset details are given below
ImageNetDataset(
	root=/ML-A100/team/mm/models/food101/food101/test_images 
	is_training=False 
	num_samples=25250
	transforms=Compose(
			Resize(size=232, interpolation=bilinear, maintain_aspect_ratio=True), 
			CenterCrop(size=(h=224, w=224)), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	 num_classes=101
)
2024-07-29 17:51:44 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=64
	 scales=[(128, 128, 196), (160, 160, 125), (192, 192, 87), (224, 224, 64), (256, 256, 49), (288, 288, 38), (320, 320, 31)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-29 17:51:44 - [34m[1mLOGS   [0m - Validation sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=50
	 scales=[(224, 224, 50)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-29 17:51:44 - [34m[1mLOGS   [0m - Number of data workers: 64
base
dci
2024-07-29 17:51:48 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_9_iter_79060.pt
2024-07-29 17:51:48 - [32m[1mINFO   [0m - Trainable parameters: ['pos_embed', 'neural_augmentor.brightness._low', 'neural_augmentor.brightness._high', 'neural_augmentor.contrast._low', 'neural_augmentor.contrast._high', 'neural_augmentor.noise._low', 'neural_augmentor.noise._high', 'patch_embed.backbone.stem.conv1.weight', 'patch_embed.backbone.stem.conv1.bias', 'patch_embed.backbone.stem.norm1.weight', 'patch_embed.backbone.stem.norm1.bias', 'patch_embed.backbone.stem.conv2.weight', 'patch_embed.backbone.stem.conv2.bias', 'patch_embed.backbone.stages.0.0.pre_norm.weight', 'patch_embed.backbone.stages.0.0.pre_norm.bias', 'patch_embed.backbone.stages.0.0.conv1_1x1.weight', 'patch_embed.backbone.stages.0.0.conv1_1x1.bias', 'patch_embed.backbone.stages.0.0.conv2_kxk.weight', 'patch_embed.backbone.stages.0.0.conv2_kxk.bias', 'patch_embed.backbone.stages.0.0.conv3_1x1.weight', 'patch_embed.backbone.stages.0.0.conv3_1x1.bias', 'patch_embed.backbone.stages.0.1.pre_norm.weight', 'patch_embed.backbone.stages.0.1.pre_norm.bias', 'patch_embed.backbone.stages.0.1.conv1_1x1.weight', 'patch_embed.backbone.stages.0.1.conv1_1x1.bias', 'patch_embed.backbone.stages.0.1.conv2_kxk.weight', 'patch_embed.backbone.stages.0.1.conv2_kxk.bias', 'patch_embed.backbone.stages.0.1.conv3_1x1.weight', 'patch_embed.backbone.stages.0.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.0.shortcut.expand.weight', 'patch_embed.backbone.stages.1.0.shortcut.expand.bias', 'patch_embed.backbone.stages.1.0.pre_norm.weight', 'patch_embed.backbone.stages.1.0.pre_norm.bias', 'patch_embed.backbone.stages.1.0.conv1_1x1.weight', 'patch_embed.backbone.stages.1.0.conv1_1x1.bias', 'patch_embed.backbone.stages.1.0.conv2_kxk.weight', 'patch_embed.backbone.stages.1.0.conv2_kxk.bias', 'patch_embed.backbone.stages.1.0.conv3_1x1.weight', 'patch_embed.backbone.stages.1.0.conv3_1x1.bias', 'patch_embed.backbone.stages.1.1.pre_norm.weight', 'patch_embed.backbone.stages.1.1.pre_norm.bias', 'patch_embed.backbone.stages.1.1.conv1_1x1.weight', 'patch_embed.backbone.stages.1.1.conv1_1x1.bias', 'patch_embed.backbone.stages.1.1.conv2_kxk.weight', 'patch_embed.backbone.stages.1.1.conv2_kxk.bias', 'patch_embed.backbone.stages.1.1.conv3_1x1.weight', 'patch_embed.backbone.stages.1.1.conv3_1x1.bias', 'patch_embed.backbone.stages.1.2.pre_norm.weight', 'patch_embed.backbone.stages.1.2.pre_norm.bias', 'patch_embed.backbone.stages.1.2.conv1_1x1.weight', 'patch_embed.backbone.stages.1.2.conv1_1x1.bias', 'patch_embed.backbone.stages.1.2.conv2_kxk.weight', 'patch_embed.backbone.stages.1.2.conv2_kxk.bias', 'patch_embed.backbone.stages.1.2.conv3_1x1.weight', 'patch_embed.backbone.stages.1.2.conv3_1x1.bias', 'patch_embed.backbone.stages.1.3.pre_norm.weight', 'patch_embed.backbone.stages.1.3.pre_norm.bias', 'patch_embed.backbone.stages.1.3.conv1_1x1.weight', 'patch_embed.backbone.stages.1.3.conv1_1x1.bias', 'patch_embed.backbone.stages.1.3.conv2_kxk.weight', 'patch_embed.backbone.stages.1.3.conv2_kxk.bias', 'patch_embed.backbone.stages.1.3.conv3_1x1.weight', 'patch_embed.backbone.stages.1.3.conv3_1x1.bias', 'patch_embed.backbone.pool.proj.weight', 'patch_embed.backbone.pool.proj.bias', 'patch_embed.backbone.pool.norm.weight', 'patch_embed.backbone.pool.norm.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.norm.weight', 'blocks.0.mlp.norm.bias', 'blocks.0.mlp.w0.weight', 'blocks.0.mlp.w0.bias', 'blocks.0.mlp.w1.weight', 'blocks.0.mlp.w1.bias', 'blocks.0.mlp.w2.weight', 'blocks.0.mlp.w2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.norm.weight', 'blocks.1.mlp.norm.bias', 'blocks.1.mlp.w0.weight', 'blocks.1.mlp.w0.bias', 'blocks.1.mlp.w1.weight', 'blocks.1.mlp.w1.bias', 'blocks.1.mlp.w2.weight', 'blocks.1.mlp.w2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.norm.weight', 'blocks.2.mlp.norm.bias', 'blocks.2.mlp.w0.weight', 'blocks.2.mlp.w0.bias', 'blocks.2.mlp.w1.weight', 'blocks.2.mlp.w1.bias', 'blocks.2.mlp.w2.weight', 'blocks.2.mlp.w2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.norm.weight', 'blocks.3.mlp.norm.bias', 'blocks.3.mlp.w0.weight', 'blocks.3.mlp.w0.bias', 'blocks.3.mlp.w1.weight', 'blocks.3.mlp.w1.bias', 'blocks.3.mlp.w2.weight', 'blocks.3.mlp.w2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.norm.weight', 'blocks.4.mlp.norm.bias', 'blocks.4.mlp.w0.weight', 'blocks.4.mlp.w0.bias', 'blocks.4.mlp.w1.weight', 'blocks.4.mlp.w1.bias', 'blocks.4.mlp.w2.weight', 'blocks.4.mlp.w2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.norm.weight', 'blocks.5.mlp.norm.bias', 'blocks.5.mlp.w0.weight', 'blocks.5.mlp.w0.bias', 'blocks.5.mlp.w1.weight', 'blocks.5.mlp.w1.bias', 'blocks.5.mlp.w2.weight', 'blocks.5.mlp.w2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.norm.weight', 'blocks.6.mlp.norm.bias', 'blocks.6.mlp.w0.weight', 'blocks.6.mlp.w0.bias', 'blocks.6.mlp.w1.weight', 'blocks.6.mlp.w1.bias', 'blocks.6.mlp.w2.weight', 'blocks.6.mlp.w2.bias', 'pool.proj.weight', 'pool.proj.bias', 'pool.norm.weight', 'pool.norm.bias', 'blocks1.0.norm1.weight', 'blocks1.0.norm1.bias', 'blocks1.0.attn.qkv.weight', 'blocks1.0.attn.qkv.bias', 'blocks1.0.attn.proj.weight', 'blocks1.0.attn.proj.bias', 'blocks1.0.norm2.weight', 'blocks1.0.norm2.bias', 'blocks1.0.mlp.norm.weight', 'blocks1.0.mlp.norm.bias', 'blocks1.0.mlp.w0.weight', 'blocks1.0.mlp.w0.bias', 'blocks1.0.mlp.w1.weight', 'blocks1.0.mlp.w1.bias', 'blocks1.0.mlp.w2.weight', 'blocks1.0.mlp.w2.bias', 'blocks1.1.norm1.weight', 'blocks1.1.norm1.bias', 'blocks1.1.attn.qkv.weight', 'blocks1.1.attn.qkv.bias', 'blocks1.1.attn.proj.weight', 'blocks1.1.attn.proj.bias', 'blocks1.1.norm2.weight', 'blocks1.1.norm2.bias', 'blocks1.1.mlp.norm.weight', 'blocks1.1.mlp.norm.bias', 'blocks1.1.mlp.w0.weight', 'blocks1.1.mlp.w0.bias', 'blocks1.1.mlp.w1.weight', 'blocks1.1.mlp.w1.bias', 'blocks1.1.mlp.w2.weight', 'blocks1.1.mlp.w2.bias', 'blocks1.2.norm1.weight', 'blocks1.2.norm1.bias', 'blocks1.2.attn.qkv.weight', 'blocks1.2.attn.qkv.bias', 'blocks1.2.attn.proj.weight', 'blocks1.2.attn.proj.bias', 'blocks1.2.norm2.weight', 'blocks1.2.norm2.bias', 'blocks1.2.mlp.norm.weight', 'blocks1.2.mlp.norm.bias', 'blocks1.2.mlp.w0.weight', 'blocks1.2.mlp.w0.bias', 'blocks1.2.mlp.w1.weight', 'blocks1.2.mlp.w1.bias', 'blocks1.2.mlp.w2.weight', 'blocks1.2.mlp.w2.bias', 'blocks1.3.norm1.weight', 'blocks1.3.norm1.bias', 'blocks1.3.attn.qkv.weight', 'blocks1.3.attn.qkv.bias', 'blocks1.3.attn.proj.weight', 'blocks1.3.attn.proj.bias', 'blocks1.3.norm2.weight', 'blocks1.3.norm2.bias', 'blocks1.3.mlp.norm.weight', 'blocks1.3.mlp.norm.bias', 'blocks1.3.mlp.w0.weight', 'blocks1.3.mlp.w0.bias', 'blocks1.3.mlp.w1.weight', 'blocks1.3.mlp.w1.bias', 'blocks1.3.mlp.w2.weight', 'blocks1.3.mlp.w2.bias', 'blocks1.4.norm1.weight', 'blocks1.4.norm1.bias', 'blocks1.4.attn.qkv.weight', 'blocks1.4.attn.qkv.bias', 'blocks1.4.attn.proj.weight', 'blocks1.4.attn.proj.bias', 'blocks1.4.norm2.weight', 'blocks1.4.norm2.bias', 'blocks1.4.mlp.norm.weight', 'blocks1.4.mlp.norm.bias', 'blocks1.4.mlp.w0.weight', 'blocks1.4.mlp.w0.bias', 'blocks1.4.mlp.w1.weight', 'blocks1.4.mlp.w1.bias', 'blocks1.4.mlp.w2.weight', 'blocks1.4.mlp.w2.bias', 'blocks1.5.norm1.weight', 'blocks1.5.norm1.bias', 'blocks1.5.attn.qkv.weight', 'blocks1.5.attn.qkv.bias', 'blocks1.5.attn.proj.weight', 'blocks1.5.attn.proj.bias', 'blocks1.5.norm2.weight', 'blocks1.5.norm2.bias', 'blocks1.5.mlp.norm.weight', 'blocks1.5.mlp.norm.bias', 'blocks1.5.mlp.w0.weight', 'blocks1.5.mlp.w0.bias', 'blocks1.5.mlp.w1.weight', 'blocks1.5.mlp.w1.bias', 'blocks1.5.mlp.w2.weight', 'blocks1.5.mlp.w2.bias', 'blocks1.6.norm1.weight', 'blocks1.6.norm1.bias', 'blocks1.6.attn.qkv.weight', 'blocks1.6.attn.qkv.bias', 'blocks1.6.attn.proj.weight', 'blocks1.6.attn.proj.bias', 'blocks1.6.norm2.weight', 'blocks1.6.norm2.bias', 'blocks1.6.mlp.norm.weight', 'blocks1.6.mlp.norm.bias', 'blocks1.6.mlp.w0.weight', 'blocks1.6.mlp.w0.bias', 'blocks1.6.mlp.w1.weight', 'blocks1.6.mlp.w1.bias', 'blocks1.6.mlp.w2.weight', 'blocks1.6.mlp.w2.bias', 'mlp.0.weight', 'mlp.0.bias', 'mlp.2.weight', 'mlp.2.bias', 'fc_norm.weight', 'fc_norm.bias', 'classifier.weight', 'classifier.bias']
2024-07-29 17:51:48 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (128,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (256,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
            (conv3_1x1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=1024, out_features=3072, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=1024, out_features=1024, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=1024, out_features=2048, bias=True)
        (w1): Linear(in_features=1024, out_features=2048, bias=True)
        (w2): Linear(in_features=2048, out_features=1024, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=1024, out_features=1024, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=1024, out_features=1024, bias=True)
  )
  (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=1024, out_features=101, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =  102.491 M
Total trainable parameters =  102.491 M

2024-07-29 17:51:48 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-29 17:51:48 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 0.102G                 | 13.398G    |
|  pos_embed                           |  (1, 1, 512)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  3.653M                |  5.52G     |
|   patch_embed.backbone.stem          |   0.151M               |   1.901G   |
|    patch_embed.backbone.stem.conv1   |    3.584K              |    43.352M |
|    patch_embed.backbone.stem.norm1   |    0.256K              |    8.028M  |
|    patch_embed.backbone.stem.conv2   |    0.148M              |    1.85G   |
|   patch_embed.backbone.stages        |   2.321M               |   3.387G   |
|    patch_embed.backbone.stages.0     |    0.274M              |    1.478G  |
|    patch_embed.backbone.stages.1     |    2.047M              |    1.909G  |
|   patch_embed.backbone.pool          |   1.181M               |   0.232G   |
|    patch_embed.backbone.pool.proj    |    1.18M               |    0.231G  |
|    patch_embed.backbone.pool.norm    |    0.512K              |    1.004M  |
|  blocks                              |  18.404M               |  3.607G    |
|   blocks.0                           |   2.629M               |   0.515G   |
|    blocks.0.norm1                    |    1.024K              |    0.502M  |
|    blocks.0.attn                     |    1.051M              |    0.206G  |
|    blocks.0.norm2                    |    1.024K              |    0.502M  |
|    blocks.0.mlp                      |    1.576M              |    0.309G  |
|   blocks.1                           |   2.629M               |   0.515G   |
|    blocks.1.norm1                    |    1.024K              |    0.502M  |
|    blocks.1.attn                     |    1.051M              |    0.206G  |
|    blocks.1.norm2                    |    1.024K              |    0.502M  |
|    blocks.1.mlp                      |    1.576M              |    0.309G  |
|   blocks.2                           |   2.629M               |   0.515G   |
|    blocks.2.norm1                    |    1.024K              |    0.502M  |
|    blocks.2.attn                     |    1.051M              |    0.206G  |
|    blocks.2.norm2                    |    1.024K              |    0.502M  |
|    blocks.2.mlp                      |    1.576M              |    0.309G  |
|   blocks.3                           |   2.629M               |   0.515G   |
|    blocks.3.norm1                    |    1.024K              |    0.502M  |
|    blocks.3.attn                     |    1.051M              |    0.206G  |
|    blocks.3.norm2                    |    1.024K              |    0.502M  |
|    blocks.3.mlp                      |    1.576M              |    0.309G  |
|   blocks.4                           |   2.629M               |   0.515G   |
|    blocks.4.norm1                    |    1.024K              |    0.502M  |
|    blocks.4.attn                     |    1.051M              |    0.206G  |
|    blocks.4.norm2                    |    1.024K              |    0.502M  |
|    blocks.4.mlp                      |    1.576M              |    0.309G  |
|   blocks.5                           |   2.629M               |   0.515G   |
|    blocks.5.norm1                    |    1.024K              |    0.502M  |
|    blocks.5.attn                     |    1.051M              |    0.206G  |
|    blocks.5.norm2                    |    1.024K              |    0.502M  |
|    blocks.5.mlp                      |    1.576M              |    0.309G  |
|   blocks.6                           |   2.629M               |   0.515G   |
|    blocks.6.norm1                    |    1.024K              |    0.502M  |
|    blocks.6.attn                     |    1.051M              |    0.206G  |
|    blocks.6.norm2                    |    1.024K              |    0.502M  |
|    blocks.6.mlp                      |    1.576M              |    0.309G  |
|  pool                                |  4.721M                |  0.463G    |
|   pool.proj                          |   4.72M                |   0.462G   |
|    pool.proj.weight                  |    (1024, 512, 3, 3)   |            |
|    pool.proj.bias                    |    (1024,)             |            |
|   pool.norm                          |   1.024K               |   1.004M   |
|    pool.norm.weight                  |    (512,)              |            |
|    pool.norm.bias                    |    (512,)              |            |
|  blocks1                             |  73.508M               |  3.602G    |
|   blocks1.0                          |   10.501M              |   0.515G   |
|    blocks1.0.norm1                   |    2.048K              |    0.251M  |
|    blocks1.0.attn                    |    4.198M              |    0.206G  |
|    blocks1.0.norm2                   |    2.048K              |    0.251M  |
|    blocks1.0.mlp                     |    6.299M              |    0.309G  |
|   blocks1.1                          |   10.501M              |   0.515G   |
|    blocks1.1.norm1                   |    2.048K              |    0.251M  |
|    blocks1.1.attn                    |    4.198M              |    0.206G  |
|    blocks1.1.norm2                   |    2.048K              |    0.251M  |
|    blocks1.1.mlp                     |    6.299M              |    0.309G  |
|   blocks1.2                          |   10.501M              |   0.515G   |
|    blocks1.2.norm1                   |    2.048K              |    0.251M  |
|    blocks1.2.attn                    |    4.198M              |    0.206G  |
|    blocks1.2.norm2                   |    2.048K              |    0.251M  |
|    blocks1.2.mlp                     |    6.299M              |    0.309G  |
|   blocks1.3                          |   10.501M              |   0.515G   |
|    blocks1.3.norm1                   |    2.048K              |    0.251M  |
|    blocks1.3.attn                    |    4.198M              |    0.206G  |
|    blocks1.3.norm2                   |    2.048K              |    0.251M  |
|    blocks1.3.mlp                     |    6.299M              |    0.309G  |
|   blocks1.4                          |   10.501M              |   0.515G   |
|    blocks1.4.norm1                   |    2.048K              |    0.251M  |
|    blocks1.4.attn                    |    4.198M              |    0.206G  |
|    blocks1.4.norm2                   |    2.048K              |    0.251M  |
|    blocks1.4.mlp                     |    6.299M              |    0.309G  |
|   blocks1.5                          |   10.501M              |   0.515G   |
|    blocks1.5.norm1                   |    2.048K              |    0.251M  |
|    blocks1.5.attn                    |    4.198M              |    0.206G  |
|    blocks1.5.norm2                   |    2.048K              |    0.251M  |
|    blocks1.5.mlp                     |    6.299M              |    0.309G  |
|   blocks1.6                          |   10.501M              |   0.515G   |
|    blocks1.6.norm1                   |    2.048K              |    0.251M  |
|    blocks1.6.attn                    |    4.198M              |    0.206G  |
|    blocks1.6.norm2                   |    2.048K              |    0.251M  |
|    blocks1.6.mlp                     |    6.299M              |    0.309G  |
|  mlp                                 |  2.099M                |  0.206G    |
|   mlp.0                              |   1.05M                |   0.103G   |
|    mlp.0.weight                      |    (1024, 1024)        |            |
|    mlp.0.bias                        |    (1024,)             |            |
|   mlp.2                              |   1.05M                |   0.103G   |
|    mlp.2.weight                      |    (1024, 1024)        |            |
|    mlp.2.bias                        |    (1024,)             |            |
|  fc_norm                             |  2.048K                |  5.12K     |
|   fc_norm.weight                     |   (1024,)              |            |
|   fc_norm.bias                       |   (1024,)              |            |
|  classifier                          |  0.104M                |  0.103M    |
|   classifier.weight                  |   (101, 1024)          |            |
|   classifier.bias                    |   (101,)               |            |
2024-07-29 17:51:49 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-29 17:51:49 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks.3.ls1', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks.6.drop_path1', 'blocks.0.ls1', 'patch_drop', 'blocks.1.attn.attn_drop', 'blocks1.2.attn.attn_drop', 'blocks1.6.attn.k_norm', 'blocks1.4.attn.q_norm', 'norm', 'blocks1.6.drop_path1', 'blocks.3.ls2', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks1.2.attn.k_norm', 'blocks1.6.attn.attn_drop', 'blocks.0.attn.k_norm', 'blocks.2.attn.k_norm', 'patch_embed.backbone.stages.0.1.drop_path', 'patch_embed.backbone.stages.0.0.down', 'blocks.1.ls1', 'patch_embed.proj', 'blocks.5.attn.attn_drop', 'patch_embed.backbone.stages.1.1.drop_path', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'blocks.0.attn.attn_drop', 'blocks1.5.drop_path2', 'patch_embed.backbone.stages.1.3.drop_path', 'patch_embed.backbone.stages.1.2.down', 'blocks.3.attn.k_norm', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks.6.attn.k_norm', 'blocks.1.attn.q_norm', 'blocks.1.drop_path1', 'blocks.2.attn.q_norm', 'blocks.3.attn.attn_drop', 'blocks1.3.drop_path2', 'blocks.5.ls1', 'neural_augmentor.contrast', 'blocks.1.ls2', 'neural_augmentor.brightness.min_fn', 'blocks.5.ls2', 'blocks1.1.ls2', 'blocks1.4.attn.attn_drop', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'neural_augmentor.noise', 'blocks1.2.ls1', 'blocks.1.attn.k_norm', 'blocks.3.drop_path2', 'blocks1.3.attn.q_norm', 'blocks1.1.ls1', 'patch_embed.backbone.stages.1.3.down', 'patch_embed.backbone.stages.1.0.down', 'blocks1.0.attn.attn_drop', 'blocks.4.attn.q_norm', 'blocks1.3.ls2', 'blocks1.0.drop_path1', 'neural_augmentor', 'neural_augmentor.contrast.max_fn', 'blocks1.3.ls1', 'blocks.2.attn.attn_drop', 'blocks1.2.drop_path2', 'neural_augmentor.noise.max_fn', 'patch_embed.backbone.stages.0.1.shortcut', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks.2.ls1', 'blocks.6.drop_path2', 'blocks1.2.drop_path1', 'blocks1.3.attn.k_norm', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'neural_augmentor.noise.min_fn', 'norm_pre', 'blocks1.3.drop_path1', 'blocks1.4.ls1', 'blocks1.1.attn.k_norm', 'blocks1.1.drop_path2', 'blocks.6.ls2', 'blocks1.2.ls2', 'blocks.3.attn.q_norm', 'blocks1.5.attn.attn_drop', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks1.6.ls2', 'blocks1.4.attn.k_norm', 'blocks1.5.attn.k_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'neural_augmentor.brightness', 'blocks1.0.ls1', 'blocks.3.drop_path1', 'patch_embed.backbone.stem.norm1.drop', 'blocks1.0.attn.k_norm', 'blocks.4.attn.k_norm', 'patch_embed.backbone.stages.0.1.down', 'blocks.6.attn.attn_drop', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks1.4.drop_path1', 'blocks.5.attn.k_norm', 'blocks1.5.drop_path1', 'blocks1.6.ls1', 'blocks1.2.attn.q_norm', 'blocks.4.ls2', 'blocks1.0.ls2', 'blocks1.6.attn.q_norm', 'blocks1.1.attn.attn_drop', 'blocks1.6.drop_path2', 'blocks1.1.attn.q_norm', 'patch_embed.backbone.stages.0.0.drop_path', 'neural_augmentor.brightness.max_fn', 'blocks.0.drop_path2', 'blocks.0.ls2', 'blocks1.0.attn.q_norm', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks.2.ls2', 'blocks.1.drop_path2', 'blocks.4.attn.attn_drop', 'blocks.0.attn.q_norm', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks.4.drop_path1', 'blocks.6.ls1', 'patch_embed.backbone.stages.1.1.down', 'neural_augmentor.contrast.min_fn', 'blocks1.5.attn.q_norm', 'blocks.4.ls1', 'blocks1.5.ls1', 'blocks.6.attn.q_norm', 'blocks1.0.drop_path2', 'blocks.0.drop_path1', 'blocks1.4.ls2', 'blocks.4.drop_path2', 'blocks1.4.drop_path2', 'blocks1.5.ls2', 'blocks.5.drop_path1', 'blocks.5.attn.q_norm', 'blocks.2.drop_path2', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks.2.drop_path1', 'blocks1.3.attn.attn_drop', 'blocks1.1.drop_path1', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks.5.drop_path2', 'patch_embed.backbone.stages.1.2.pre_norm.act'}
2024-07-29 17:51:49 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::add_': 14, 'aten::avg_pool2d': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-29 17:51:49 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-29 17:51:49 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	CrossEntropy(  ignore_idx=-1  class_weighting=False  label_smoothing=0.1 loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-29 17:51:49 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-07-29 17:51:49 - [34m[1mLOGS   [0m - Max. epochs for training: 30
2024-07-29 17:51:49 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=5e-06
 	 max_lr=5e-05
 	 period=30
 	 warmup_init_lr=1e-06
 	 warmup_iters=500
 )
2024-07-29 17:51:49 - [34m[1mLOGS   [0m - No checkpoint found at '/ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt'
2024-07-29 17:51:49 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-29 17:51:51 - [32m[1mINFO   [0m - Training epoch 0
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/ML-A800/home/guoshuyue/anaconda3/envs/corenet/lib/python3.10/site-packages/torch/autograd/__init__.py:266: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1024, 1024]
bucket_view.sizes() = [256, 1024, 1, 1], strides() = [1024, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:322.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
2024-07-29 17:55:04 - [34m[1mLOGS   [0m - Epoch:   0 [       1/10000000], loss: {'classification': 6.0654, 'neural_augmentation': 0.1891, 'total_loss': 6.2545}, LR: [1e-06, 1e-06], Avg. batch load time: 189.551, Elapsed time: 192.60
2024-07-29 17:55:42 - [34m[1mLOGS   [0m - *** Training summary for epoch 0
	 loss={'classification': 3.7406, 'neural_augmentation': 0.2008, 'total_loss': 3.9414}
2024-07-29 17:59:07 - [34m[1mLOGS   [0m - *** Validation summary for epoch 0
	 loss={'classification': 0.4673, 'neural_augmentation': 0.0, 'total_loss': 0.4673} || top1={'logits': 90.8268} || top5={'logits': 97.7402}
2024-07-29 17:59:07 - [34m[1mLOGS   [0m - Best checkpoint with score 90.83 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 17:59:10 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 17:59:14 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 17:59:21 - [34m[1mLOGS   [0m - Training checkpoint for epoch 0/iteration 216 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_0_iter_216.pt
2024-07-29 17:59:22 - [34m[1mLOGS   [0m - Model state for epoch 0/iteration 216 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_0_iter_216.pt
[31m===========================================================================[0m
2024-07-29 17:59:24 - [32m[1mINFO   [0m - Training epoch 1
2024-07-29 17:59:25 - [34m[1mLOGS   [0m - Epoch:   1 [     217/10000000], loss: {'classification': 1.5609, 'neural_augmentation': 0.1835, 'total_loss': 1.7444}, LR: [2.2e-05, 2.2e-05], Avg. batch load time: 1.570, Elapsed time:  1.75
2024-07-29 18:00:04 - [34m[1mLOGS   [0m - *** Training summary for epoch 1
	 loss={'classification': 1.4504, 'neural_augmentation': 0.2003, 'total_loss': 1.6507}
2024-07-29 18:00:14 - [34m[1mLOGS   [0m - *** Validation summary for epoch 1
	 loss={'classification': 0.344, 'neural_augmentation': 0.0, 'total_loss': 0.344} || top1={'logits': 93.7126} || top5={'logits': 98.8937}
2024-07-29 18:00:14 - [34m[1mLOGS   [0m - Best checkpoint with score 93.71 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:00:16 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:00:17 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:00:19 - [34m[1mLOGS   [0m - Training checkpoint for epoch 1/iteration 440 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_1_iter_440.pt
2024-07-29 18:00:19 - [34m[1mLOGS   [0m - Model state for epoch 1/iteration 440 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_1_iter_440.pt
[31m===========================================================================[0m
2024-07-29 18:00:21 - [32m[1mINFO   [0m - Training epoch 2
2024-07-29 18:00:22 - [34m[1mLOGS   [0m - Epoch:   2 [     441/10000000], loss: {'classification': 1.2588, 'neural_augmentation': 0.1946, 'total_loss': 1.4534}, LR: [4.4e-05, 4.4e-05], Avg. batch load time: 0.474, Elapsed time:  0.66
2024-07-29 18:01:05 - [34m[1mLOGS   [0m - *** Training summary for epoch 2
	 loss={'classification': 1.3105, 'neural_augmentation': 0.1959, 'total_loss': 1.5064}
2024-07-29 18:01:15 - [34m[1mLOGS   [0m - *** Validation summary for epoch 2
	 loss={'classification': 0.3142, 'neural_augmentation': 0.0, 'total_loss': 0.3142} || top1={'logits': 94.252} || top5={'logits': 99.1693}
2024-07-29 18:01:16 - [34m[1mLOGS   [0m - Best checkpoint with score 94.25 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:01:18 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:01:20 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:01:22 - [34m[1mLOGS   [0m - Training checkpoint for epoch 2/iteration 679 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_2_iter_679.pt
2024-07-29 18:01:23 - [34m[1mLOGS   [0m - Model state for epoch 2/iteration 679 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_2_iter_679.pt
[31m===========================================================================[0m
2024-07-29 18:01:25 - [32m[1mINFO   [0m - Training epoch 3
2024-07-29 18:01:25 - [34m[1mLOGS   [0m - Epoch:   3 [     680/10000000], loss: {'classification': 1.1597, 'neural_augmentation': 0.1904, 'total_loss': 1.3501}, LR: [4.9e-05, 4.9e-05], Avg. batch load time: 0.422, Elapsed time:  0.60
2024-07-29 18:02:03 - [34m[1mLOGS   [0m - *** Training summary for epoch 3
	 loss={'classification': 1.2434, 'neural_augmentation': 0.1897, 'total_loss': 1.4332}
2024-07-29 18:02:16 - [34m[1mLOGS   [0m - *** Validation summary for epoch 3
	 loss={'classification': 0.2977, 'neural_augmentation': 0.0, 'total_loss': 0.2977} || top1={'logits': 94.563} || top5={'logits': 99.2244}
2024-07-29 18:02:17 - [34m[1mLOGS   [0m - Best checkpoint with score 94.56 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:02:20 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:02:21 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:02:22 - [34m[1mLOGS   [0m - Training checkpoint for epoch 3/iteration 895 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_3_iter_895.pt
2024-07-29 18:02:23 - [34m[1mLOGS   [0m - Model state for epoch 3/iteration 895 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_3_iter_895.pt
[31m===========================================================================[0m
2024-07-29 18:02:25 - [32m[1mINFO   [0m - Training epoch 4
2024-07-29 18:02:25 - [34m[1mLOGS   [0m - Epoch:   4 [     896/10000000], loss: {'classification': 1.1844, 'neural_augmentation': 0.1867, 'total_loss': 1.3711}, LR: [4.8e-05, 4.8e-05], Avg. batch load time: 0.556, Elapsed time:  0.74
2024-07-29 18:03:06 - [34m[1mLOGS   [0m - *** Training summary for epoch 4
	 loss={'classification': 1.2074, 'neural_augmentation': 0.1854, 'total_loss': 1.3927}
2024-07-29 18:03:15 - [34m[1mLOGS   [0m - *** Validation summary for epoch 4
	 loss={'classification': 0.2958, 'neural_augmentation': 0.0, 'total_loss': 0.2958} || top1={'logits': 94.9016} || top5={'logits': 99.2244}
2024-07-29 18:03:16 - [34m[1mLOGS   [0m - Best checkpoint with score 94.90 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:03:17 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:03:18 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:03:20 - [34m[1mLOGS   [0m - Training checkpoint for epoch 4/iteration 1109 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_4_iter_1109.pt
2024-07-29 18:03:21 - [34m[1mLOGS   [0m - Model state for epoch 4/iteration 1109 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_4_iter_1109.pt
[31m===========================================================================[0m
2024-07-29 18:03:23 - [32m[1mINFO   [0m - Training epoch 5
2024-07-29 18:03:25 - [34m[1mLOGS   [0m - Epoch:   5 [    1110/10000000], loss: {'classification': 1.2046, 'neural_augmentation': 0.1874, 'total_loss': 1.392}, LR: [4.7e-05, 4.7e-05], Avg. batch load time: 1.993, Elapsed time:  2.17
2024-07-29 18:04:04 - [34m[1mLOGS   [0m - *** Training summary for epoch 5
	 loss={'classification': 1.1699, 'neural_augmentation': 0.1816, 'total_loss': 1.3515}
2024-07-29 18:04:15 - [34m[1mLOGS   [0m - *** Validation summary for epoch 5
	 loss={'classification': 0.2799, 'neural_augmentation': 0.0, 'total_loss': 0.2799} || top1={'logits': 94.7677} || top5={'logits': 99.2323}
2024-07-29 18:04:22 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:04:23 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:04:24 - [34m[1mLOGS   [0m - Training checkpoint for epoch 5/iteration 1335 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_5_iter_1335.pt
2024-07-29 18:04:25 - [34m[1mLOGS   [0m - Model state for epoch 5/iteration 1335 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_5_iter_1335.pt
[31m===========================================================================[0m
2024-07-29 18:04:27 - [32m[1mINFO   [0m - Training epoch 6
2024-07-29 18:04:29 - [34m[1mLOGS   [0m - Epoch:   6 [    1336/10000000], loss: {'classification': 1.1705, 'neural_augmentation': 0.1753, 'total_loss': 1.3458}, LR: [4.6e-05, 4.6e-05], Avg. batch load time: 1.988, Elapsed time:  2.17
2024-07-29 18:05:07 - [34m[1mLOGS   [0m - *** Training summary for epoch 6
	 loss={'classification': 1.1382, 'neural_augmentation': 0.1797, 'total_loss': 1.3179}
2024-07-29 18:05:17 - [34m[1mLOGS   [0m - *** Validation summary for epoch 6
	 loss={'classification': 0.2833, 'neural_augmentation': 0.0, 'total_loss': 0.2833} || top1={'logits': 94.8819} || top5={'logits': 99.2559}
2024-07-29 18:05:18 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:05:19 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:05:21 - [34m[1mLOGS   [0m - Training checkpoint for epoch 6/iteration 1559 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_6_iter_1559.pt
2024-07-29 18:05:22 - [34m[1mLOGS   [0m - Model state for epoch 6/iteration 1559 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_6_iter_1559.pt
[31m===========================================================================[0m
2024-07-29 18:05:24 - [32m[1mINFO   [0m - Training epoch 7
2024-07-29 18:05:26 - [34m[1mLOGS   [0m - Epoch:   7 [    1560/10000000], loss: {'classification': 1.1997, 'neural_augmentation': 0.1759, 'total_loss': 1.3756}, LR: [4.4e-05, 4.4e-05], Avg. batch load time: 2.041, Elapsed time:  2.22
2024-07-29 18:06:09 - [34m[1mLOGS   [0m - *** Training summary for epoch 7
	 loss={'classification': 1.1088, 'neural_augmentation': 0.1801, 'total_loss': 1.2889}
2024-07-29 18:06:19 - [34m[1mLOGS   [0m - *** Validation summary for epoch 7
	 loss={'classification': 0.2828, 'neural_augmentation': 0.0, 'total_loss': 0.2828} || top1={'logits': 94.8701} || top5={'logits': 99.2953}
2024-07-29 18:06:21 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:06:21 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:06:25 - [34m[1mLOGS   [0m - Training checkpoint for epoch 7/iteration 1804 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_7_iter_1804.pt
2024-07-29 18:06:25 - [34m[1mLOGS   [0m - Model state for epoch 7/iteration 1804 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_7_iter_1804.pt
[31m===========================================================================[0m
2024-07-29 18:06:27 - [32m[1mINFO   [0m - Training epoch 8
2024-07-29 18:06:28 - [34m[1mLOGS   [0m - Epoch:   8 [    1805/10000000], loss: {'classification': 1.0427, 'neural_augmentation': 0.1966, 'total_loss': 1.2392}, LR: [4.3e-05, 4.3e-05], Avg. batch load time: 0.400, Elapsed time:  0.58
2024-07-29 18:07:06 - [34m[1mLOGS   [0m - *** Training summary for epoch 8
	 loss={'classification': 1.0979, 'neural_augmentation': 0.1839, 'total_loss': 1.2818}
2024-07-29 18:07:16 - [34m[1mLOGS   [0m - *** Validation summary for epoch 8
	 loss={'classification': 0.2821, 'neural_augmentation': 0.0, 'total_loss': 0.2821} || top1={'logits': 95.1181} || top5={'logits': 99.3031}
2024-07-29 18:07:17 - [34m[1mLOGS   [0m - Best checkpoint with score 95.12 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:07:18 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_score_90.8268.pt
2024-07-29 18:07:18 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_93.7126.pt', 'checkpoint_score_94.2520.pt', 'checkpoint_score_94.5630.pt', 'checkpoint_score_94.9016.pt', 'checkpoint_score_95.1181.pt']
2024-07-29 18:07:31 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_avg.pt
2024-07-29 18:07:32 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:07:32 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:07:34 - [34m[1mLOGS   [0m - Training checkpoint for epoch 8/iteration 2019 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_8_iter_2019.pt
2024-07-29 18:07:34 - [34m[1mLOGS   [0m - Model state for epoch 8/iteration 2019 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_8_iter_2019.pt
[31m===========================================================================[0m
2024-07-29 18:07:36 - [32m[1mINFO   [0m - Training epoch 9
2024-07-29 18:07:38 - [34m[1mLOGS   [0m - Epoch:   9 [    2020/10000000], loss: {'classification': 0.979, 'neural_augmentation': 0.1836, 'total_loss': 1.1626}, LR: [4.1e-05, 4.1e-05], Avg. batch load time: 1.824, Elapsed time:  2.01
2024-07-29 18:08:19 - [34m[1mLOGS   [0m - *** Training summary for epoch 9
	 loss={'classification': 1.078, 'neural_augmentation': 0.1939, 'total_loss': 1.2719}
2024-07-29 18:08:28 - [34m[1mLOGS   [0m - *** Validation summary for epoch 9
	 loss={'classification': 0.2751, 'neural_augmentation': 0.0, 'total_loss': 0.2751} || top1={'logits': 95.1102} || top5={'logits': 99.2559}
2024-07-29 18:08:30 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:08:30 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:08:33 - [34m[1mLOGS   [0m - Training checkpoint for epoch 9/iteration 2238 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_9_iter_2238.pt
2024-07-29 18:08:34 - [34m[1mLOGS   [0m - Model state for epoch 9/iteration 2238 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_9_iter_2238.pt
[31m===========================================================================[0m
2024-07-29 18:08:36 - [32m[1mINFO   [0m - Training epoch 10
2024-07-29 18:08:37 - [34m[1mLOGS   [0m - Epoch:  10 [    2239/10000000], loss: {'classification': 0.9511, 'neural_augmentation': 0.1875, 'total_loss': 1.1387}, LR: [3.9e-05, 3.9e-05], Avg. batch load time: 1.002, Elapsed time:  1.18
2024-07-29 18:09:17 - [34m[1mLOGS   [0m - *** Training summary for epoch 10
	 loss={'classification': 1.0623, 'neural_augmentation': 0.2076, 'total_loss': 1.2699}
2024-07-29 18:09:27 - [34m[1mLOGS   [0m - *** Validation summary for epoch 10
	 loss={'classification': 0.289, 'neural_augmentation': 0.0, 'total_loss': 0.289} || top1={'logits': 95.0157} || top5={'logits': 99.2913}
2024-07-29 18:09:29 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:09:29 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:09:33 - [34m[1mLOGS   [0m - Training checkpoint for epoch 10/iteration 2461 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_10_iter_2461.pt
2024-07-29 18:09:34 - [34m[1mLOGS   [0m - Model state for epoch 10/iteration 2461 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_10_iter_2461.pt
[31m===========================================================================[0m
2024-07-29 18:09:36 - [32m[1mINFO   [0m - Training epoch 11
2024-07-29 18:09:36 - [34m[1mLOGS   [0m - Epoch:  11 [    2462/10000000], loss: {'classification': 0.9211, 'neural_augmentation': 0.2164, 'total_loss': 1.1375}, LR: [3.7e-05, 3.7e-05], Avg. batch load time: 0.396, Elapsed time:  0.57
2024-07-29 18:10:13 - [34m[1mLOGS   [0m - *** Training summary for epoch 11
	 loss={'classification': 1.047, 'neural_augmentation': 0.227, 'total_loss': 1.274}
2024-07-29 18:10:26 - [34m[1mLOGS   [0m - *** Validation summary for epoch 11
	 loss={'classification': 0.2849, 'neural_augmentation': 0.0, 'total_loss': 0.2849} || top1={'logits': 95.0} || top5={'logits': 99.2913}
2024-07-29 18:10:28 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:10:28 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:10:31 - [34m[1mLOGS   [0m - Training checkpoint for epoch 11/iteration 2673 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_11_iter_2673.pt
2024-07-29 18:10:32 - [34m[1mLOGS   [0m - Model state for epoch 11/iteration 2673 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_11_iter_2673.pt
[31m===========================================================================[0m
2024-07-29 18:10:34 - [32m[1mINFO   [0m - Training epoch 12
2024-07-29 18:10:34 - [34m[1mLOGS   [0m - Epoch:  12 [    2674/10000000], loss: {'classification': 0.9881, 'neural_augmentation': 0.2361, 'total_loss': 1.2242}, LR: [3.4e-05, 3.4e-05], Avg. batch load time: 0.445, Elapsed time:  0.62
2024-07-29 18:11:17 - [34m[1mLOGS   [0m - *** Training summary for epoch 12
	 loss={'classification': 1.0342, 'neural_augmentation': 0.2488, 'total_loss': 1.2831}
2024-07-29 18:11:27 - [34m[1mLOGS   [0m - *** Validation summary for epoch 12
	 loss={'classification': 0.2855, 'neural_augmentation': 0.0, 'total_loss': 0.2855} || top1={'logits': 94.9252} || top5={'logits': 99.2402}
2024-07-29 18:11:29 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:11:29 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:11:32 - [34m[1mLOGS   [0m - Training checkpoint for epoch 12/iteration 2902 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_12_iter_2902.pt
2024-07-29 18:11:32 - [34m[1mLOGS   [0m - Model state for epoch 12/iteration 2902 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_12_iter_2902.pt
[31m===========================================================================[0m
2024-07-29 18:11:34 - [32m[1mINFO   [0m - Training epoch 13
2024-07-29 18:11:35 - [34m[1mLOGS   [0m - Epoch:  13 [    2903/10000000], loss: {'classification': 1.0971, 'neural_augmentation': 0.2715, 'total_loss': 1.3686}, LR: [3.2e-05, 3.2e-05], Avg. batch load time: 0.712, Elapsed time:  0.89
2024-07-29 18:12:11 - [34m[1mLOGS   [0m - *** Training summary for epoch 13
	 loss={'classification': 1.0306, 'neural_augmentation': 0.278, 'total_loss': 1.3086}
2024-07-29 18:12:20 - [34m[1mLOGS   [0m - *** Validation summary for epoch 13
	 loss={'classification': 0.2825, 'neural_augmentation': 0.0, 'total_loss': 0.2825} || top1={'logits': 95.1181} || top5={'logits': 99.2283}
2024-07-29 18:12:23 - [34m[1mLOGS   [0m - Best checkpoint with score 95.12 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:12:32 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:12:32 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:12:34 - [34m[1mLOGS   [0m - Training checkpoint for epoch 13/iteration 3104 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_13_iter_3104.pt
2024-07-29 18:12:35 - [34m[1mLOGS   [0m - Model state for epoch 13/iteration 3104 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_13_iter_3104.pt
[31m===========================================================================[0m
2024-07-29 18:12:37 - [32m[1mINFO   [0m - Training epoch 14
2024-07-29 18:12:38 - [34m[1mLOGS   [0m - Epoch:  14 [    3105/10000000], loss: {'classification': 1.0384, 'neural_augmentation': 0.3089, 'total_loss': 1.3472}, LR: [3e-05, 3e-05], Avg. batch load time: 1.428, Elapsed time:  1.60
2024-07-29 18:13:14 - [34m[1mLOGS   [0m - *** Training summary for epoch 14
	 loss={'classification': 1.0144, 'neural_augmentation': 0.3103, 'total_loss': 1.3248}
2024-07-29 18:13:24 - [34m[1mLOGS   [0m - *** Validation summary for epoch 14
	 loss={'classification': 0.2833, 'neural_augmentation': 0.0, 'total_loss': 0.2833} || top1={'logits': 95.185} || top5={'logits': 99.2283}
2024-07-29 18:13:25 - [34m[1mLOGS   [0m - Best checkpoint with score 95.19 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:13:25 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_score_93.7126.pt
2024-07-29 18:13:25 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_94.2520.pt', 'checkpoint_score_94.5630.pt', 'checkpoint_score_94.9016.pt', 'checkpoint_score_95.1181.pt', 'checkpoint_score_95.1850.pt']
2024-07-29 18:13:31 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_avg.pt
2024-07-29 18:13:32 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:13:33 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:13:34 - [34m[1mLOGS   [0m - Training checkpoint for epoch 14/iteration 3314 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_14_iter_3314.pt
2024-07-29 18:13:34 - [34m[1mLOGS   [0m - Model state for epoch 14/iteration 3314 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_14_iter_3314.pt
[31m===========================================================================[0m
2024-07-29 18:13:36 - [32m[1mINFO   [0m - Training epoch 15
2024-07-29 18:13:37 - [34m[1mLOGS   [0m - Epoch:  15 [    3315/10000000], loss: {'classification': 0.8748, 'neural_augmentation': 0.3558, 'total_loss': 1.2306}, LR: [2.8e-05, 2.8e-05], Avg. batch load time: 0.477, Elapsed time:  0.65
2024-07-29 18:14:14 - [34m[1mLOGS   [0m - *** Training summary for epoch 15
	 loss={'classification': 1.0024, 'neural_augmentation': 0.3467, 'total_loss': 1.349}
2024-07-29 18:14:24 - [34m[1mLOGS   [0m - *** Validation summary for epoch 15
	 loss={'classification': 0.2842, 'neural_augmentation': 0.0, 'total_loss': 0.2842} || top1={'logits': 95.1457} || top5={'logits': 99.2441}
2024-07-29 18:14:26 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:14:26 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:14:30 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 3520 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_15_iter_3520.pt
2024-07-29 18:14:30 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 3520 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_15_iter_3520.pt
[31m===========================================================================[0m
2024-07-29 18:14:32 - [32m[1mINFO   [0m - Training epoch 16
2024-07-29 18:14:34 - [34m[1mLOGS   [0m - Epoch:  16 [    3521/10000000], loss: {'classification': 1.0344, 'neural_augmentation': 0.3872, 'total_loss': 1.4216}, LR: [2.5e-05, 2.5e-05], Avg. batch load time: 1.908, Elapsed time:  2.08
2024-07-29 18:15:14 - [34m[1mLOGS   [0m - *** Training summary for epoch 16
	 loss={'classification': 0.9934, 'neural_augmentation': 0.3856, 'total_loss': 1.3789}
2024-07-29 18:15:23 - [34m[1mLOGS   [0m - *** Validation summary for epoch 16
	 loss={'classification': 0.2771, 'neural_augmentation': 0.0, 'total_loss': 0.2771} || top1={'logits': 95.2756} || top5={'logits': 99.2717}
2024-07-29 18:15:24 - [34m[1mLOGS   [0m - Best checkpoint with score 95.28 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:15:24 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_score_94.2520.pt
2024-07-29 18:15:24 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_94.5630.pt', 'checkpoint_score_94.9016.pt', 'checkpoint_score_95.1181.pt', 'checkpoint_score_95.1850.pt', 'checkpoint_score_95.2756.pt']
2024-07-29 18:15:41 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_avg.pt
2024-07-29 18:15:42 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:15:42 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:15:44 - [34m[1mLOGS   [0m - Training checkpoint for epoch 16/iteration 3750 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_16_iter_3750.pt
2024-07-29 18:15:44 - [34m[1mLOGS   [0m - Model state for epoch 16/iteration 3750 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_16_iter_3750.pt
[31m===========================================================================[0m
2024-07-29 18:15:46 - [32m[1mINFO   [0m - Training epoch 17
2024-07-29 18:15:47 - [34m[1mLOGS   [0m - Epoch:  17 [    3751/10000000], loss: {'classification': 0.9529, 'neural_augmentation': 0.4325, 'total_loss': 1.3854}, LR: [2.3e-05, 2.3e-05], Avg. batch load time: 1.244, Elapsed time:  1.46
2024-07-29 18:16:29 - [34m[1mLOGS   [0m - *** Training summary for epoch 17
	 loss={'classification': 0.9891, 'neural_augmentation': 0.4259, 'total_loss': 1.415}
2024-07-29 18:16:39 - [34m[1mLOGS   [0m - *** Validation summary for epoch 17
	 loss={'classification': 0.2824, 'neural_augmentation': 0.0, 'total_loss': 0.2824} || top1={'logits': 95.2835} || top5={'logits': 99.2205}
2024-07-29 18:16:40 - [34m[1mLOGS   [0m - Best checkpoint with score 95.28 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:16:40 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_score_94.5630.pt
2024-07-29 18:16:40 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_94.9016.pt', 'checkpoint_score_95.1181.pt', 'checkpoint_score_95.1850.pt', 'checkpoint_score_95.2756.pt', 'checkpoint_score_95.2835.pt']
2024-07-29 18:16:46 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_avg.pt
2024-07-29 18:16:47 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:16:48 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:16:49 - [34m[1mLOGS   [0m - Training checkpoint for epoch 17/iteration 3968 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_17_iter_3968.pt
2024-07-29 18:16:49 - [34m[1mLOGS   [0m - Model state for epoch 17/iteration 3968 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_17_iter_3968.pt
[31m===========================================================================[0m
2024-07-29 18:16:51 - [32m[1mINFO   [0m - Training epoch 18
2024-07-29 18:16:53 - [34m[1mLOGS   [0m - Epoch:  18 [    3969/10000000], loss: {'classification': 0.9804, 'neural_augmentation': 0.4835, 'total_loss': 1.4639}, LR: [2.1e-05, 2.1e-05], Avg. batch load time: 1.243, Elapsed time:  1.42
2024-07-29 18:17:32 - [34m[1mLOGS   [0m - *** Training summary for epoch 18
	 loss={'classification': 0.9822, 'neural_augmentation': 0.4664, 'total_loss': 1.4486}
2024-07-29 18:17:42 - [34m[1mLOGS   [0m - *** Validation summary for epoch 18
	 loss={'classification': 0.2835, 'neural_augmentation': 0.0, 'total_loss': 0.2835} || top1={'logits': 95.248} || top5={'logits': 99.2441}
2024-07-29 18:17:44 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:17:44 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:17:48 - [34m[1mLOGS   [0m - Training checkpoint for epoch 18/iteration 4190 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_18_iter_4190.pt
2024-07-29 18:17:48 - [34m[1mLOGS   [0m - Model state for epoch 18/iteration 4190 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_18_iter_4190.pt
[31m===========================================================================[0m
2024-07-29 18:17:50 - [32m[1mINFO   [0m - Training epoch 19
2024-07-29 18:17:51 - [34m[1mLOGS   [0m - Epoch:  19 [    4191/10000000], loss: {'classification': 1.0146, 'neural_augmentation': 0.5227, 'total_loss': 1.5373}, LR: [1.8e-05, 1.8e-05], Avg. batch load time: 0.623, Elapsed time:  0.80
2024-07-29 18:18:31 - [34m[1mLOGS   [0m - *** Training summary for epoch 19
	 loss={'classification': 0.969, 'neural_augmentation': 0.5087, 'total_loss': 1.4777}
2024-07-29 18:18:42 - [34m[1mLOGS   [0m - *** Validation summary for epoch 19
	 loss={'classification': 0.2839, 'neural_augmentation': 0.0, 'total_loss': 0.2839} || top1={'logits': 95.1693} || top5={'logits': 99.2087}
2024-07-29 18:18:43 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:18:44 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:18:51 - [34m[1mLOGS   [0m - Training checkpoint for epoch 19/iteration 4418 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_19_iter_4418.pt
2024-07-29 18:18:53 - [34m[1mLOGS   [0m - Model state for epoch 19/iteration 4418 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_19_iter_4418.pt
[31m===========================================================================[0m
2024-07-29 18:18:55 - [32m[1mINFO   [0m - Training epoch 20
2024-07-29 18:18:57 - [34m[1mLOGS   [0m - Epoch:  20 [    4419/10000000], loss: {'classification': 0.9425, 'neural_augmentation': 0.5538, 'total_loss': 1.4963}, LR: [1.6e-05, 1.6e-05], Avg. batch load time: 1.780, Elapsed time:  1.97
2024-07-29 18:19:38 - [34m[1mLOGS   [0m - *** Training summary for epoch 20
	 loss={'classification': 0.9649, 'neural_augmentation': 0.5502, 'total_loss': 1.5151}
2024-07-29 18:19:48 - [34m[1mLOGS   [0m - *** Validation summary for epoch 20
	 loss={'classification': 0.2858, 'neural_augmentation': 0.0, 'total_loss': 0.2858} || top1={'logits': 95.2087} || top5={'logits': 99.1772}
2024-07-29 18:19:50 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:19:50 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:19:53 - [34m[1mLOGS   [0m - Training checkpoint for epoch 20/iteration 4653 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_20_iter_4653.pt
2024-07-29 18:19:53 - [34m[1mLOGS   [0m - Model state for epoch 20/iteration 4653 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_20_iter_4653.pt
[31m===========================================================================[0m
2024-07-29 18:19:55 - [32m[1mINFO   [0m - Training epoch 21
2024-07-29 18:19:57 - [34m[1mLOGS   [0m - Epoch:  21 [    4654/10000000], loss: {'classification': 0.9137, 'neural_augmentation': 0.6091, 'total_loss': 1.5229}, LR: [1.4e-05, 1.4e-05], Avg. batch load time: 1.089, Elapsed time:  1.27
2024-07-29 18:20:36 - [34m[1mLOGS   [0m - *** Training summary for epoch 21
	 loss={'classification': 0.9605, 'neural_augmentation': 0.5884, 'total_loss': 1.5488}
2024-07-29 18:20:47 - [34m[1mLOGS   [0m - *** Validation summary for epoch 21
	 loss={'classification': 0.2854, 'neural_augmentation': 0.0, 'total_loss': 0.2854} || top1={'logits': 95.1457} || top5={'logits': 99.1929}
2024-07-29 18:20:49 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:20:49 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:20:53 - [34m[1mLOGS   [0m - Training checkpoint for epoch 21/iteration 4876 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_21_iter_4876.pt
2024-07-29 18:20:53 - [34m[1mLOGS   [0m - Model state for epoch 21/iteration 4876 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_21_iter_4876.pt
[31m===========================================================================[0m
2024-07-29 18:20:55 - [32m[1mINFO   [0m - Training epoch 22
2024-07-29 18:20:57 - [34m[1mLOGS   [0m - Epoch:  22 [    4877/10000000], loss: {'classification': 0.927, 'neural_augmentation': 0.6376, 'total_loss': 1.5646}, LR: [1.2e-05, 1.2e-05], Avg. batch load time: 1.392, Elapsed time:  1.57
2024-07-29 18:21:36 - [34m[1mLOGS   [0m - *** Training summary for epoch 22
	 loss={'classification': 0.9608, 'neural_augmentation': 0.627, 'total_loss': 1.5878}
2024-07-29 18:21:48 - [34m[1mLOGS   [0m - *** Validation summary for epoch 22
	 loss={'classification': 0.2848, 'neural_augmentation': 0.0, 'total_loss': 0.2848} || top1={'logits': 95.1693} || top5={'logits': 99.2087}
2024-07-29 18:21:49 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:21:49 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:21:52 - [34m[1mLOGS   [0m - Training checkpoint for epoch 22/iteration 5107 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_22_iter_5107.pt
2024-07-29 18:21:53 - [34m[1mLOGS   [0m - Model state for epoch 22/iteration 5107 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_22_iter_5107.pt
[31m===========================================================================[0m
2024-07-29 18:21:55 - [32m[1mINFO   [0m - Training epoch 23
2024-07-29 18:21:56 - [34m[1mLOGS   [0m - Epoch:  23 [    5108/10000000], loss: {'classification': 1.0345, 'neural_augmentation': 0.6406, 'total_loss': 1.6751}, LR: [1.1e-05, 1.1e-05], Avg. batch load time: 1.607, Elapsed time:  1.80
2024-07-29 18:22:39 - [34m[1mLOGS   [0m - *** Training summary for epoch 23
	 loss={'classification': 0.957, 'neural_augmentation': 0.661, 'total_loss': 1.618}
2024-07-29 18:22:49 - [34m[1mLOGS   [0m - *** Validation summary for epoch 23
	 loss={'classification': 0.2835, 'neural_augmentation': 0.0, 'total_loss': 0.2835} || top1={'logits': 95.248} || top5={'logits': 99.2008}
2024-07-29 18:22:50 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:22:50 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:22:53 - [34m[1mLOGS   [0m - Training checkpoint for epoch 23/iteration 5337 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_23_iter_5337.pt
2024-07-29 18:22:54 - [34m[1mLOGS   [0m - Model state for epoch 23/iteration 5337 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_23_iter_5337.pt
[31m===========================================================================[0m
2024-07-29 18:22:56 - [32m[1mINFO   [0m - Training epoch 24
2024-07-29 18:22:56 - [34m[1mLOGS   [0m - Epoch:  24 [    5338/10000000], loss: {'classification': 0.9981, 'neural_augmentation': 0.7098, 'total_loss': 1.7079}, LR: [9e-06, 9e-06], Avg. batch load time: 0.421, Elapsed time:  0.60
2024-07-29 18:23:37 - [34m[1mLOGS   [0m - *** Training summary for epoch 24
	 loss={'classification': 0.951, 'neural_augmentation': 0.6943, 'total_loss': 1.6453}
2024-07-29 18:23:47 - [34m[1mLOGS   [0m - *** Validation summary for epoch 24
	 loss={'classification': 0.287, 'neural_augmentation': 0.0, 'total_loss': 0.287} || top1={'logits': 95.1417} || top5={'logits': 99.1929}
2024-07-29 18:23:49 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:23:49 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:23:53 - [34m[1mLOGS   [0m - Training checkpoint for epoch 24/iteration 5564 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_24_iter_5564.pt
2024-07-29 18:23:54 - [34m[1mLOGS   [0m - Model state for epoch 24/iteration 5564 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_24_iter_5564.pt
[31m===========================================================================[0m
2024-07-29 18:23:56 - [32m[1mINFO   [0m - Training epoch 25
2024-07-29 18:23:56 - [34m[1mLOGS   [0m - Epoch:  25 [    5565/10000000], loss: {'classification': 0.9113, 'neural_augmentation': 0.7045, 'total_loss': 1.6158}, LR: [8e-06, 8e-06], Avg. batch load time: 0.384, Elapsed time:  0.56
2024-07-29 18:24:33 - [34m[1mLOGS   [0m - *** Training summary for epoch 25
	 loss={'classification': 0.9515, 'neural_augmentation': 0.7221, 'total_loss': 1.6737}
2024-07-29 18:24:44 - [34m[1mLOGS   [0m - *** Validation summary for epoch 25
	 loss={'classification': 0.2849, 'neural_augmentation': 0.0, 'total_loss': 0.2849} || top1={'logits': 95.248} || top5={'logits': 99.2244}
2024-07-29 18:24:46 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:24:48 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:24:51 - [34m[1mLOGS   [0m - Training checkpoint for epoch 25/iteration 5775 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_25_iter_5775.pt
2024-07-29 18:24:51 - [34m[1mLOGS   [0m - Model state for epoch 25/iteration 5775 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_25_iter_5775.pt
[31m===========================================================================[0m
2024-07-29 18:24:53 - [32m[1mINFO   [0m - Training epoch 26
2024-07-29 18:24:54 - [34m[1mLOGS   [0m - Epoch:  26 [    5776/10000000], loss: {'classification': 0.9445, 'neural_augmentation': 0.742, 'total_loss': 1.6865}, LR: [7e-06, 7e-06], Avg. batch load time: 1.035, Elapsed time:  1.21
2024-07-29 18:25:32 - [34m[1mLOGS   [0m - *** Training summary for epoch 26
	 loss={'classification': 0.9481, 'neural_augmentation': 0.7479, 'total_loss': 1.696}
2024-07-29 18:25:42 - [34m[1mLOGS   [0m - *** Validation summary for epoch 26
	 loss={'classification': 0.2838, 'neural_augmentation': 0.0, 'total_loss': 0.2838} || top1={'logits': 95.2953} || top5={'logits': 99.2126}
2024-07-29 18:25:43 - [34m[1mLOGS   [0m - Best checkpoint with score 95.30 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:25:43 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_score_94.9016.pt
2024-07-29 18:25:43 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_95.1181.pt', 'checkpoint_score_95.1850.pt', 'checkpoint_score_95.2756.pt', 'checkpoint_score_95.2835.pt', 'checkpoint_score_95.2953.pt']
2024-07-29 18:25:49 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_avg.pt
2024-07-29 18:25:50 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:25:50 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:25:51 - [34m[1mLOGS   [0m - Training checkpoint for epoch 26/iteration 5988 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_26_iter_5988.pt
2024-07-29 18:25:52 - [34m[1mLOGS   [0m - Model state for epoch 26/iteration 5988 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_26_iter_5988.pt
[31m===========================================================================[0m
2024-07-29 18:25:54 - [32m[1mINFO   [0m - Training epoch 27
2024-07-29 18:25:55 - [34m[1mLOGS   [0m - Epoch:  27 [    5989/10000000], loss: {'classification': 0.8456, 'neural_augmentation': 0.7883, 'total_loss': 1.6338}, LR: [6e-06, 6e-06], Avg. batch load time: 1.092, Elapsed time:  1.27
2024-07-29 18:26:37 - [34m[1mLOGS   [0m - *** Training summary for epoch 27
	 loss={'classification': 0.9419, 'neural_augmentation': 0.7685, 'total_loss': 1.7104}
2024-07-29 18:26:48 - [34m[1mLOGS   [0m - *** Validation summary for epoch 27
	 loss={'classification': 0.2852, 'neural_augmentation': 0.0, 'total_loss': 0.2852} || top1={'logits': 95.3071} || top5={'logits': 99.2008}
2024-07-29 18:26:49 - [34m[1mLOGS   [0m - Best checkpoint with score 95.31 saved at /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_best.pt
2024-07-29 18:26:49 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_score_95.1181.pt
2024-07-29 18:26:49 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_95.1850.pt', 'checkpoint_score_95.2756.pt', 'checkpoint_score_95.2835.pt', 'checkpoint_score_95.2953.pt', 'checkpoint_score_95.3071.pt']
2024-07-29 18:26:56 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_avg.pt
2024-07-29 18:26:57 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:26:58 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:26:59 - [34m[1mLOGS   [0m - Training checkpoint for epoch 27/iteration 6228 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_27_iter_6228.pt
2024-07-29 18:26:59 - [34m[1mLOGS   [0m - Model state for epoch 27/iteration 6228 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_27_iter_6228.pt
[31m===========================================================================[0m
2024-07-29 18:27:01 - [32m[1mINFO   [0m - Training epoch 28
2024-07-29 18:27:03 - [34m[1mLOGS   [0m - Epoch:  28 [    6229/10000000], loss: {'classification': 0.9685, 'neural_augmentation': 0.7819, 'total_loss': 1.7503}, LR: [5e-06, 5e-06], Avg. batch load time: 2.083, Elapsed time:  2.26
2024-07-29 18:27:42 - [34m[1mLOGS   [0m - *** Training summary for epoch 28
	 loss={'classification': 0.9403, 'neural_augmentation': 0.7846, 'total_loss': 1.7248}
2024-07-29 18:27:52 - [34m[1mLOGS   [0m - *** Validation summary for epoch 28
	 loss={'classification': 0.2866, 'neural_augmentation': 0.0, 'total_loss': 0.2866} || top1={'logits': 95.2638} || top5={'logits': 99.1929}
2024-07-29 18:28:02 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:28:03 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:28:05 - [34m[1mLOGS   [0m - Training checkpoint for epoch 28/iteration 6449 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_28_iter_6449.pt
2024-07-29 18:28:06 - [34m[1mLOGS   [0m - Model state for epoch 28/iteration 6449 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_28_iter_6449.pt
[31m===========================================================================[0m
2024-07-29 18:28:08 - [32m[1mINFO   [0m - Training epoch 29
2024-07-29 18:28:08 - [34m[1mLOGS   [0m - Epoch:  29 [    6450/10000000], loss: {'classification': 0.9361, 'neural_augmentation': 0.8023, 'total_loss': 1.7384}, LR: [5e-06, 5e-06], Avg. batch load time: 0.395, Elapsed time:  0.58
2024-07-29 18:28:49 - [34m[1mLOGS   [0m - *** Training summary for epoch 29
	 loss={'classification': 0.944, 'neural_augmentation': 0.7979, 'total_loss': 1.7419}
2024-07-29 18:28:59 - [34m[1mLOGS   [0m - *** Validation summary for epoch 29
	 loss={'classification': 0.2852, 'neural_augmentation': 0.0, 'total_loss': 0.2852} || top1={'logits': 95.3031} || top5={'logits': 99.2008}
2024-07-29 18:29:02 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_last.pt
2024-07-29 18:29:02 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_last.pt
2024-07-29 18:29:04 - [34m[1mLOGS   [0m - Training checkpoint for epoch 29/iteration 6678 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/training_checkpoint_epoch_29_iter_6678.pt
2024-07-29 18:29:04 - [34m[1mLOGS   [0m - Model state for epoch 29/iteration 6678 is saved at: /ML-A100/team/mm/models/catlip_data/results_base_dci/9_food101/train/checkpoint_epoch_29_iter_6678.pt
2024-07-29 18:29:04 - [34m[1mLOGS   [0m - Training took 00:37:14.83
