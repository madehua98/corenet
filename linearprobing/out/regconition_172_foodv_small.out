nohup: ignoring input
2024-07-29 12:39:39 - [93m[1mDEBUG   [0m - Cannot load internal arguments, skipping.
small
dci
2024-07-29 12:39:40 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/catlip_data/results_small_dci/train/checkpoint_epoch_9_iter_79046.pt
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: neural_augmentor
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: patch_embed
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: pos_drop
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: patch_drop
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: norm_pre
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: blocks
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: pool
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: blocks1
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: norm
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: mlp
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing module: fc_norm
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: pos_embed
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.brightness._low
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.brightness._high
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.contrast._low
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.contrast._high
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.noise._low
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.noise._high
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.conv1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.conv1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.conv2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.conv2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.pre_norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.pre_norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv1_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv1_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv2_kxk.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv2_kxk.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv3_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv3_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.pre_norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.pre_norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv1_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv1_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv2_kxk.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv2_kxk.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv3_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv3_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.shortcut.expand.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.shortcut.expand.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.pre_norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.pre_norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv1_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv1_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv2_kxk.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv2_kxk.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv3_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv3_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.pre_norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.pre_norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv1_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv1_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv2_kxk.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv2_kxk.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv3_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv3_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.pre_norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.pre_norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv1_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv1_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv2_kxk.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv2_kxk.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv3_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv3_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.pre_norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.pre_norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv1_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv1_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv2_kxk.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv2_kxk.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv3_1x1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv3_1x1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.pool.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.pool.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.pool.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.pool.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: pool.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: pool.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: pool.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: pool.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.norm1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.norm1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.attn.qkv.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.attn.qkv.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.attn.proj.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.attn.proj.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.norm2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.norm2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w1.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w1.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: mlp.0.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: mlp.0.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: mlp.2.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: mlp.2.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: fc_norm.weight
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Freezing parameter: fc_norm.bias
2024-07-29 12:39:40 - [32m[1mINFO   [0m - Trainable parameters: ['classifier.weight', 'classifier.bias']
2024-07-29 12:39:40 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=172, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =   25.743 M
Total trainable parameters =    0.088 M

2024-07-29 12:39:40 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-29 12:39:40 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 25.743M                | 3.385G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  0.116G    |
|   pool.proj                          |   1.18M                |   0.116G   |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.502M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  51.38M    |
|   mlp.0                              |   0.263M               |   25.69M   |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   25.69M   |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  88.236K               |  88.064K   |
|   classifier.weight                  |   (172, 512)           |            |
|   classifier.bias                    |   (172,)               |            |
2024-07-29 12:39:41 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-29 12:39:41 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks1.0.attn.attn_drop', 'blocks1.1.attn.attn_drop', 'blocks.2.attn.k_norm', 'blocks.3.drop_path1', 'blocks1.3.attn.attn_drop', 'blocks.3.attn.k_norm', 'blocks.5.ls2', 'blocks.4.drop_path1', 'patch_embed.backbone.stages.1.1.drop_path', 'patch_embed.proj', 'patch_drop', 'blocks.4.attn.q_norm', 'blocks1.5.attn.k_norm', 'blocks.0.attn.k_norm', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'blocks.6.attn.attn_drop', 'patch_embed.backbone.stages.1.3.down', 'blocks1.5.ls1', 'neural_augmentor.contrast.max_fn', 'blocks.0.drop_path1', 'neural_augmentor', 'blocks.2.attn.attn_drop', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks1.3.ls2', 'patch_embed.backbone.stages.1.2.shortcut', 'blocks1.6.drop_path2', 'blocks.5.ls1', 'blocks1.0.ls1', 'neural_augmentor.contrast.min_fn', 'neural_augmentor.noise.min_fn', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks1.4.ls1', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks1.3.drop_path1', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks.0.ls2', 'blocks1.6.ls1', 'blocks1.4.drop_path2', 'blocks.4.ls1', 'blocks1.3.drop_path2', 'blocks.2.ls2', 'blocks1.4.ls2', 'blocks.2.drop_path1', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'blocks1.2.ls1', 'blocks.1.attn.attn_drop', 'blocks1.2.drop_path2', 'blocks1.5.drop_path2', 'blocks.4.ls2', 'blocks.0.ls1', 'blocks.6.drop_path1', 'blocks.3.attn.q_norm', 'neural_augmentor.brightness.min_fn', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks1.4.attn.q_norm', 'blocks1.2.attn.attn_drop', 'blocks1.2.drop_path1', 'patch_embed.backbone.stages.0.0.down', 'patch_embed.backbone.stages.1.1.down', 'blocks1.1.drop_path1', 'norm_pre', 'blocks1.6.attn.attn_drop', 'blocks1.3.attn.k_norm', 'blocks1.4.attn.attn_drop', 'blocks.5.drop_path2', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks.1.ls1', 'neural_augmentor.contrast', 'blocks.3.drop_path2', 'blocks.4.attn.attn_drop', 'patch_embed.backbone.stages.0.1.down', 'patch_embed.backbone.stages.1.0.down', 'blocks.0.attn.q_norm', 'blocks1.4.drop_path1', 'blocks.6.ls2', 'blocks1.0.ls2', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'blocks.5.attn.k_norm', 'neural_augmentor.noise', 'patch_embed.backbone.stages.0.0.drop_path', 'blocks1.0.attn.q_norm', 'blocks1.3.ls1', 'blocks.4.drop_path2', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks.5.drop_path1', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks1.5.drop_path1', 'blocks1.3.attn.q_norm', 'neural_augmentor.brightness.max_fn', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'blocks.2.drop_path2', 'blocks1.2.ls2', 'blocks.3.ls1', 'blocks1.2.attn.q_norm', 'blocks1.6.ls2', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks.3.ls2', 'blocks1.1.attn.q_norm', 'blocks.1.attn.q_norm', 'blocks1.6.attn.k_norm', 'blocks1.1.ls1', 'blocks.2.ls1', 'blocks1.2.attn.k_norm', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks.6.attn.k_norm', 'blocks.6.drop_path2', 'blocks1.5.ls2', 'blocks1.6.drop_path1', 'blocks.5.attn.q_norm', 'norm', 'blocks.2.attn.q_norm', 'blocks.1.ls2', 'blocks.1.drop_path1', 'blocks.4.attn.k_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks1.1.ls2', 'blocks1.0.attn.k_norm', 'blocks.3.attn.attn_drop', 'blocks.6.ls1', 'blocks1.1.drop_path2', 'blocks1.4.attn.k_norm', 'blocks.6.attn.q_norm', 'blocks.0.drop_path2', 'blocks1.6.attn.q_norm', 'patch_embed.backbone.stem.norm1.drop', 'blocks1.5.attn.attn_drop', 'neural_augmentor.brightness', 'blocks.1.drop_path2', 'blocks.1.attn.k_norm', 'blocks1.5.attn.q_norm', 'patch_embed.backbone.stages.0.1.shortcut', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks.0.attn.attn_drop', 'neural_augmentor.noise.max_fn', 'blocks1.1.attn.k_norm', 'blocks1.0.drop_path1', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks.5.attn.attn_drop', 'blocks1.0.drop_path2', 'patch_embed.backbone.stages.1.2.down'}
2024-07-29 12:39:41 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::add_': 14, 'aten::avg_pool2d': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-29 12:39:41 - [34m[1mLOGS   [0m - Random seeds are set to 0
2024-07-29 12:39:41 - [34m[1mLOGS   [0m - Using PyTorch version 2.2.1+cu121
2024-07-29 12:39:41 - [34m[1mLOGS   [0m - Available GPUs: 4
2024-07-29 12:39:41 - [34m[1mLOGS   [0m - CUDNN is enabled
2024-07-29 12:39:41 - [34m[1mLOGS   [0m - Setting --ddp.world-size the same as the number of available gpus.
2024-07-29 12:39:41 - [34m[1mLOGS   [0m - Directory created at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train
2024-07-29 12:39:45 - [32m[1mINFO   [0m - distributed init (rank 0): tcp://localhost:40002
2024-07-29 12:39:49 - [34m[1mLOGS   [0m - Number of categories: 172
2024-07-29 12:39:49 - [34m[1mLOGS   [0m - Total number of samples: 66071
2024-07-29 12:39:49 - [34m[1mLOGS   [0m - Using all samples in the dataset.
2024-07-29 12:39:49 - [34m[1mLOGS   [0m - Training dataset details are given below
ImageNetDataset(
	root=/ML-A100/team/mm/models/food172/food_172/train_images 
	is_training=True 
	num_samples=66071
	transforms=Compose(
			RandomResizedCrop(scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), size=(224, 224), interpolation=bilinear), 
			RandomHorizontalFlip(p=0.5), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	 num_classes=172
)
2024-07-29 12:39:50 - [34m[1mLOGS   [0m - Number of categories: 172
2024-07-29 12:39:50 - [34m[1mLOGS   [0m - Total number of samples: 44170
2024-07-29 12:39:50 - [34m[1mLOGS   [0m - Using all samples in the dataset.
2024-07-29 12:39:50 - [34m[1mLOGS   [0m - Validation dataset details are given below
ImageNetDataset(
	root=/ML-A100/team/mm/models/food172/food_172/test_images 
	is_training=False 
	num_samples=44170
	transforms=Compose(
			Resize(size=232, interpolation=bilinear, maintain_aspect_ratio=True), 
			CenterCrop(size=(h=224, w=224)), 
			ToTensor(dtype=torch.float32, norm_factor=255)
		)
	 num_classes=172
)
2024-07-29 12:39:50 - [34m[1mLOGS   [0m - Training sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=128
	 scales=[(128, 128, 392), (160, 160, 250), (192, 192, 174), (224, 224, 128), (256, 256, 98), (288, 288, 77), (320, 320, 62)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-29 12:39:50 - [34m[1mLOGS   [0m - Validation sampler details: VariableBatchSamplerDDP(
	 num_repeat=1
	 trunc_rep_aug=False
	 sharding=False
	 disable_shuffle_sharding=False
	 base_im_size=(h=224, w=224)
	 base_batch_size=100
	 scales=[(224, 224, 100)]
	 scale_inc=False
	 min_scale_inc_factor=1.0
	 max_scale_inc_factor=1.0
	 ep_intervals=[40]
)
2024-07-29 12:39:50 - [34m[1mLOGS   [0m - Number of data workers: 64
small
dci
2024-07-29 12:39:51 - [34m[1mLOGS   [0m - Pretrained weights are loaded from /ML-A100/team/mm/models/catlip_data/results_small_dci/train/checkpoint_epoch_9_iter_79046.pt
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: neural_augmentor
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: patch_embed
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: pos_drop
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: patch_drop
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: norm_pre
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: blocks
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: pool
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: blocks1
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: norm
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: mlp
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing module: fc_norm
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: pos_embed
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.brightness._low
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.brightness._high
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.contrast._low
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.contrast._high
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.noise._low
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: neural_augmentor.noise._high
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.conv1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.conv1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.conv2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stem.conv2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.pre_norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.pre_norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv1_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv1_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv2_kxk.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv2_kxk.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv3_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.0.conv3_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.pre_norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.pre_norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv1_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv1_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv2_kxk.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv2_kxk.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv3_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.0.1.conv3_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.shortcut.expand.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.shortcut.expand.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.pre_norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.pre_norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv1_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv1_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv2_kxk.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv2_kxk.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv3_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.0.conv3_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.pre_norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.pre_norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv1_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv1_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv2_kxk.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv2_kxk.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv3_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.1.conv3_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.pre_norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.pre_norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv1_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv1_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv2_kxk.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv2_kxk.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv3_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.2.conv3_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.pre_norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.pre_norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv1_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv1_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv2_kxk.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv2_kxk.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv3_1x1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.stages.1.3.conv3_1x1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.pool.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.pool.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.pool.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: patch_embed.backbone.pool.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.0.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.1.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.2.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.3.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.4.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.5.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks.6.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: pool.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: pool.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: pool.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: pool.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.0.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.1.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.2.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.3.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.4.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.5.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.norm1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.norm1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.attn.qkv.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.attn.qkv.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.attn.proj.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.attn.proj.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.norm2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.norm2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w1.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w1.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: blocks1.6.mlp.w2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: mlp.0.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: mlp.0.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: mlp.2.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: mlp.2.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: fc_norm.weight
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Freezing parameter: fc_norm.bias
2024-07-29 12:39:51 - [32m[1mINFO   [0m - Trainable parameters: ['classifier.weight', 'classifier.bias']
2024-07-29 12:39:51 - [34m[1mLOGS   [0m - [36mModel[0m
Foodv(
  (neural_augmentor): DistributionNeuralAugmentor(
  	Brightness=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Contrast=UniformSampler(min_fn=Clip(min=0.1, max=0.9, clipping=soft), max_fn=Clip(min=1.1, max=10.0, clipping=soft)), 
  	Noise=UniformSampler(min_fn=Clip(min=0.0, max=5e-05, clipping=soft), max_fn=Clip(min=0.0001, max=1.0, clipping=soft)), )
  (patch_embed): HybridEmbed(
    (backbone): MbConvStages(
      (stem): Stem(
        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm1): LayerNormAct2d(
          (64,), eps=1e-06, elementwise_affine=True
          (drop): Identity()
          (act): GELU()
        )
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (stages): ModuleList(
        (0): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Identity()
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
            (conv3_1x1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
        (1): Sequential(
          (0): MbConvLNBlock(
            (shortcut): Downsample2d(
              (pool): AvgPool2d(kernel_size=3, stride=2, padding=1)
              (expand): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (pre_norm): LayerNormAct2d(
              (64,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (1): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (2): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
          (3): MbConvLNBlock(
            (shortcut): Identity()
            (pre_norm): LayerNormAct2d(
              (128,), eps=1e-06, elementwise_affine=True
              (drop): Identity()
              (act): Identity()
            )
            (down): Identity()
            (conv1_1x1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
            (act1): GELU()
            (act2): GELU()
            (conv2_kxk): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
            (conv3_1x1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
            (drop_path): Identity()
          )
        )
      )
      (pool): StridedConv(
        (proj): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      )
    )
    (proj): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (patch_drop): Identity()
  (norm_pre): Identity()
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=256, out_features=768, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=256, out_features=256, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=256, out_features=512, bias=True)
        (w1): Linear(in_features=256, out_features=512, bias=True)
        (w2): Linear(in_features=512, out_features=256, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (pool): StridedConv(
    (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)
  )
  (blocks1): Sequential(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (2): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (3): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (4): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (5): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
    (6): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=512, out_features=1536, bias=True)
        (q_norm): Identity()
        (k_norm): Identity()
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): Identity()
      (drop_path1): Identity()
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): GeGluMlp(
        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (act): GELU(approximate='none')
        (w0): Linear(in_features=512, out_features=1024, bias=True)
        (w1): Linear(in_features=512, out_features=1024, bias=True)
        (w2): Linear(in_features=1024, out_features=512, bias=True)
      )
      (ls2): Identity()
      (drop_path2): Identity()
    )
  )
  (norm): Identity()
  (mlp): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): GELU(approximate='none')
    (2): Linear(in_features=512, out_features=512, bias=True)
  )
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (classifier_drop): Dropout(p=0.0, inplace=False)
  (classifier): LinearLayer(in_features=512, out_features=172, bias=True, channel_first=False)
)
[31m=================================================================[0m
                              Foodv Summary
[31m=================================================================[0m
Total parameters     =   25.743 M
Total trainable parameters =    0.088 M

2024-07-29 12:39:51 - [34m[1mLOGS   [0m - FVCore Analysis:
2024-07-29 12:39:51 - [34m[1mLOGS   [0m - Input sizes: [1, 3, 224, 224]
| module                               | #parameters or shape   | #flops     |
|:-------------------------------------|:-----------------------|:-----------|
| model                                | 25.743M                | 3.385G     |
|  pos_embed                           |  (1, 1, 256)           |            |
|  neural_augmentor                    |  6                     |            |
|   neural_augmentor.brightness        |   2                    |            |
|    neural_augmentor.brightness._low  |    ()                  |            |
|    neural_augmentor.brightness._high |    ()                  |            |
|   neural_augmentor.contrast          |   2                    |            |
|    neural_augmentor.contrast._low    |    ()                  |            |
|    neural_augmentor.contrast._high   |    ()                  |            |
|   neural_augmentor.noise             |   2                    |            |
|    neural_augmentor.noise._low       |    ()                  |            |
|    neural_augmentor.noise._high      |    ()                  |            |
|  patch_embed.backbone                |  0.93M                 |  1.411G    |
|   patch_embed.backbone.stem          |   38.848K              |   0.488G   |
|    patch_embed.backbone.stem.conv1   |    1.792K              |    21.676M |
|    patch_embed.backbone.stem.norm1   |    0.128K              |    4.014M  |
|    patch_embed.backbone.stem.conv2   |    36.928K             |    0.462G  |
|   patch_embed.backbone.stages        |   0.595M               |   0.865G   |
|    patch_embed.backbone.stages.0     |    71.552K             |    0.379G  |
|    patch_embed.backbone.stages.1     |    0.524M              |    0.486G  |
|   patch_embed.backbone.pool          |   0.295M               |   58.305M  |
|    patch_embed.backbone.pool.proj    |    0.295M              |    57.803M |
|    patch_embed.backbone.pool.norm    |    0.256K              |    0.502M  |
|  blocks                              |  4.614M                |  0.904G    |
|   blocks.0                           |   0.659M               |   0.129G   |
|    blocks.0.norm1                    |    0.512K              |    0.251M  |
|    blocks.0.attn                     |    0.263M              |    51.38M  |
|    blocks.0.norm2                    |    0.512K              |    0.251M  |
|    blocks.0.mlp                      |    0.395M              |    77.321M |
|   blocks.1                           |   0.659M               |   0.129G   |
|    blocks.1.norm1                    |    0.512K              |    0.251M  |
|    blocks.1.attn                     |    0.263M              |    51.38M  |
|    blocks.1.norm2                    |    0.512K              |    0.251M  |
|    blocks.1.mlp                      |    0.395M              |    77.321M |
|   blocks.2                           |   0.659M               |   0.129G   |
|    blocks.2.norm1                    |    0.512K              |    0.251M  |
|    blocks.2.attn                     |    0.263M              |    51.38M  |
|    blocks.2.norm2                    |    0.512K              |    0.251M  |
|    blocks.2.mlp                      |    0.395M              |    77.321M |
|   blocks.3                           |   0.659M               |   0.129G   |
|    blocks.3.norm1                    |    0.512K              |    0.251M  |
|    blocks.3.attn                     |    0.263M              |    51.38M  |
|    blocks.3.norm2                    |    0.512K              |    0.251M  |
|    blocks.3.mlp                      |    0.395M              |    77.321M |
|   blocks.4                           |   0.659M               |   0.129G   |
|    blocks.4.norm1                    |    0.512K              |    0.251M  |
|    blocks.4.attn                     |    0.263M              |    51.38M  |
|    blocks.4.norm2                    |    0.512K              |    0.251M  |
|    blocks.4.mlp                      |    0.395M              |    77.321M |
|   blocks.5                           |   0.659M               |   0.129G   |
|    blocks.5.norm1                    |    0.512K              |    0.251M  |
|    blocks.5.attn                     |    0.263M              |    51.38M  |
|    blocks.5.norm2                    |    0.512K              |    0.251M  |
|    blocks.5.mlp                      |    0.395M              |    77.321M |
|   blocks.6                           |   0.659M               |   0.129G   |
|    blocks.6.norm1                    |    0.512K              |    0.251M  |
|    blocks.6.attn                     |    0.263M              |    51.38M  |
|    blocks.6.norm2                    |    0.512K              |    0.251M  |
|    blocks.6.mlp                      |    0.395M              |    77.321M |
|  pool                                |  1.181M                |  0.116G    |
|   pool.proj                          |   1.18M                |   0.116G   |
|    pool.proj.weight                  |    (512, 256, 3, 3)    |            |
|    pool.proj.bias                    |    (512,)              |            |
|   pool.norm                          |   0.512K               |   0.502M   |
|    pool.norm.weight                  |    (256,)              |            |
|    pool.norm.bias                    |    (256,)              |            |
|  blocks1                             |  18.404M               |  0.902G    |
|   blocks1.0                          |   2.629M               |   0.129G   |
|    blocks1.0.norm1                   |    1.024K              |    0.125M  |
|    blocks1.0.attn                    |    1.051M              |    51.38M  |
|    blocks1.0.norm2                   |    1.024K              |    0.125M  |
|    blocks1.0.mlp                     |    1.576M              |    77.196M |
|   blocks1.1                          |   2.629M               |   0.129G   |
|    blocks1.1.norm1                   |    1.024K              |    0.125M  |
|    blocks1.1.attn                    |    1.051M              |    51.38M  |
|    blocks1.1.norm2                   |    1.024K              |    0.125M  |
|    blocks1.1.mlp                     |    1.576M              |    77.196M |
|   blocks1.2                          |   2.629M               |   0.129G   |
|    blocks1.2.norm1                   |    1.024K              |    0.125M  |
|    blocks1.2.attn                    |    1.051M              |    51.38M  |
|    blocks1.2.norm2                   |    1.024K              |    0.125M  |
|    blocks1.2.mlp                     |    1.576M              |    77.196M |
|   blocks1.3                          |   2.629M               |   0.129G   |
|    blocks1.3.norm1                   |    1.024K              |    0.125M  |
|    blocks1.3.attn                    |    1.051M              |    51.38M  |
|    blocks1.3.norm2                   |    1.024K              |    0.125M  |
|    blocks1.3.mlp                     |    1.576M              |    77.196M |
|   blocks1.4                          |   2.629M               |   0.129G   |
|    blocks1.4.norm1                   |    1.024K              |    0.125M  |
|    blocks1.4.attn                    |    1.051M              |    51.38M  |
|    blocks1.4.norm2                   |    1.024K              |    0.125M  |
|    blocks1.4.mlp                     |    1.576M              |    77.196M |
|   blocks1.5                          |   2.629M               |   0.129G   |
|    blocks1.5.norm1                   |    1.024K              |    0.125M  |
|    blocks1.5.attn                    |    1.051M              |    51.38M  |
|    blocks1.5.norm2                   |    1.024K              |    0.125M  |
|    blocks1.5.mlp                     |    1.576M              |    77.196M |
|   blocks1.6                          |   2.629M               |   0.129G   |
|    blocks1.6.norm1                   |    1.024K              |    0.125M  |
|    blocks1.6.attn                    |    1.051M              |    51.38M  |
|    blocks1.6.norm2                   |    1.024K              |    0.125M  |
|    blocks1.6.mlp                     |    1.576M              |    77.196M |
|  mlp                                 |  0.525M                |  51.38M    |
|   mlp.0                              |   0.263M               |   25.69M   |
|    mlp.0.weight                      |    (512, 512)          |            |
|    mlp.0.bias                        |    (512,)              |            |
|   mlp.2                              |   0.263M               |   25.69M   |
|    mlp.2.weight                      |    (512, 512)          |            |
|    mlp.2.bias                        |    (512,)              |            |
|  fc_norm                             |  1.024K                |  2.56K     |
|   fc_norm.weight                     |   (512,)               |            |
|   fc_norm.bias                       |   (512,)               |            |
|  classifier                          |  88.236K               |  88.064K   |
|   classifier.weight                  |   (172, 512)           |            |
|   classifier.bias                    |   (172,)               |            |
2024-07-29 12:39:52 - [33m[1mWARNING[0m - 
** Please be cautious when using the results in papers. Certain operations may or may not be accounted in FLOP computation in FVCore. Therefore, you want to manually ensure that FLOP computation is correct.
2024-07-29 12:39:52 - [33m[1mWARNING[0m - Uncalled Modules:
{'blocks1.3.ls2', 'blocks.1.attn.k_norm', 'blocks.2.attn.k_norm', 'blocks.0.attn.attn_drop', 'blocks.6.attn.k_norm', 'patch_embed.backbone.stages.1.0.pre_norm.drop', 'norm', 'blocks1.6.drop_path2', 'patch_embed.backbone.stages.1.2.down', 'blocks1.4.drop_path2', 'blocks1.1.attn.k_norm', 'patch_embed.backbone.stages.1.0.drop_path', 'blocks.2.drop_path2', 'neural_augmentor.noise.min_fn', 'patch_embed.backbone.stages.1.3.drop_path', 'blocks.5.attn.k_norm', 'blocks.5.drop_path2', 'blocks.0.drop_path2', 'blocks1.3.attn.k_norm', 'blocks1.2.ls2', 'blocks.1.attn.attn_drop', 'neural_augmentor.contrast.max_fn', 'blocks1.0.attn.attn_drop', 'blocks1.3.ls1', 'blocks1.0.drop_path1', 'blocks1.3.drop_path2', 'blocks.0.ls2', 'patch_embed.backbone.stages.1.2.drop_path', 'blocks.2.ls2', 'patch_drop', 'blocks1.4.ls2', 'blocks.5.attn.attn_drop', 'blocks.6.ls2', 'patch_embed.backbone.stages.0.0.shortcut.expand', 'neural_augmentor.noise.max_fn', 'blocks1.4.ls1', 'blocks.6.ls1', 'blocks.0.ls1', 'blocks.4.attn.attn_drop', 'patch_embed.backbone.stages.0.1.pre_norm.act', 'blocks1.0.attn.q_norm', 'blocks.3.ls1', 'blocks1.1.attn.q_norm', 'blocks.4.drop_path1', 'blocks1.2.drop_path1', 'blocks.1.ls2', 'neural_augmentor.brightness.min_fn', 'blocks.1.drop_path2', 'blocks.3.attn.attn_drop', 'blocks1.1.ls1', 'blocks1.6.attn.attn_drop', 'blocks1.6.ls2', 'patch_embed.backbone.stages.1.3.down', 'blocks1.5.ls1', 'blocks1.1.drop_path2', 'patch_embed.backbone.stages.0.1.pre_norm.drop', 'blocks.6.attn.attn_drop', 'patch_embed.backbone.stages.1.3.pre_norm.drop', 'patch_embed.backbone.stages.1.1.pre_norm.drop', 'blocks1.6.drop_path1', 'blocks1.4.attn.q_norm', 'blocks.3.drop_path1', 'patch_embed.backbone.stages.1.1.down', 'blocks.5.attn.q_norm', 'patch_embed.backbone.stages.0.0.pre_norm.drop', 'neural_augmentor', 'blocks1.0.ls1', 'blocks1.2.attn.q_norm', 'patch_embed.backbone.stages.1.3.shortcut', 'blocks.5.ls1', 'neural_augmentor.noise', 'neural_augmentor.brightness', 'patch_embed.backbone.stages.1.2.pre_norm.drop', 'blocks.2.ls1', 'blocks.4.attn.k_norm', 'blocks.2.attn.q_norm', 'blocks.1.ls1', 'blocks.3.ls2', 'blocks.5.ls2', 'blocks.3.drop_path2', 'blocks.4.ls2', 'blocks.4.drop_path2', 'patch_embed.backbone.stages.1.1.drop_path', 'blocks1.5.attn.k_norm', 'blocks.2.attn.attn_drop', 'blocks.6.drop_path2', 'patch_embed.backbone.stages.1.0.pre_norm.act', 'blocks1.6.attn.k_norm', 'patch_embed.backbone.stages.0.1.drop_path', 'blocks.2.drop_path1', 'blocks1.0.ls2', 'patch_embed.backbone.stages.1.3.pre_norm.act', 'blocks.5.drop_path1', 'patch_embed.backbone.stages.1.1.pre_norm.act', 'blocks1.5.attn.attn_drop', 'blocks1.5.ls2', 'blocks.3.attn.k_norm', 'neural_augmentor.brightness.max_fn', 'blocks.4.ls1', 'patch_embed.backbone.stages.1.2.pre_norm.act', 'blocks1.3.drop_path1', 'blocks1.5.drop_path1', 'blocks1.3.attn.attn_drop', 'blocks1.2.attn.attn_drop', 'blocks1.5.drop_path2', 'blocks1.0.drop_path2', 'blocks.1.drop_path1', 'blocks.3.attn.q_norm', 'blocks.0.drop_path1', 'patch_embed.backbone.stages.1.1.shortcut', 'blocks.0.attn.k_norm', 'blocks1.2.attn.k_norm', 'patch_embed.backbone.stages.0.1.down', 'patch_embed.proj', 'blocks1.6.attn.q_norm', 'patch_embed.backbone.stages.0.0.drop_path', 'patch_embed.backbone.stages.1.0.down', 'blocks.1.attn.q_norm', 'patch_embed.backbone.stages.0.0.pre_norm.act', 'patch_embed.backbone.stages.0.1.shortcut', 'blocks1.4.attn.k_norm', 'blocks1.6.ls1', 'blocks.0.attn.q_norm', 'blocks1.2.ls1', 'blocks1.2.drop_path2', 'blocks1.5.attn.q_norm', 'blocks1.1.drop_path1', 'blocks.6.attn.q_norm', 'blocks.6.drop_path1', 'blocks1.4.drop_path1', 'blocks.4.attn.q_norm', 'norm_pre', 'blocks1.3.attn.q_norm', 'blocks1.4.attn.attn_drop', 'neural_augmentor.contrast', 'patch_embed.backbone.stages.1.2.shortcut', 'patch_embed.backbone.stem.norm1.drop', 'blocks1.1.ls2', 'patch_embed.backbone.stages.0.0.down', 'neural_augmentor.contrast.min_fn', 'blocks1.1.attn.attn_drop', 'blocks1.0.attn.k_norm'}
2024-07-29 12:39:52 - [33m[1mWARNING[0m - Unsupported Ops:
Counter({'aten::add': 35, 'aten::gelu': 28, 'aten::scaled_dot_product_attention': 14, 'aten::mul': 14, 'aten::add_': 14, 'aten::avg_pool2d': 2, 'aten::div': 2, 'aten::mean': 1})
[31m=================================================================[0m
2024-07-29 12:39:52 - [34m[1mLOGS   [0m - Using DistributedDataParallel.
2024-07-29 12:39:52 - [34m[1mLOGS   [0m - [36mLoss function[0m
CompositeLoss(
	CrossEntropy(  ignore_idx=-1  class_weighting=False  label_smoothing=0.1 loss_wt=1.0)
	NeuralAugmentation(  target_metric=psnr  target_value=[40, 20]  curriculum_learning=True  alpha=0.0015378700499807767 loss_wt=1.0)
	
)
2024-07-29 12:39:52 - [34m[1mLOGS   [0m - [36mOptimizer[0m
2024-07-29 12:39:52 - [34m[1mLOGS   [0m - Max. epochs for training: 30
2024-07-29 12:39:52 - [34m[1mLOGS   [0m - [36mLearning rate scheduler[0m
CosineScheduler(
 	 min_lr=5e-05
 	 max_lr=0.0005
 	 period=30
 	 warmup_init_lr=1e-05
 	 warmup_iters=500
 )
2024-07-29 12:39:52 - [34m[1mLOGS   [0m - No checkpoint found at '/ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt'
2024-07-29 12:39:52 - [32m[1mINFO   [0m - Configuration file is stored here: [36m/ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/config.yaml[0m
[31m===========================================================================[0m
2024-07-29 12:39:54 - [32m[1mINFO   [0m - Training epoch 0
2024-07-29 12:39:45 - [32m[1mINFO   [0m - distributed init (rank 2): tcp://localhost:40002
small
dci
2024-07-29 12:39:45 - [32m[1mINFO   [0m - distributed init (rank 1): tcp://localhost:40002
small
dci
2024-07-29 12:39:45 - [32m[1mINFO   [0m - distributed init (rank 3): tcp://localhost:40002
small
dci
2024-07-29 12:42:42 - [34m[1mLOGS   [0m - Epoch:   0 [       1/10000000], loss: {'classification': 7.0389, 'neural_augmentation': 0.3239, 'total_loss': 7.3628}, LR: [1e-05, 1e-05], Avg. batch load time: 165.150, Elapsed time: 167.99
2024-07-29 12:42:52 - [34m[1mLOGS   [0m - *** Training summary for epoch 0
	 loss={'classification': 6.2001, 'neural_augmentation': 0.34, 'total_loss': 6.5401}
2024-07-29 12:45:44 - [34m[1mLOGS   [0m - *** Validation summary for epoch 0
	 loss={'classification': 4.9386, 'neural_augmentation': 0.0, 'total_loss': 4.9386} || top1={'logits': 3.491} || top5={'logits': 13.2387}
2024-07-29 12:45:45 - [34m[1mLOGS   [0m - Best checkpoint with score 3.49 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:45:45 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:45:45 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:45:45 - [34m[1mLOGS   [0m - Training checkpoint for epoch 0/iteration 103 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_0_iter_103.pt
2024-07-29 12:45:45 - [34m[1mLOGS   [0m - Model state for epoch 0/iteration 103 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_0_iter_103.pt
[31m===========================================================================[0m
2024-07-29 12:45:47 - [32m[1mINFO   [0m - Training epoch 1
2024-07-29 12:45:51 - [34m[1mLOGS   [0m - Epoch:   1 [     104/10000000], loss: {'classification': 5.0697, 'neural_augmentation': 0.3205, 'total_loss': 5.3902}, LR: [0.000111, 0.000111], Avg. batch load time: 3.288, Elapsed time:  3.35
2024-07-29 12:45:57 - [34m[1mLOGS   [0m - *** Training summary for epoch 1
	 loss={'classification': 4.0802, 'neural_augmentation': 0.3371, 'total_loss': 4.4174}
2024-07-29 12:46:08 - [34m[1mLOGS   [0m - *** Validation summary for epoch 1
	 loss={'classification': 2.6899, 'neural_augmentation': 0.0, 'total_loss': 2.6899} || top1={'logits': 48.2703} || top5={'logits': 75.1689}
2024-07-29 12:46:08 - [34m[1mLOGS   [0m - Best checkpoint with score 48.27 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:46:08 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:46:08 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:46:08 - [34m[1mLOGS   [0m - Training checkpoint for epoch 1/iteration 198 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_1_iter_198.pt
2024-07-29 12:46:08 - [34m[1mLOGS   [0m - Model state for epoch 1/iteration 198 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_1_iter_198.pt
[31m===========================================================================[0m
2024-07-29 12:46:10 - [32m[1mINFO   [0m - Training epoch 2
2024-07-29 12:46:14 - [34m[1mLOGS   [0m - Epoch:   2 [     199/10000000], loss: {'classification': 3.2189, 'neural_augmentation': 0.3487, 'total_loss': 3.5676}, LR: [0.000204, 0.000204], Avg. batch load time: 3.216, Elapsed time:  3.28
2024-07-29 12:46:20 - [34m[1mLOGS   [0m - *** Training summary for epoch 2
	 loss={'classification': 2.7558, 'neural_augmentation': 0.336, 'total_loss': 3.0918}
2024-07-29 12:46:31 - [34m[1mLOGS   [0m - *** Validation summary for epoch 2
	 loss={'classification': 1.6141, 'neural_augmentation': 0.0, 'total_loss': 1.6141} || top1={'logits': 67.8829} || top5={'logits': 89.9662}
2024-07-29 12:46:31 - [34m[1mLOGS   [0m - Best checkpoint with score 67.88 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:46:31 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:46:32 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:46:32 - [34m[1mLOGS   [0m - Training checkpoint for epoch 2/iteration 306 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_2_iter_306.pt
2024-07-29 12:46:32 - [34m[1mLOGS   [0m - Model state for epoch 2/iteration 306 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_2_iter_306.pt
[31m===========================================================================[0m
2024-07-29 12:46:34 - [32m[1mINFO   [0m - Training epoch 3
2024-07-29 12:46:37 - [34m[1mLOGS   [0m - Epoch:   3 [     307/10000000], loss: {'classification': 2.3743, 'neural_augmentation': 0.367, 'total_loss': 2.7413}, LR: [0.00031, 0.00031], Avg. batch load time: 1.309, Elapsed time:  2.92
2024-07-29 12:46:43 - [34m[1mLOGS   [0m - *** Training summary for epoch 3
	 loss={'classification': 2.2889, 'neural_augmentation': 0.3293, 'total_loss': 2.6182}
2024-07-29 12:46:53 - [34m[1mLOGS   [0m - *** Validation summary for epoch 3
	 loss={'classification': 1.2365, 'neural_augmentation': 0.0, 'total_loss': 1.2365} || top1={'logits': 74.277} || top5={'logits': 92.9887}
2024-07-29 12:46:54 - [34m[1mLOGS   [0m - Best checkpoint with score 74.28 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:46:54 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:46:54 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:46:54 - [34m[1mLOGS   [0m - Training checkpoint for epoch 3/iteration 408 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_3_iter_408.pt
2024-07-29 12:46:54 - [34m[1mLOGS   [0m - Model state for epoch 3/iteration 408 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_3_iter_408.pt
[31m===========================================================================[0m
2024-07-29 12:46:56 - [32m[1mINFO   [0m - Training epoch 4
2024-07-29 12:47:00 - [34m[1mLOGS   [0m - Epoch:   4 [     409/10000000], loss: {'classification': 2.3289, 'neural_augmentation': 0.3237, 'total_loss': 2.6526}, LR: [0.00041, 0.00041], Avg. batch load time: 3.541, Elapsed time:  3.64
2024-07-29 12:47:05 - [34m[1mLOGS   [0m - *** Training summary for epoch 4
	 loss={'classification': 2.1076, 'neural_augmentation': 0.3272, 'total_loss': 2.4347}
2024-07-29 12:47:15 - [34m[1mLOGS   [0m - *** Validation summary for epoch 4
	 loss={'classification': 1.0659, 'neural_augmentation': 0.0, 'total_loss': 1.0659} || top1={'logits': 77.3896} || top5={'logits': 94.4009}
2024-07-29 12:47:16 - [34m[1mLOGS   [0m - Best checkpoint with score 77.39 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:47:16 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:47:16 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:47:16 - [34m[1mLOGS   [0m - Training checkpoint for epoch 4/iteration 500 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_4_iter_500.pt
2024-07-29 12:47:16 - [34m[1mLOGS   [0m - Model state for epoch 4/iteration 500 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_4_iter_500.pt
[31m===========================================================================[0m
2024-07-29 12:47:18 - [32m[1mINFO   [0m - Training epoch 5
2024-07-29 12:47:20 - [34m[1mLOGS   [0m - Epoch:   5 [     501/10000000], loss: {'classification': 1.9293, 'neural_augmentation': 0.2998, 'total_loss': 2.2291}, LR: [0.00047, 0.00047], Avg. batch load time: 1.385, Elapsed time:  1.47
2024-07-29 12:47:27 - [34m[1mLOGS   [0m - *** Training summary for epoch 5
	 loss={'classification': 2.0102, 'neural_augmentation': 0.3224, 'total_loss': 2.3326}
2024-07-29 12:47:37 - [34m[1mLOGS   [0m - *** Validation summary for epoch 5
	 loss={'classification': 0.9733, 'neural_augmentation': 0.0, 'total_loss': 0.9733} || top1={'logits': 79.1081} || top5={'logits': 95.0991}
2024-07-29 12:47:37 - [34m[1mLOGS   [0m - Best checkpoint with score 79.11 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:47:38 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_3.4910.pt
2024-07-29 12:47:38 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_48.2703.pt', 'checkpoint_score_67.8829.pt', 'checkpoint_score_74.2770.pt', 'checkpoint_score_77.3896.pt', 'checkpoint_score_79.1081.pt']
2024-07-29 12:47:38 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:47:38 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:47:38 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:47:39 - [34m[1mLOGS   [0m - Training checkpoint for epoch 5/iteration 598 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_5_iter_598.pt
2024-07-29 12:47:39 - [34m[1mLOGS   [0m - Model state for epoch 5/iteration 598 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_5_iter_598.pt
[31m===========================================================================[0m
2024-07-29 12:47:41 - [32m[1mINFO   [0m - Training epoch 6
2024-07-29 12:47:42 - [34m[1mLOGS   [0m - Epoch:   6 [     599/10000000], loss: {'classification': 1.7774, 'neural_augmentation': 0.3199, 'total_loss': 2.0973}, LR: [0.000457, 0.000457], Avg. batch load time: 1.304, Elapsed time:  1.38
2024-07-29 12:47:49 - [34m[1mLOGS   [0m - *** Training summary for epoch 6
	 loss={'classification': 1.9493, 'neural_augmentation': 0.3156, 'total_loss': 2.2649}
2024-07-29 12:47:59 - [34m[1mLOGS   [0m - *** Validation summary for epoch 6
	 loss={'classification': 0.9136, 'neural_augmentation': 0.0, 'total_loss': 0.9136} || top1={'logits': 80.4932} || top5={'logits': 95.4392}
2024-07-29 12:47:59 - [34m[1mLOGS   [0m - Best checkpoint with score 80.49 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:47:59 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_48.2703.pt
2024-07-29 12:47:59 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_67.8829.pt', 'checkpoint_score_74.2770.pt', 'checkpoint_score_77.3896.pt', 'checkpoint_score_79.1081.pt', 'checkpoint_score_80.4932.pt']
2024-07-29 12:48:00 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:48:00 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:48:00 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:48:00 - [34m[1mLOGS   [0m - Training checkpoint for epoch 6/iteration 691 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_6_iter_691.pt
2024-07-29 12:48:00 - [34m[1mLOGS   [0m - Model state for epoch 6/iteration 691 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_6_iter_691.pt
[31m===========================================================================[0m
2024-07-29 12:48:02 - [32m[1mINFO   [0m - Training epoch 7
2024-07-29 12:48:05 - [34m[1mLOGS   [0m - Epoch:   7 [     692/10000000], loss: {'classification': 1.8277, 'neural_augmentation': 0.3117, 'total_loss': 2.1394}, LR: [0.000442, 0.000442], Avg. batch load time: 2.485, Elapsed time:  2.55
2024-07-29 12:48:11 - [34m[1mLOGS   [0m - *** Training summary for epoch 7
	 loss={'classification': 1.8829, 'neural_augmentation': 0.3132, 'total_loss': 2.1961}
2024-07-29 12:48:21 - [34m[1mLOGS   [0m - *** Validation summary for epoch 7
	 loss={'classification': 0.8764, 'neural_augmentation': 0.0, 'total_loss': 0.8764} || top1={'logits': 81.2477} || top5={'logits': 95.9189}
2024-07-29 12:48:21 - [34m[1mLOGS   [0m - Best checkpoint with score 81.25 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:48:21 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_67.8829.pt
2024-07-29 12:48:21 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_74.2770.pt', 'checkpoint_score_77.3896.pt', 'checkpoint_score_79.1081.pt', 'checkpoint_score_80.4932.pt', 'checkpoint_score_81.2477.pt']
2024-07-29 12:48:22 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:48:22 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:48:22 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:48:22 - [34m[1mLOGS   [0m - Training checkpoint for epoch 7/iteration 787 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_7_iter_787.pt
2024-07-29 12:48:22 - [34m[1mLOGS   [0m - Model state for epoch 7/iteration 787 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_7_iter_787.pt
[31m===========================================================================[0m
2024-07-29 12:48:24 - [32m[1mINFO   [0m - Training epoch 8
2024-07-29 12:48:28 - [34m[1mLOGS   [0m - Epoch:   8 [     788/10000000], loss: {'classification': 1.8649, 'neural_augmentation': 0.3105, 'total_loss': 2.1754}, LR: [0.000426, 0.000426], Avg. batch load time: 3.153, Elapsed time:  3.22
2024-07-29 12:48:33 - [34m[1mLOGS   [0m - *** Training summary for epoch 8
	 loss={'classification': 1.8591, 'neural_augmentation': 0.3108, 'total_loss': 2.1699}
2024-07-29 12:48:43 - [34m[1mLOGS   [0m - *** Validation summary for epoch 8
	 loss={'classification': 0.8515, 'neural_augmentation': 0.0, 'total_loss': 0.8515} || top1={'logits': 81.7117} || top5={'logits': 96.0135}
2024-07-29 12:48:43 - [34m[1mLOGS   [0m - Best checkpoint with score 81.71 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:48:43 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_74.2770.pt
2024-07-29 12:48:43 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_77.3896.pt', 'checkpoint_score_79.1081.pt', 'checkpoint_score_80.4932.pt', 'checkpoint_score_81.2477.pt', 'checkpoint_score_81.7117.pt']
2024-07-29 12:48:44 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:48:44 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:48:44 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:48:44 - [34m[1mLOGS   [0m - Training checkpoint for epoch 8/iteration 886 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_8_iter_886.pt
2024-07-29 12:48:45 - [34m[1mLOGS   [0m - Model state for epoch 8/iteration 886 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_8_iter_886.pt
[31m===========================================================================[0m
2024-07-29 12:48:47 - [32m[1mINFO   [0m - Training epoch 9
2024-07-29 12:48:49 - [34m[1mLOGS   [0m - Epoch:   9 [     887/10000000], loss: {'classification': 1.8351, 'neural_augmentation': 0.3242, 'total_loss': 2.1594}, LR: [0.000407, 0.000407], Avg. batch load time: 1.915, Elapsed time:  1.99
2024-07-29 12:48:55 - [34m[1mLOGS   [0m - *** Training summary for epoch 9
	 loss={'classification': 1.836, 'neural_augmentation': 0.3083, 'total_loss': 2.1442}
2024-07-29 12:49:06 - [34m[1mLOGS   [0m - *** Validation summary for epoch 9
	 loss={'classification': 0.8266, 'neural_augmentation': 0.0, 'total_loss': 0.8266} || top1={'logits': 82.0203} || top5={'logits': 96.2207}
2024-07-29 12:49:06 - [34m[1mLOGS   [0m - Best checkpoint with score 82.02 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:49:06 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_77.3896.pt
2024-07-29 12:49:06 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_79.1081.pt', 'checkpoint_score_80.4932.pt', 'checkpoint_score_81.2477.pt', 'checkpoint_score_81.7117.pt', 'checkpoint_score_82.0203.pt']
2024-07-29 12:49:07 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:49:07 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:49:07 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:49:07 - [34m[1mLOGS   [0m - Training checkpoint for epoch 9/iteration 992 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_9_iter_992.pt
2024-07-29 12:49:07 - [34m[1mLOGS   [0m - Model state for epoch 9/iteration 992 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_9_iter_992.pt
[31m===========================================================================[0m
2024-07-29 12:49:09 - [32m[1mINFO   [0m - Training epoch 10
2024-07-29 12:49:12 - [34m[1mLOGS   [0m - Epoch:  10 [     993/10000000], loss: {'classification': 1.9587, 'neural_augmentation': 0.3114, 'total_loss': 2.2701}, LR: [0.000387, 0.000387], Avg. batch load time: 3.108, Elapsed time:  3.17
2024-07-29 12:49:18 - [34m[1mLOGS   [0m - *** Training summary for epoch 10
	 loss={'classification': 1.8211, 'neural_augmentation': 0.3114, 'total_loss': 2.1325}
2024-07-29 12:49:28 - [34m[1mLOGS   [0m - *** Validation summary for epoch 10
	 loss={'classification': 0.8083, 'neural_augmentation': 0.0, 'total_loss': 0.8083} || top1={'logits': 82.6284} || top5={'logits': 96.3446}
2024-07-29 12:49:28 - [34m[1mLOGS   [0m - Best checkpoint with score 82.63 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:49:28 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_79.1081.pt
2024-07-29 12:49:28 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_80.4932.pt', 'checkpoint_score_81.2477.pt', 'checkpoint_score_81.7117.pt', 'checkpoint_score_82.0203.pt', 'checkpoint_score_82.6284.pt']
2024-07-29 12:49:29 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:49:29 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:49:29 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:49:29 - [34m[1mLOGS   [0m - Training checkpoint for epoch 10/iteration 1085 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_10_iter_1085.pt
2024-07-29 12:49:29 - [34m[1mLOGS   [0m - Model state for epoch 10/iteration 1085 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_10_iter_1085.pt
[31m===========================================================================[0m
2024-07-29 12:49:31 - [32m[1mINFO   [0m - Training epoch 11
2024-07-29 12:49:34 - [34m[1mLOGS   [0m - Epoch:  11 [    1086/10000000], loss: {'classification': 1.744, 'neural_augmentation': 0.3289, 'total_loss': 2.0729}, LR: [0.000367, 0.000367], Avg. batch load time: 2.718, Elapsed time:  2.79
2024-07-29 12:49:40 - [34m[1mLOGS   [0m - *** Training summary for epoch 11
	 loss={'classification': 1.7961, 'neural_augmentation': 0.3191, 'total_loss': 2.1152}
2024-07-29 12:49:50 - [34m[1mLOGS   [0m - *** Validation summary for epoch 11
	 loss={'classification': 0.794, 'neural_augmentation': 0.0, 'total_loss': 0.794} || top1={'logits': 83.009} || top5={'logits': 96.5315}
2024-07-29 12:49:51 - [34m[1mLOGS   [0m - Best checkpoint with score 83.01 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:49:51 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_80.4932.pt
2024-07-29 12:49:51 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_81.2477.pt', 'checkpoint_score_81.7117.pt', 'checkpoint_score_82.0203.pt', 'checkpoint_score_82.6284.pt', 'checkpoint_score_83.0090.pt']
2024-07-29 12:49:52 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:49:52 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:49:52 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:49:52 - [34m[1mLOGS   [0m - Training checkpoint for epoch 11/iteration 1191 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_11_iter_1191.pt
2024-07-29 12:49:52 - [34m[1mLOGS   [0m - Model state for epoch 11/iteration 1191 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_11_iter_1191.pt
[31m===========================================================================[0m
2024-07-29 12:49:54 - [32m[1mINFO   [0m - Training epoch 12
2024-07-29 12:49:56 - [34m[1mLOGS   [0m - Epoch:  12 [    1192/10000000], loss: {'classification': 1.5852, 'neural_augmentation': 0.3345, 'total_loss': 1.9197}, LR: [0.000345, 0.000345], Avg. batch load time: 1.210, Elapsed time:  1.60
2024-07-29 12:50:03 - [34m[1mLOGS   [0m - *** Training summary for epoch 12
	 loss={'classification': 1.7752, 'neural_augmentation': 0.3275, 'total_loss': 2.1027}
2024-07-29 12:50:13 - [34m[1mLOGS   [0m - *** Validation summary for epoch 12
	 loss={'classification': 0.7837, 'neural_augmentation': 0.0, 'total_loss': 0.7837} || top1={'logits': 83.1306} || top5={'logits': 96.6712}
2024-07-29 12:50:13 - [34m[1mLOGS   [0m - Best checkpoint with score 83.13 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:50:13 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_81.2477.pt
2024-07-29 12:50:13 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_81.7117.pt', 'checkpoint_score_82.0203.pt', 'checkpoint_score_82.6284.pt', 'checkpoint_score_83.0090.pt', 'checkpoint_score_83.1306.pt']
2024-07-29 12:50:14 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:50:14 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:50:14 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:50:14 - [34m[1mLOGS   [0m - Training checkpoint for epoch 12/iteration 1292 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_12_iter_1292.pt
2024-07-29 12:50:14 - [34m[1mLOGS   [0m - Model state for epoch 12/iteration 1292 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_12_iter_1292.pt
[31m===========================================================================[0m
2024-07-29 12:50:16 - [32m[1mINFO   [0m - Training epoch 13
2024-07-29 12:50:18 - [34m[1mLOGS   [0m - Epoch:  13 [    1293/10000000], loss: {'classification': 1.6327, 'neural_augmentation': 0.3539, 'total_loss': 1.9866}, LR: [0.000322, 0.000322], Avg. batch load time: 1.663, Elapsed time:  1.73
2024-07-29 12:50:26 - [34m[1mLOGS   [0m - *** Training summary for epoch 13
	 loss={'classification': 1.7451, 'neural_augmentation': 0.3391, 'total_loss': 2.0842}
2024-07-29 12:50:35 - [34m[1mLOGS   [0m - *** Validation summary for epoch 13
	 loss={'classification': 0.7759, 'neural_augmentation': 0.0, 'total_loss': 0.7759} || top1={'logits': 83.2793} || top5={'logits': 96.6622}
2024-07-29 12:50:36 - [34m[1mLOGS   [0m - Best checkpoint with score 83.28 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:50:36 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_81.7117.pt
2024-07-29 12:50:36 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_82.0203.pt', 'checkpoint_score_82.6284.pt', 'checkpoint_score_83.0090.pt', 'checkpoint_score_83.1306.pt', 'checkpoint_score_83.2793.pt']
2024-07-29 12:50:37 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:50:37 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:50:37 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:50:37 - [34m[1mLOGS   [0m - Training checkpoint for epoch 13/iteration 1397 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_13_iter_1397.pt
2024-07-29 12:50:37 - [34m[1mLOGS   [0m - Model state for epoch 13/iteration 1397 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_13_iter_1397.pt
[31m===========================================================================[0m
2024-07-29 12:50:39 - [32m[1mINFO   [0m - Training epoch 14
2024-07-29 12:50:41 - [34m[1mLOGS   [0m - Epoch:  14 [    1398/10000000], loss: {'classification': 1.5594, 'neural_augmentation': 0.3614, 'total_loss': 1.9208}, LR: [0.000299, 0.000299], Avg. batch load time: 1.996, Elapsed time:  2.06
2024-07-29 12:50:47 - [34m[1mLOGS   [0m - *** Training summary for epoch 14
	 loss={'classification': 1.7748, 'neural_augmentation': 0.3531, 'total_loss': 2.1279}
2024-07-29 12:50:58 - [34m[1mLOGS   [0m - *** Validation summary for epoch 14
	 loss={'classification': 0.7658, 'neural_augmentation': 0.0, 'total_loss': 0.7658} || top1={'logits': 83.5045} || top5={'logits': 96.6847}
2024-07-29 12:50:58 - [34m[1mLOGS   [0m - Best checkpoint with score 83.50 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:50:58 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_82.0203.pt
2024-07-29 12:50:58 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_82.6284.pt', 'checkpoint_score_83.0090.pt', 'checkpoint_score_83.1306.pt', 'checkpoint_score_83.2793.pt', 'checkpoint_score_83.5045.pt']
2024-07-29 12:50:59 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:50:59 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:50:59 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:50:59 - [34m[1mLOGS   [0m - Training checkpoint for epoch 14/iteration 1493 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_14_iter_1493.pt
2024-07-29 12:50:59 - [34m[1mLOGS   [0m - Model state for epoch 14/iteration 1493 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_14_iter_1493.pt
[31m===========================================================================[0m
2024-07-29 12:51:01 - [32m[1mINFO   [0m - Training epoch 15
2024-07-29 12:51:04 - [34m[1mLOGS   [0m - Epoch:  15 [    1494/10000000], loss: {'classification': 1.7404, 'neural_augmentation': 0.3863, 'total_loss': 2.1267}, LR: [0.000275, 0.000275], Avg. batch load time: 3.084, Elapsed time:  3.15
2024-07-29 12:51:11 - [34m[1mLOGS   [0m - *** Training summary for epoch 15
	 loss={'classification': 1.7355, 'neural_augmentation': 0.3739, 'total_loss': 2.1095}
2024-07-29 12:51:21 - [34m[1mLOGS   [0m - *** Validation summary for epoch 15
	 loss={'classification': 0.7599, 'neural_augmentation': 0.0, 'total_loss': 0.7599} || top1={'logits': 83.7297} || top5={'logits': 96.7815}
2024-07-29 12:51:21 - [34m[1mLOGS   [0m - Best checkpoint with score 83.73 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:51:22 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_82.6284.pt
2024-07-29 12:51:22 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_83.0090.pt', 'checkpoint_score_83.1306.pt', 'checkpoint_score_83.2793.pt', 'checkpoint_score_83.5045.pt', 'checkpoint_score_83.7297.pt']
2024-07-29 12:51:22 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:51:22 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:51:22 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:51:23 - [34m[1mLOGS   [0m - Training checkpoint for epoch 15/iteration 1603 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_15_iter_1603.pt
2024-07-29 12:51:23 - [34m[1mLOGS   [0m - Model state for epoch 15/iteration 1603 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_15_iter_1603.pt
[31m===========================================================================[0m
2024-07-29 12:51:25 - [32m[1mINFO   [0m - Training epoch 16
2024-07-29 12:51:26 - [34m[1mLOGS   [0m - Epoch:  16 [    1604/10000000], loss: {'classification': 1.6268, 'neural_augmentation': 0.3911, 'total_loss': 2.0178}, LR: [0.000251, 0.000251], Avg. batch load time: 1.655, Elapsed time:  1.72
2024-07-29 12:51:33 - [34m[1mLOGS   [0m - *** Training summary for epoch 16
	 loss={'classification': 1.7518, 'neural_augmentation': 0.3952, 'total_loss': 2.147}
2024-07-29 12:51:43 - [34m[1mLOGS   [0m - *** Validation summary for epoch 16
	 loss={'classification': 0.7537, 'neural_augmentation': 0.0, 'total_loss': 0.7537} || top1={'logits': 83.6689} || top5={'logits': 96.795}
2024-07-29 12:51:43 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:51:44 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:51:44 - [34m[1mLOGS   [0m - Training checkpoint for epoch 16/iteration 1701 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_16_iter_1701.pt
2024-07-29 12:51:44 - [34m[1mLOGS   [0m - Model state for epoch 16/iteration 1701 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_16_iter_1701.pt
[31m===========================================================================[0m
2024-07-29 12:51:46 - [32m[1mINFO   [0m - Training epoch 17
2024-07-29 12:51:48 - [34m[1mLOGS   [0m - Epoch:  17 [    1702/10000000], loss: {'classification': 1.5582, 'neural_augmentation': 0.432, 'total_loss': 1.9902}, LR: [0.000228, 0.000228], Avg. batch load time: 2.410, Elapsed time:  2.47
2024-07-29 12:51:54 - [34m[1mLOGS   [0m - *** Training summary for epoch 17
	 loss={'classification': 1.7601, 'neural_augmentation': 0.4184, 'total_loss': 2.1785}
2024-07-29 12:52:04 - [34m[1mLOGS   [0m - *** Validation summary for epoch 17
	 loss={'classification': 0.7492, 'neural_augmentation': 0.0, 'total_loss': 0.7492} || top1={'logits': 83.7883} || top5={'logits': 96.8131}
2024-07-29 12:52:05 - [34m[1mLOGS   [0m - Best checkpoint with score 83.79 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:52:05 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_83.0090.pt
2024-07-29 12:52:05 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_83.1306.pt', 'checkpoint_score_83.2793.pt', 'checkpoint_score_83.5045.pt', 'checkpoint_score_83.7297.pt', 'checkpoint_score_83.7883.pt']
2024-07-29 12:52:06 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:52:06 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:52:06 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:52:06 - [34m[1mLOGS   [0m - Training checkpoint for epoch 17/iteration 1789 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_17_iter_1789.pt
2024-07-29 12:52:06 - [34m[1mLOGS   [0m - Model state for epoch 17/iteration 1789 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_17_iter_1789.pt
[31m===========================================================================[0m
2024-07-29 12:52:08 - [32m[1mINFO   [0m - Training epoch 18
2024-07-29 12:52:10 - [34m[1mLOGS   [0m - Epoch:  18 [    1790/10000000], loss: {'classification': 1.6889, 'neural_augmentation': 0.4372, 'total_loss': 2.1261}, LR: [0.000205, 0.000205], Avg. batch load time: 1.777, Elapsed time:  1.84
2024-07-29 12:52:17 - [34m[1mLOGS   [0m - *** Training summary for epoch 18
	 loss={'classification': 1.7303, 'neural_augmentation': 0.4455, 'total_loss': 2.1758}
2024-07-29 12:52:27 - [34m[1mLOGS   [0m - *** Validation summary for epoch 18
	 loss={'classification': 0.7446, 'neural_augmentation': 0.0, 'total_loss': 0.7446} || top1={'logits': 83.9685} || top5={'logits': 96.8581}
2024-07-29 12:52:27 - [34m[1mLOGS   [0m - Best checkpoint with score 83.97 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:52:27 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_83.1306.pt
2024-07-29 12:52:27 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_83.2793.pt', 'checkpoint_score_83.5045.pt', 'checkpoint_score_83.7297.pt', 'checkpoint_score_83.7883.pt', 'checkpoint_score_83.9685.pt']
2024-07-29 12:52:28 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:52:28 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:52:28 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:52:28 - [34m[1mLOGS   [0m - Training checkpoint for epoch 18/iteration 1888 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_18_iter_1888.pt
2024-07-29 12:52:28 - [34m[1mLOGS   [0m - Model state for epoch 18/iteration 1888 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_18_iter_1888.pt
[31m===========================================================================[0m
2024-07-29 12:52:30 - [32m[1mINFO   [0m - Training epoch 19
2024-07-29 12:52:32 - [34m[1mLOGS   [0m - Epoch:  19 [    1889/10000000], loss: {'classification': 1.6277, 'neural_augmentation': 0.4711, 'total_loss': 2.0988}, LR: [0.000183, 0.000183], Avg. batch load time: 1.598, Elapsed time:  1.66
2024-07-29 12:52:38 - [34m[1mLOGS   [0m - *** Training summary for epoch 19
	 loss={'classification': 1.7364, 'neural_augmentation': 0.4738, 'total_loss': 2.2102}
2024-07-29 12:52:48 - [34m[1mLOGS   [0m - *** Validation summary for epoch 19
	 loss={'classification': 0.7413, 'neural_augmentation': 0.0, 'total_loss': 0.7413} || top1={'logits': 84.1532} || top5={'logits': 96.8919}
2024-07-29 12:52:49 - [34m[1mLOGS   [0m - Best checkpoint with score 84.15 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:52:49 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_83.2793.pt
2024-07-29 12:52:49 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_83.5045.pt', 'checkpoint_score_83.7297.pt', 'checkpoint_score_83.7883.pt', 'checkpoint_score_83.9685.pt', 'checkpoint_score_84.1532.pt']
2024-07-29 12:52:50 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:52:50 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:52:50 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:52:50 - [34m[1mLOGS   [0m - Training checkpoint for epoch 19/iteration 1987 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_19_iter_1987.pt
2024-07-29 12:52:50 - [34m[1mLOGS   [0m - Model state for epoch 19/iteration 1987 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_19_iter_1987.pt
[31m===========================================================================[0m
2024-07-29 12:52:52 - [32m[1mINFO   [0m - Training epoch 20
2024-07-29 12:52:54 - [34m[1mLOGS   [0m - Epoch:  20 [    1988/10000000], loss: {'classification': 1.5874, 'neural_augmentation': 0.4893, 'total_loss': 2.0767}, LR: [0.000162, 0.000162], Avg. batch load time: 1.771, Elapsed time:  1.84
2024-07-29 12:53:00 - [34m[1mLOGS   [0m - *** Training summary for epoch 20
	 loss={'classification': 1.7318, 'neural_augmentation': 0.5036, 'total_loss': 2.2354}
2024-07-29 12:53:10 - [34m[1mLOGS   [0m - *** Validation summary for epoch 20
	 loss={'classification': 0.7371, 'neural_augmentation': 0.0, 'total_loss': 0.7371} || top1={'logits': 84.3063} || top5={'logits': 96.9234}
2024-07-29 12:53:10 - [34m[1mLOGS   [0m - Best checkpoint with score 84.31 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:53:10 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_83.5045.pt
2024-07-29 12:53:10 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_83.7297.pt', 'checkpoint_score_83.7883.pt', 'checkpoint_score_83.9685.pt', 'checkpoint_score_84.1532.pt', 'checkpoint_score_84.3063.pt']
2024-07-29 12:53:11 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:53:11 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:53:11 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:53:11 - [34m[1mLOGS   [0m - Training checkpoint for epoch 20/iteration 2085 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_20_iter_2085.pt
2024-07-29 12:53:11 - [34m[1mLOGS   [0m - Model state for epoch 20/iteration 2085 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_20_iter_2085.pt
[31m===========================================================================[0m
2024-07-29 12:53:13 - [32m[1mINFO   [0m - Training epoch 21
2024-07-29 12:53:16 - [34m[1mLOGS   [0m - Epoch:  21 [    2086/10000000], loss: {'classification': 1.5419, 'neural_augmentation': 0.5507, 'total_loss': 2.0926}, LR: [0.000143, 0.000143], Avg. batch load time: 2.320, Elapsed time:  2.38
2024-07-29 12:53:22 - [34m[1mLOGS   [0m - *** Training summary for epoch 21
	 loss={'classification': 1.7266, 'neural_augmentation': 0.5331, 'total_loss': 2.2597}
2024-07-29 12:53:32 - [34m[1mLOGS   [0m - *** Validation summary for epoch 21
	 loss={'classification': 0.7352, 'neural_augmentation': 0.0, 'total_loss': 0.7352} || top1={'logits': 84.2162} || top5={'logits': 96.8784}
2024-07-29 12:53:32 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:53:33 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:53:33 - [34m[1mLOGS   [0m - Training checkpoint for epoch 21/iteration 2187 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_21_iter_2187.pt
2024-07-29 12:53:33 - [34m[1mLOGS   [0m - Model state for epoch 21/iteration 2187 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_21_iter_2187.pt
[31m===========================================================================[0m
2024-07-29 12:53:35 - [32m[1mINFO   [0m - Training epoch 22
2024-07-29 12:53:39 - [34m[1mLOGS   [0m - Epoch:  22 [    2188/10000000], loss: {'classification': 1.8125, 'neural_augmentation': 0.5711, 'total_loss': 2.3836}, LR: [0.000124, 0.000124], Avg. batch load time: 4.095, Elapsed time:  4.20
2024-07-29 12:53:45 - [34m[1mLOGS   [0m - *** Training summary for epoch 22
	 loss={'classification': 1.7127, 'neural_augmentation': 0.563, 'total_loss': 2.2757}
2024-07-29 12:53:55 - [34m[1mLOGS   [0m - *** Validation summary for epoch 22
	 loss={'classification': 0.7321, 'neural_augmentation': 0.0, 'total_loss': 0.7321} || top1={'logits': 84.3626} || top5={'logits': 96.9414}
2024-07-29 12:53:55 - [34m[1mLOGS   [0m - Best checkpoint with score 84.36 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:53:55 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_83.7297.pt
2024-07-29 12:53:55 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_83.7883.pt', 'checkpoint_score_83.9685.pt', 'checkpoint_score_84.1532.pt', 'checkpoint_score_84.3063.pt', 'checkpoint_score_84.3626.pt']
2024-07-29 12:53:56 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:53:56 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:53:56 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:53:56 - [34m[1mLOGS   [0m - Training checkpoint for epoch 22/iteration 2287 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_22_iter_2287.pt
2024-07-29 12:53:56 - [34m[1mLOGS   [0m - Model state for epoch 22/iteration 2287 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_22_iter_2287.pt
[31m===========================================================================[0m
2024-07-29 12:53:58 - [32m[1mINFO   [0m - Training epoch 23
2024-07-29 12:53:59 - [34m[1mLOGS   [0m - Epoch:  23 [    2288/10000000], loss: {'classification': 1.6787, 'neural_augmentation': 0.5899, 'total_loss': 2.2686}, LR: [0.000108, 0.000108], Avg. batch load time: 1.151, Elapsed time:  1.22
2024-07-29 12:54:07 - [34m[1mLOGS   [0m - *** Training summary for epoch 23
	 loss={'classification': 1.7238, 'neural_augmentation': 0.5896, 'total_loss': 2.3134}
2024-07-29 12:54:17 - [34m[1mLOGS   [0m - *** Validation summary for epoch 23
	 loss={'classification': 0.7303, 'neural_augmentation': 0.0, 'total_loss': 0.7303} || top1={'logits': 84.3288} || top5={'logits': 96.9887}
2024-07-29 12:54:17 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:54:17 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:54:17 - [34m[1mLOGS   [0m - Training checkpoint for epoch 23/iteration 2381 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_23_iter_2381.pt
2024-07-29 12:54:18 - [34m[1mLOGS   [0m - Model state for epoch 23/iteration 2381 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_23_iter_2381.pt
[31m===========================================================================[0m
2024-07-29 12:54:20 - [32m[1mINFO   [0m - Training epoch 24
2024-07-29 12:54:22 - [34m[1mLOGS   [0m - Epoch:  24 [    2382/10000000], loss: {'classification': 1.6206, 'neural_augmentation': 0.6221, 'total_loss': 2.2426}, LR: [9.3e-05, 9.3e-05], Avg. batch load time: 2.490, Elapsed time:  2.55
2024-07-29 12:54:29 - [34m[1mLOGS   [0m - *** Training summary for epoch 24
	 loss={'classification': 1.7075, 'neural_augmentation': 0.6166, 'total_loss': 2.3241}
2024-07-29 12:54:39 - [34m[1mLOGS   [0m - *** Validation summary for epoch 24
	 loss={'classification': 0.7289, 'neural_augmentation': 0.0, 'total_loss': 0.7289} || top1={'logits': 84.4617} || top5={'logits': 97.0023}
2024-07-29 12:54:39 - [34m[1mLOGS   [0m - Best checkpoint with score 84.46 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:54:39 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_83.7883.pt
2024-07-29 12:54:39 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_83.9685.pt', 'checkpoint_score_84.1532.pt', 'checkpoint_score_84.3063.pt', 'checkpoint_score_84.3626.pt', 'checkpoint_score_84.4617.pt']
2024-07-29 12:54:40 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:54:40 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:54:40 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:54:40 - [34m[1mLOGS   [0m - Training checkpoint for epoch 24/iteration 2487 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_24_iter_2487.pt
2024-07-29 12:54:40 - [34m[1mLOGS   [0m - Model state for epoch 24/iteration 2487 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_24_iter_2487.pt
[31m===========================================================================[0m
2024-07-29 12:54:42 - [32m[1mINFO   [0m - Training epoch 25
2024-07-29 12:54:44 - [34m[1mLOGS   [0m - Epoch:  25 [    2488/10000000], loss: {'classification': 1.5788, 'neural_augmentation': 0.6394, 'total_loss': 2.2182}, LR: [8e-05, 8e-05], Avg. batch load time: 1.263, Elapsed time:  1.37
2024-07-29 12:54:51 - [34m[1mLOGS   [0m - *** Training summary for epoch 25
	 loss={'classification': 1.715, 'neural_augmentation': 0.641, 'total_loss': 2.356}
2024-07-29 12:55:02 - [34m[1mLOGS   [0m - *** Validation summary for epoch 25
	 loss={'classification': 0.7274, 'neural_augmentation': 0.0, 'total_loss': 0.7274} || top1={'logits': 84.509} || top5={'logits': 97.0203}
2024-07-29 12:55:02 - [34m[1mLOGS   [0m - Best checkpoint with score 84.51 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:55:02 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_83.9685.pt
2024-07-29 12:55:02 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_84.1532.pt', 'checkpoint_score_84.3063.pt', 'checkpoint_score_84.3626.pt', 'checkpoint_score_84.4617.pt', 'checkpoint_score_84.5090.pt']
2024-07-29 12:55:03 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:55:03 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:55:03 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:55:03 - [34m[1mLOGS   [0m - Training checkpoint for epoch 25/iteration 2589 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_25_iter_2589.pt
2024-07-29 12:55:03 - [34m[1mLOGS   [0m - Model state for epoch 25/iteration 2589 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_25_iter_2589.pt
[31m===========================================================================[0m
2024-07-29 12:55:05 - [32m[1mINFO   [0m - Training epoch 26
2024-07-29 12:55:07 - [34m[1mLOGS   [0m - Epoch:  26 [    2590/10000000], loss: {'classification': 1.6478, 'neural_augmentation': 0.651, 'total_loss': 2.2988}, LR: [6.9e-05, 6.9e-05], Avg. batch load time: 1.847, Elapsed time:  1.95
2024-07-29 12:55:14 - [34m[1mLOGS   [0m - *** Training summary for epoch 26
	 loss={'classification': 1.7011, 'neural_augmentation': 0.6634, 'total_loss': 2.3645}
2024-07-29 12:55:24 - [34m[1mLOGS   [0m - *** Validation summary for epoch 26
	 loss={'classification': 0.7266, 'neural_augmentation': 0.0, 'total_loss': 0.7266} || top1={'logits': 84.4437} || top5={'logits': 97.0225}
2024-07-29 12:55:25 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:55:25 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:55:25 - [34m[1mLOGS   [0m - Training checkpoint for epoch 26/iteration 2697 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_26_iter_2697.pt
2024-07-29 12:55:25 - [34m[1mLOGS   [0m - Model state for epoch 26/iteration 2697 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_26_iter_2697.pt
[31m===========================================================================[0m
2024-07-29 12:55:27 - [32m[1mINFO   [0m - Training epoch 27
2024-07-29 12:55:31 - [34m[1mLOGS   [0m - Epoch:  27 [    2698/10000000], loss: {'classification': 1.7814, 'neural_augmentation': 0.6668, 'total_loss': 2.4482}, LR: [6.1e-05, 6.1e-05], Avg. batch load time: 3.996, Elapsed time:  4.11
2024-07-29 12:55:36 - [34m[1mLOGS   [0m - *** Training summary for epoch 27
	 loss={'classification': 1.7174, 'neural_augmentation': 0.6788, 'total_loss': 2.3962}
2024-07-29 12:55:46 - [34m[1mLOGS   [0m - *** Validation summary for epoch 27
	 loss={'classification': 0.7256, 'neural_augmentation': 0.0, 'total_loss': 0.7256} || top1={'logits': 84.509} || top5={'logits': 97.0113}
2024-07-29 12:55:47 - [34m[1mLOGS   [0m - Best checkpoint with score 84.51 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:55:47 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:55:47 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:55:47 - [34m[1mLOGS   [0m - Training checkpoint for epoch 27/iteration 2790 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_27_iter_2790.pt
2024-07-29 12:55:47 - [34m[1mLOGS   [0m - Model state for epoch 27/iteration 2790 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_27_iter_2790.pt
[31m===========================================================================[0m
2024-07-29 12:55:49 - [32m[1mINFO   [0m - Training epoch 28
2024-07-29 12:55:52 - [34m[1mLOGS   [0m - Epoch:  28 [    2791/10000000], loss: {'classification': 1.5357, 'neural_augmentation': 0.645, 'total_loss': 2.1807}, LR: [5.5e-05, 5.5e-05], Avg. batch load time: 1.453, Elapsed time:  2.39
2024-07-29 12:55:59 - [34m[1mLOGS   [0m - *** Training summary for epoch 28
	 loss={'classification': 1.6933, 'neural_augmentation': 0.6933, 'total_loss': 2.3866}
2024-07-29 12:56:09 - [34m[1mLOGS   [0m - *** Validation summary for epoch 28
	 loss={'classification': 0.724, 'neural_augmentation': 0.0, 'total_loss': 0.724} || top1={'logits': 84.5923} || top5={'logits': 97.0383}
2024-07-29 12:56:10 - [34m[1mLOGS   [0m - Best checkpoint with score 84.59 saved at /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_best.pt
2024-07-29 12:56:10 - [34m[1mLOGS   [0m - Deleting checkpoint: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_score_84.1532.pt
2024-07-29 12:56:10 - [34m[1mLOGS   [0m - Averaging checkpoints: ['checkpoint_score_84.3063.pt', 'checkpoint_score_84.3626.pt', 'checkpoint_score_84.4617.pt', 'checkpoint_score_84.5090.pt', 'checkpoint_score_84.5923.pt']
2024-07-29 12:56:11 - [34m[1mLOGS   [0m - Averaged checkpoint saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_avg.pt
2024-07-29 12:56:11 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:56:11 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:56:11 - [34m[1mLOGS   [0m - Training checkpoint for epoch 28/iteration 2894 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_28_iter_2894.pt
2024-07-29 12:56:11 - [34m[1mLOGS   [0m - Model state for epoch 28/iteration 2894 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_28_iter_2894.pt
[31m===========================================================================[0m
2024-07-29 12:56:13 - [32m[1mINFO   [0m - Training epoch 29
2024-07-29 12:56:16 - [34m[1mLOGS   [0m - Epoch:  29 [    2895/10000000], loss: {'classification': 1.6791, 'neural_augmentation': 0.7155, 'total_loss': 2.3946}, LR: [5.1e-05, 5.1e-05], Avg. batch load time: 2.050, Elapsed time:  3.16
2024-07-29 12:56:22 - [34m[1mLOGS   [0m - *** Training summary for epoch 29
	 loss={'classification': 1.6994, 'neural_augmentation': 0.7041, 'total_loss': 2.4035}
2024-07-29 12:56:32 - [34m[1mLOGS   [0m - *** Validation summary for epoch 29
	 loss={'classification': 0.7243, 'neural_augmentation': 0.0, 'total_loss': 0.7243} || top1={'logits': 84.5495} || top5={'logits': 97.0495}
2024-07-29 12:56:33 - [34m[1mLOGS   [0m - Last training checkpoint is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_last.pt
2024-07-29 12:56:35 - [34m[1mLOGS   [0m - Last checkpoint's model state is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_last.pt
2024-07-29 12:56:35 - [34m[1mLOGS   [0m - Training checkpoint for epoch 29/iteration 3004 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/training_checkpoint_epoch_29_iter_3004.pt
2024-07-29 12:56:35 - [34m[1mLOGS   [0m - Model state for epoch 29/iteration 3004 is saved at: /ML-A100/team/mm/models/catlip_data/results_small_dci/9_food172_lp/train/checkpoint_epoch_29_iter_3004.pt
2024-07-29 12:56:35 - [34m[1mLOGS   [0m - Training took 00:16:43.01
