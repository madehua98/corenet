# pytest: disable

taskname: '+ ViT-B/16 [FT-IN1k]'

common:
  run_label: "train"
  log_freq: 500
  auto_resume: true
  mixed_precision: true
  mixed_precision_dtype: "bfloat16"
  grad_clip: 1.0
  save_all_checkpoints: true

# dataset:
#   root: "/ML-A100/team/mm/models/food101/food101"
#   # effective batch size is 512 (512 * 1 A100 80 GB GPU)
#   train_batch_size0: 64
#   val_batch_size0: 50
#   eval_batch_size0: 50
#   workers: 64 # use all CPUs
#   persistent_workers: true
#   pin_memory: true 
#   name: "food101"
#   category: "classification"

dataset:
  root_train: "/ML-A100/team/mm/models/food172/food_172/train_images"
  root_val: "/ML-A100/team/mm/models/food172/food_172/test_images"
  # effective batch size is 512 (512 * 1 A100 80 GB GPU)
  train_batch_size0: 128
  val_batch_size0: 100
  eval_batch_size0: 100
  workers: 64 # use all CPUs
  persistent_workers: true
  pin_memory: true
  name: "imagenet"
  category: "classification"

image_augmentation:
  # training related augmentation
  random_resized_crop:
    enable: true
    interpolation: "bilinear"
  random_horizontal_flip:
    enable: true
  # validation related augmentation
  resize:
    enable: true
    size: 232
    interpolation: "bilinear"
  center_crop:
    enable: true
    size: 224
  # mixup:
  #   alpha: 0.2
  #   enable: true
  #   p: 1.0

sampler:
  name: "variable_batch_sampler"
  vbs:
    crop_size_width: 224
    crop_size_height: 224
    max_n_scales: 25
    min_crop_size_width: 128
    max_crop_size_width: 320
    min_crop_size_height: 128
    max_crop_size_height: 320
    check_scale: 32

loss:
  category: "composite_loss"
  composite_loss:
    - loss_category: "classification"
      loss_weight: 1.0
      classification:
        name: "cross_entropy"
        cross_entropy:
          label_smoothing: 0.1
    - loss_category: "neural_augmentation"
      loss_weight: 1.0
      neural_augmentation:
        perceptual_metric: "psnr"
        target_value: [ 40, 20 ]
        curriculum_method: "cosine"

optim:
  name: "adamw"
  weight_decay: 0.05
  no_decay_bn_filter_bias: true
  adamw:
    beta1: 0.9
    beta2: 0.999

scheduler:
  name: "cosine"
  max_epochs: 30
  warmup_iterations: 500
  warmup_init_lr: 1.e-6
  cosine:
    max_lr: 0.00003
    min_lr: 0.000003

model:
  activation_checkpointing: true
  # during finetuning, we need to skip loading the classifier from a pre-trained model
  # because the number of classes in down-stream task and pre-training are different.
  #resume_exclude_scopes: [ "head" ]
  resume_exclude_scopes: [ "classifier" ]
  ft: true
  classification:
    name: "foodv"
    #name: "vit"
    n_classes: 172
    # Initalize the model with a pre-trained model and finetune it.
    # Note that during evaluation, we need to set 'resume_exclude_scopes' to an empty string
    # and replace the 'pretrained' with the fine-tuned model path.
    pretrained: "/ML-A100/team/mm/models/catlip_data/results_base_dci/train/checkpoint_epoch_9_iter_79060.pt"
    #pretrained: "/ML-A100/team/mm/models/vit_base.pt"
    foodv:
      mode: "base"
      norm_layer: "layer_norm_fp32"
      use_flash_attention: true
      connector_type: "dci"

  # use rangeaugment
  learn_augmentation: 
    brightness: true
    contrast: true
    noise: true
    mode: "distribution"
  activation:
    name: "gelu"
  layer:
    conv_init: "kaiming_normal"
    linear_init: "trunc_normal"
    linear_init_std_dev: 0.02
  
ema:
  enable: false
  momentum: 0.0005

stats:
  train: ["loss"]
  #val: [ "loss", "top1", "top5", "multiclass_classification_pr(pred=logits)" ]
  val: [ "loss", "top1", "top5" ]
  checkpoint_metric: "top1.logits"
  checkpoint_metric_max: true
