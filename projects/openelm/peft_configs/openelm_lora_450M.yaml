# pytest: disable

taskname: '+ [OpenELM-LoRA-450M]'

_anchor_context_length: &_anchor_context_length 2048
# actual vocab size is 32001 after adding padding token, so we add few extra tokens to make it more hardware friendly
# for classification layer in LM model
_anchor_vocab_size: &_anchor_vocab_size 32128 
_anchor_padding_index: &_anchor_padding_index 32000

common:
  run_label: "train"
  log_freq: 500
  auto_resume: true
  grad_clip: 1.0
  save_all_checkpoints: true
  save_interval_freq: 5000
  eval_every_k_iterations: 10000
  mixed_precision: true
  mixed_precision_dtype: "bfloat16"
  accum_freq: 2

dataset:
  root_train: ""
  disable_val: true
  train_batch_size0: 8
  workers: 4
  persistent_workers: true
  pin_memory: true

  category: "language_modeling"
  name: "commonsense_170k"
  language_modeling:
    commonsense_170k:
      # Uncomment the below line and update the path to CommonSense170k.
      # path: <PATH_TO_COMMONSENSE170K>
    sequence_length: *_anchor_context_length
    min_tokens_per_text: 0
    min_characters_per_text: 0
    shuffle_data: true

text_tokenizer:
  name: "sentence_piece"
  sentence_piece:
    enable_nfc_normalization: true
    append_sot_token: true
    append_eot_token: true
    # Uncomment the below line and update the path of LLAMA SentencePiece model file
    # model_path: <PATH_OF_LLAMA_SPM_MODEL>

loss:
  category: "language_modeling"
  language_modeling:
    name: "cross_entropy"
    cross_entropy:
      ignore_index: *_anchor_padding_index
      use_z_loss: true

optim:
  name: "adamw"
  weight_decay: 0
  no_decay_bn_filter_bias: true
  adamw:
    beta1: 0.9
    beta2: 0.95
    eps: 1.e-8

scheduler:
  is_iteration_based: true
  # Train for 3 epochs.
  max_iterations: 4375
  name: cosine
  warmup_init_lr: 1.e-06
  warmup_iterations: 1000
  cosine:
    max_lr: 0.0003
    # papers use min_lr= 0.1 x max_lr
    min_lr: 0.00003

model:
  # Allow reloading the pretrained model.
  rename_scopes_map:
    - ['token_embeddings.weight', 'token_embeddings.base_layer.weight']
    - ['layers.(.*).qkv_proj.weight', 'layers.\1.qkv_proj.base_layer.weight']
    - ['layers.(.*).out_proj.weight', 'layers.\1.out_proj.base_layer.weight']
    - ['layers.(.*).proj_(\d).weight', 'layers.\1.proj_\2.base_layer.weight']
  ignore_missing_scopes:
    - ".*lora.*"
  freeze_modules: [".*base_layer.*", ".*norm.*"]
  activation_checkpointing: true
  language_modeling:
    name: "general_gpt"
    general_gpt:
      model_name: "OpenELM-450M"
      vocab_size: *_anchor_vocab_size
      max_context_length: *_anchor_context_length
      padding_index: *_anchor_padding_index
  lora:
    use_lora: true
    config:
      - regex: '.*token_embedding.*'
        module_type: 'embedding'
        params:
          adapter_name: lora
          r: &r 32
          lora_alpha: &alpha 32
          lora_dropout: &dropout 0.1
          init_lora_weights: &ilw true
          use_rslora: &ur false
          use_dora: &ud false
      - regex: '.*out_proj.*'
        module_type: 'linear'
        params:
          adapter_name: lora
          r: *r
          lora_alpha: *alpha
          lora_dropout: *dropout
          init_lora_weights: *ilw
          use_rslora: *ur
          use_dora: *ud
      - regex: '.*qkv_proj.*'
        module_type: 'linear'
        params:
          adapter_name: lora
          r: *r
          lora_alpha: *alpha
          lora_dropout: *dropout
          init_lora_weights: *ilw
          use_rslora: *ur
          use_dora: *ud
      - regex: '.*proj_\d.*'
        module_type: 'linear'
        params:
          adapter_name: lora
          r: *r
          lora_alpha: *alpha
          lora_dropout: *dropout
          init_lora_weights: *ilw
          use_rslora: *ur
          use_dora: *ud

stats:
  val: [ "loss"]
  train: ["loss"]
  checkpoint_metric: "loss.total_loss"
  checkpoint_metric_max: false

lm_eval_wrapper:
  add_sot_token: true
